"""
@generated by mypy-protobuf.  Do not edit manually!
isort:skip_file
"""
import builtins
import google.protobuf.descriptor
import google.protobuf.internal.enum_type_wrapper
import google.protobuf.message
import google.rpc.status_pb2
import typing
import typing_extensions

DESCRIPTOR: google.protobuf.descriptor.FileDescriptor = ...

class ConverseConfig(google.protobuf.message.Message):
    """Specifies how to process the `ConverseRequest` messages."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    AUDIO_IN_CONFIG_FIELD_NUMBER: builtins.int
    AUDIO_OUT_CONFIG_FIELD_NUMBER: builtins.int
    CONVERSE_STATE_FIELD_NUMBER: builtins.int
    @property
    def audio_in_config(self) -> global___AudioInConfig:
        """*Required* Specifies how to process the subsequent incoming audio."""
        pass
    @property
    def audio_out_config(self) -> global___AudioOutConfig:
        """*Required* Specifies how to format the audio that will be returned."""
        pass
    @property
    def converse_state(self) -> global___ConverseState:
        """*Required* Represents the current dialog state."""
        pass
    def __init__(self,
        *,
        audio_in_config : typing.Optional[global___AudioInConfig] = ...,
        audio_out_config : typing.Optional[global___AudioOutConfig] = ...,
        converse_state : typing.Optional[global___ConverseState] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["audio_in_config",b"audio_in_config","audio_out_config",b"audio_out_config","converse_state",b"converse_state"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["audio_in_config",b"audio_in_config","audio_out_config",b"audio_out_config","converse_state",b"converse_state"]) -> None: ...
global___ConverseConfig = ConverseConfig

class AudioInConfig(google.protobuf.message.Message):
    """Specifies how to process the `audio_in` data that will be provided in
    subsequent requests. For recommended settings, see the Google Assistant SDK
    [best
    practices](https://developers.google.com/assistant/sdk/develop/grpc/best-practices/audio).
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class _Encoding:
        ValueType = typing.NewType('ValueType', builtins.int)
        V: typing_extensions.TypeAlias = ValueType
    class _EncodingEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_Encoding.ValueType], builtins.type):
        DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor = ...
        ENCODING_UNSPECIFIED: AudioInConfig.Encoding.ValueType = ...  # 0
        """Not specified. Will return result [google.rpc.Code.INVALID_ARGUMENT][]."""

        LINEAR16: AudioInConfig.Encoding.ValueType = ...  # 1
        """Uncompressed 16-bit signed little-endian samples (Linear PCM).
        This encoding includes no header, only the raw audio bytes.
        """

        FLAC: AudioInConfig.Encoding.ValueType = ...  # 2
        """[`FLAC`](https://xiph.org/flac/documentation.html) (Free Lossless Audio
        Codec) is the recommended encoding because it is
        lossless--therefore recognition is not compromised--and
        requires only about half the bandwidth of `LINEAR16`. This encoding
        includes the `FLAC` stream header followed by audio data. It supports
        16-bit and 24-bit samples, however, not all fields in `STREAMINFO` are
        supported.
        """

    class Encoding(_Encoding, metaclass=_EncodingEnumTypeWrapper):
        """Audio encoding of the data sent in the audio message.
        Audio must be one-channel (mono). The only language supported is "en-US".
        """
        pass

    ENCODING_UNSPECIFIED: AudioInConfig.Encoding.ValueType = ...  # 0
    """Not specified. Will return result [google.rpc.Code.INVALID_ARGUMENT][]."""

    LINEAR16: AudioInConfig.Encoding.ValueType = ...  # 1
    """Uncompressed 16-bit signed little-endian samples (Linear PCM).
    This encoding includes no header, only the raw audio bytes.
    """

    FLAC: AudioInConfig.Encoding.ValueType = ...  # 2
    """[`FLAC`](https://xiph.org/flac/documentation.html) (Free Lossless Audio
    Codec) is the recommended encoding because it is
    lossless--therefore recognition is not compromised--and
    requires only about half the bandwidth of `LINEAR16`. This encoding
    includes the `FLAC` stream header followed by audio data. It supports
    16-bit and 24-bit samples, however, not all fields in `STREAMINFO` are
    supported.
    """


    ENCODING_FIELD_NUMBER: builtins.int
    SAMPLE_RATE_HERTZ_FIELD_NUMBER: builtins.int
    encoding: global___AudioInConfig.Encoding.ValueType = ...
    """*Required* Encoding of audio data sent in all `audio_in` messages."""

    sample_rate_hertz: builtins.int = ...
    """*Required* Sample rate (in Hertz) of the audio data sent in all `audio_in`
    messages. Valid values are from 16000-24000, but 16000 is optimal.
    For best results, set the sampling rate of the audio source to 16000 Hz.
    If that's not possible, use the native sample rate of the audio source
    (instead of re-sampling).
    """

    def __init__(self,
        *,
        encoding : global___AudioInConfig.Encoding.ValueType = ...,
        sample_rate_hertz : builtins.int = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["encoding",b"encoding","sample_rate_hertz",b"sample_rate_hertz"]) -> None: ...
global___AudioInConfig = AudioInConfig

class AudioOutConfig(google.protobuf.message.Message):
    """Specifies the desired format for the server to use when it returns
    `audio_out` messages.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class _Encoding:
        ValueType = typing.NewType('ValueType', builtins.int)
        V: typing_extensions.TypeAlias = ValueType
    class _EncodingEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_Encoding.ValueType], builtins.type):
        DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor = ...
        ENCODING_UNSPECIFIED: AudioOutConfig.Encoding.ValueType = ...  # 0
        """Not specified. Will return result [google.rpc.Code.INVALID_ARGUMENT][]."""

        LINEAR16: AudioOutConfig.Encoding.ValueType = ...  # 1
        """Uncompressed 16-bit signed little-endian samples (Linear PCM)."""

        MP3: AudioOutConfig.Encoding.ValueType = ...  # 2
        """MP3 audio encoding. The sample rate is encoded in the payload."""

        OPUS_IN_OGG: AudioOutConfig.Encoding.ValueType = ...  # 3
        """Opus-encoded audio wrapped in an ogg container. The result will be a
        file which can be played natively on Android and in some browsers (such
        as Chrome). The quality of the encoding is considerably higher than MP3
        while using the same bitrate. The sample rate is encoded in the payload.
        """

    class Encoding(_Encoding, metaclass=_EncodingEnumTypeWrapper):
        """Audio encoding of the data returned in the audio message. All encodings are
        raw audio bytes with no header, except as indicated below.
        """
        pass

    ENCODING_UNSPECIFIED: AudioOutConfig.Encoding.ValueType = ...  # 0
    """Not specified. Will return result [google.rpc.Code.INVALID_ARGUMENT][]."""

    LINEAR16: AudioOutConfig.Encoding.ValueType = ...  # 1
    """Uncompressed 16-bit signed little-endian samples (Linear PCM)."""

    MP3: AudioOutConfig.Encoding.ValueType = ...  # 2
    """MP3 audio encoding. The sample rate is encoded in the payload."""

    OPUS_IN_OGG: AudioOutConfig.Encoding.ValueType = ...  # 3
    """Opus-encoded audio wrapped in an ogg container. The result will be a
    file which can be played natively on Android and in some browsers (such
    as Chrome). The quality of the encoding is considerably higher than MP3
    while using the same bitrate. The sample rate is encoded in the payload.
    """


    ENCODING_FIELD_NUMBER: builtins.int
    SAMPLE_RATE_HERTZ_FIELD_NUMBER: builtins.int
    VOLUME_PERCENTAGE_FIELD_NUMBER: builtins.int
    encoding: global___AudioOutConfig.Encoding.ValueType = ...
    """*Required* The encoding of audio data to be returned in all `audio_out`
    messages.
    """

    sample_rate_hertz: builtins.int = ...
    """*Required* The sample rate in Hertz of the audio data returned in
    `audio_out` messages. Valid values are: 16000-24000.
    """

    volume_percentage: builtins.int = ...
    """*Required* Current volume setting of the device's audio output.
    Valid values are 1 to 100 (corresponding to 1% to 100%).
    """

    def __init__(self,
        *,
        encoding : global___AudioOutConfig.Encoding.ValueType = ...,
        sample_rate_hertz : builtins.int = ...,
        volume_percentage : builtins.int = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["encoding",b"encoding","sample_rate_hertz",b"sample_rate_hertz","volume_percentage",b"volume_percentage"]) -> None: ...
global___AudioOutConfig = AudioOutConfig

class ConverseState(google.protobuf.message.Message):
    """Provides information about the current dialog state."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    CONVERSATION_STATE_FIELD_NUMBER: builtins.int
    conversation_state: builtins.bytes = ...
    """*Required* The `conversation_state` value returned in the prior
    `ConverseResponse`. Omit (do not set the field) if there was no prior
    `ConverseResponse`. If there was a prior `ConverseResponse`, do not omit
    this field; doing so will end that conversation (and this new request will
    start a new conversation).
    """

    def __init__(self,
        *,
        conversation_state : builtins.bytes = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["conversation_state",b"conversation_state"]) -> None: ...
global___ConverseState = ConverseState

class AudioOut(google.protobuf.message.Message):
    """The audio containing the assistant's response to the query. Sequential chunks
    of audio data are received in sequential `ConverseResponse` messages.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    AUDIO_DATA_FIELD_NUMBER: builtins.int
    audio_data: builtins.bytes = ...
    """*Output-only* The audio data containing the assistant's response to the
    query. Sequential chunks of audio data are received in sequential
    `ConverseResponse` messages.
    """

    def __init__(self,
        *,
        audio_data : builtins.bytes = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["audio_data",b"audio_data"]) -> None: ...
global___AudioOut = AudioOut

class ConverseResult(google.protobuf.message.Message):
    """The semantic result for the user's spoken query."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class _MicrophoneMode:
        ValueType = typing.NewType('ValueType', builtins.int)
        V: typing_extensions.TypeAlias = ValueType
    class _MicrophoneModeEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_MicrophoneMode.ValueType], builtins.type):
        DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor = ...
        MICROPHONE_MODE_UNSPECIFIED: ConverseResult.MicrophoneMode.ValueType = ...  # 0
        """No mode specified."""

        CLOSE_MICROPHONE: ConverseResult.MicrophoneMode.ValueType = ...  # 1
        """The service is not expecting a follow-on question from the user.
        The microphone should remain off until the user re-activates it.
        """

        DIALOG_FOLLOW_ON: ConverseResult.MicrophoneMode.ValueType = ...  # 2
        """The service is expecting a follow-on question from the user. The
        microphone should be re-opened when the `AudioOut` playback completes
        (by starting a new `Converse` RPC call to send the new audio).
        """

    class MicrophoneMode(_MicrophoneMode, metaclass=_MicrophoneModeEnumTypeWrapper):
        """Possible states of the microphone after a `Converse` RPC completes."""
        pass

    MICROPHONE_MODE_UNSPECIFIED: ConverseResult.MicrophoneMode.ValueType = ...  # 0
    """No mode specified."""

    CLOSE_MICROPHONE: ConverseResult.MicrophoneMode.ValueType = ...  # 1
    """The service is not expecting a follow-on question from the user.
    The microphone should remain off until the user re-activates it.
    """

    DIALOG_FOLLOW_ON: ConverseResult.MicrophoneMode.ValueType = ...  # 2
    """The service is expecting a follow-on question from the user. The
    microphone should be re-opened when the `AudioOut` playback completes
    (by starting a new `Converse` RPC call to send the new audio).
    """


    SPOKEN_REQUEST_TEXT_FIELD_NUMBER: builtins.int
    SPOKEN_RESPONSE_TEXT_FIELD_NUMBER: builtins.int
    CONVERSATION_STATE_FIELD_NUMBER: builtins.int
    MICROPHONE_MODE_FIELD_NUMBER: builtins.int
    VOLUME_PERCENTAGE_FIELD_NUMBER: builtins.int
    spoken_request_text: typing.Text = ...
    """*Output-only* The recognized transcript of what the user said."""

    spoken_response_text: typing.Text = ...
    """*Output-only* The text of the assistant's spoken response. This is only
    returned for an IFTTT action.
    """

    conversation_state: builtins.bytes = ...
    """*Output-only* State information for subsequent `ConverseRequest`. This
    value should be saved in the client and returned in the
    `conversation_state` with the next `ConverseRequest`. (The client does not
    need to interpret or otherwise use this value.) There is no need to save
    this information across device restarts.
    """

    microphone_mode: global___ConverseResult.MicrophoneMode.ValueType = ...
    """*Output-only* Specifies the mode of the microphone after this `Converse`
    RPC is processed.
    """

    volume_percentage: builtins.int = ...
    """*Output-only* Updated volume level. The value will be 0 or omitted
    (indicating no change) unless a voice command such as "Increase the volume"
    or "Set volume level 4" was recognized, in which case the value will be
    between 1 and 100 (corresponding to the new volume level of 1% to 100%).
    Typically, a client should use this volume level when playing the
    `audio_out` data, and retain this value as the current volume level and
    supply it in the `AudioOutConfig` of the next `ConverseRequest`. (Some
    clients may also implement other ways to allow the current volume level to
    be changed, for example, by providing a knob that the user can turn.)
    """

    def __init__(self,
        *,
        spoken_request_text : typing.Text = ...,
        spoken_response_text : typing.Text = ...,
        conversation_state : builtins.bytes = ...,
        microphone_mode : global___ConverseResult.MicrophoneMode.ValueType = ...,
        volume_percentage : builtins.int = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["conversation_state",b"conversation_state","microphone_mode",b"microphone_mode","spoken_request_text",b"spoken_request_text","spoken_response_text",b"spoken_response_text","volume_percentage",b"volume_percentage"]) -> None: ...
global___ConverseResult = ConverseResult

class ConverseRequest(google.protobuf.message.Message):
    """The top-level message sent by the client. Clients must send at least two, and
    typically numerous `ConverseRequest` messages. The first message must
    contain a `config` message and must not contain `audio_in` data. All
    subsequent messages must contain `audio_in` data and must not contain a
    `config` message.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    CONFIG_FIELD_NUMBER: builtins.int
    AUDIO_IN_FIELD_NUMBER: builtins.int
    @property
    def config(self) -> global___ConverseConfig:
        """The `config` message provides information to the recognizer that
        specifies how to process the request.
        The first `ConverseRequest` message must contain a `config` message.
        """
        pass
    audio_in: builtins.bytes = ...
    """The audio data to be recognized. Sequential chunks of audio data are sent
    in sequential `ConverseRequest` messages. The first `ConverseRequest`
    message must not contain `audio_in` data and all subsequent
    `ConverseRequest` messages must contain `audio_in` data. The audio bytes
    must be encoded as specified in `AudioInConfig`.
    Audio must be sent at approximately real-time (16000 samples per second).
    An error will be returned if audio is sent significantly faster or
    slower.
    """

    def __init__(self,
        *,
        config : typing.Optional[global___ConverseConfig] = ...,
        audio_in : builtins.bytes = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["audio_in",b"audio_in","config",b"config","converse_request",b"converse_request"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["audio_in",b"audio_in","config",b"config","converse_request",b"converse_request"]) -> None: ...
    def WhichOneof(self, oneof_group: typing_extensions.Literal["converse_request",b"converse_request"]) -> typing.Optional[typing_extensions.Literal["config","audio_in"]]: ...
global___ConverseRequest = ConverseRequest

class ConverseResponse(google.protobuf.message.Message):
    """The top-level message received by the client. A series of one or more
    `ConverseResponse` messages are streamed back to the client.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class _EventType:
        ValueType = typing.NewType('ValueType', builtins.int)
        V: typing_extensions.TypeAlias = ValueType
    class _EventTypeEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_EventType.ValueType], builtins.type):
        DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor = ...
        EVENT_TYPE_UNSPECIFIED: ConverseResponse.EventType.ValueType = ...  # 0
        """No event specified."""

        END_OF_UTTERANCE: ConverseResponse.EventType.ValueType = ...  # 1
        """This event indicates that the server has detected the end of the user's
        speech utterance and expects no additional speech. Therefore, the server
        will not process additional audio (although it may subsequently return
        additional results). The client should stop sending additional audio
        data, half-close the gRPC connection, and wait for any additional results
        until the server closes the gRPC connection.
        """

    class EventType(_EventType, metaclass=_EventTypeEnumTypeWrapper):
        """Indicates the type of event."""
        pass

    EVENT_TYPE_UNSPECIFIED: ConverseResponse.EventType.ValueType = ...  # 0
    """No event specified."""

    END_OF_UTTERANCE: ConverseResponse.EventType.ValueType = ...  # 1
    """This event indicates that the server has detected the end of the user's
    speech utterance and expects no additional speech. Therefore, the server
    will not process additional audio (although it may subsequently return
    additional results). The client should stop sending additional audio
    data, half-close the gRPC connection, and wait for any additional results
    until the server closes the gRPC connection.
    """


    ERROR_FIELD_NUMBER: builtins.int
    EVENT_TYPE_FIELD_NUMBER: builtins.int
    AUDIO_OUT_FIELD_NUMBER: builtins.int
    RESULT_FIELD_NUMBER: builtins.int
    @property
    def error(self) -> google.rpc.status_pb2.Status:
        """*Output-only* If set, returns a [google.rpc.Status][google.rpc.Status]
        message that specifies the error for the operation. If an error occurs
        during processing, this message will be set and there will be no further
        messages sent.
        """
        pass
    event_type: global___ConverseResponse.EventType.ValueType = ...
    """*Output-only* Indicates the type of event."""

    @property
    def audio_out(self) -> global___AudioOut:
        """*Output-only* The audio containing the assistant's response to the query."""
        pass
    @property
    def result(self) -> global___ConverseResult:
        """*Output-only* The semantic result for the user's spoken query."""
        pass
    def __init__(self,
        *,
        error : typing.Optional[google.rpc.status_pb2.Status] = ...,
        event_type : global___ConverseResponse.EventType.ValueType = ...,
        audio_out : typing.Optional[global___AudioOut] = ...,
        result : typing.Optional[global___ConverseResult] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["audio_out",b"audio_out","converse_response",b"converse_response","error",b"error","event_type",b"event_type","result",b"result"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["audio_out",b"audio_out","converse_response",b"converse_response","error",b"error","event_type",b"event_type","result",b"result"]) -> None: ...
    def WhichOneof(self, oneof_group: typing_extensions.Literal["converse_response",b"converse_response"]) -> typing.Optional[typing_extensions.Literal["error","event_type","audio_out","result"]]: ...
global___ConverseResponse = ConverseResponse
