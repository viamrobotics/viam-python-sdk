"""
@generated by mypy-protobuf.  Do not edit manually!
isort:skip_file
"""
import builtins
import google.cloud.dataproc.v1.shared_pb2
import google.protobuf.descriptor
import google.protobuf.duration_pb2
import google.protobuf.field_mask_pb2
import google.protobuf.internal.containers
import google.protobuf.internal.enum_type_wrapper
import google.protobuf.message
import google.protobuf.timestamp_pb2
import typing
import typing_extensions

DESCRIPTOR: google.protobuf.descriptor.FileDescriptor = ...

class Cluster(google.protobuf.message.Message):
    """Describes the identifying information, config, and status of
    a Dataproc cluster
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class LabelsEntry(google.protobuf.message.Message):
        DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
        KEY_FIELD_NUMBER: builtins.int
        VALUE_FIELD_NUMBER: builtins.int
        key: typing.Text = ...
        value: typing.Text = ...
        def __init__(self,
            *,
            key : typing.Text = ...,
            value : typing.Text = ...,
            ) -> None: ...
        def ClearField(self, field_name: typing_extensions.Literal["key",b"key","value",b"value"]) -> None: ...

    PROJECT_ID_FIELD_NUMBER: builtins.int
    CLUSTER_NAME_FIELD_NUMBER: builtins.int
    CONFIG_FIELD_NUMBER: builtins.int
    LABELS_FIELD_NUMBER: builtins.int
    STATUS_FIELD_NUMBER: builtins.int
    STATUS_HISTORY_FIELD_NUMBER: builtins.int
    CLUSTER_UUID_FIELD_NUMBER: builtins.int
    METRICS_FIELD_NUMBER: builtins.int
    project_id: typing.Text = ...
    """Required. The Google Cloud Platform project ID that the cluster belongs to."""

    cluster_name: typing.Text = ...
    """Required. The cluster name. Cluster names within a project must be
    unique. Names of deleted clusters can be reused.
    """

    @property
    def config(self) -> global___ClusterConfig:
        """Optional. The cluster config for a cluster of Compute Engine Instances.
        Note that Dataproc may set default values, and values may change
        when clusters are updated.
        """
        pass
    @property
    def labels(self) -> google.protobuf.internal.containers.ScalarMap[typing.Text, typing.Text]:
        """Optional. The labels to associate with this cluster.
        Label **keys** must contain 1 to 63 characters, and must conform to
        [RFC 1035](https://www.ietf.org/rfc/rfc1035.txt).
        Label **values** may be empty, but, if present, must contain 1 to 63
        characters, and must conform to [RFC
        1035](https://www.ietf.org/rfc/rfc1035.txt). No more than 32 labels can be
        associated with a cluster.
        """
        pass
    @property
    def status(self) -> global___ClusterStatus:
        """Output only. Cluster status."""
        pass
    @property
    def status_history(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___ClusterStatus]:
        """Output only. The previous cluster status."""
        pass
    cluster_uuid: typing.Text = ...
    """Output only. A cluster UUID (Unique Universal Identifier). Dataproc
    generates this value when it creates the cluster.
    """

    @property
    def metrics(self) -> global___ClusterMetrics:
        """Output only. Contains cluster daemon metrics such as HDFS and YARN stats.

        **Beta Feature**: This report is available for testing purposes only. It
        may be changed before final release.
        """
        pass
    def __init__(self,
        *,
        project_id : typing.Text = ...,
        cluster_name : typing.Text = ...,
        config : typing.Optional[global___ClusterConfig] = ...,
        labels : typing.Optional[typing.Mapping[typing.Text, typing.Text]] = ...,
        status : typing.Optional[global___ClusterStatus] = ...,
        status_history : typing.Optional[typing.Iterable[global___ClusterStatus]] = ...,
        cluster_uuid : typing.Text = ...,
        metrics : typing.Optional[global___ClusterMetrics] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["config",b"config","metrics",b"metrics","status",b"status"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["cluster_name",b"cluster_name","cluster_uuid",b"cluster_uuid","config",b"config","labels",b"labels","metrics",b"metrics","project_id",b"project_id","status",b"status","status_history",b"status_history"]) -> None: ...
global___Cluster = Cluster

class ClusterConfig(google.protobuf.message.Message):
    """The cluster config."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    CONFIG_BUCKET_FIELD_NUMBER: builtins.int
    TEMP_BUCKET_FIELD_NUMBER: builtins.int
    GCE_CLUSTER_CONFIG_FIELD_NUMBER: builtins.int
    MASTER_CONFIG_FIELD_NUMBER: builtins.int
    WORKER_CONFIG_FIELD_NUMBER: builtins.int
    SECONDARY_WORKER_CONFIG_FIELD_NUMBER: builtins.int
    SOFTWARE_CONFIG_FIELD_NUMBER: builtins.int
    INITIALIZATION_ACTIONS_FIELD_NUMBER: builtins.int
    ENCRYPTION_CONFIG_FIELD_NUMBER: builtins.int
    AUTOSCALING_CONFIG_FIELD_NUMBER: builtins.int
    SECURITY_CONFIG_FIELD_NUMBER: builtins.int
    LIFECYCLE_CONFIG_FIELD_NUMBER: builtins.int
    ENDPOINT_CONFIG_FIELD_NUMBER: builtins.int
    METASTORE_CONFIG_FIELD_NUMBER: builtins.int
    GKE_CLUSTER_CONFIG_FIELD_NUMBER: builtins.int
    config_bucket: typing.Text = ...
    """Optional. A Cloud Storage bucket used to stage job
    dependencies, config files, and job driver console output.
    If you do not specify a staging bucket, Cloud
    Dataproc will determine a Cloud Storage location (US,
    ASIA, or EU) for your cluster's staging bucket according to the
    Compute Engine zone where your cluster is deployed, and then create
    and manage this project-level, per-location bucket (see
    [Dataproc staging and temp
    buckets](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)).
    **This field requires a Cloud Storage bucket name, not a `gs://...` URI to
    a Cloud Storage bucket.**
    """

    temp_bucket: typing.Text = ...
    """Optional. A Cloud Storage bucket used to store ephemeral cluster and jobs data,
    such as Spark and MapReduce history files.
    If you do not specify a temp bucket,
    Dataproc will determine a Cloud Storage location (US,
    ASIA, or EU) for your cluster's temp bucket according to the
    Compute Engine zone where your cluster is deployed, and then create
    and manage this project-level, per-location bucket. The default bucket has
    a TTL of 90 days, but you can use any TTL (or none) if you specify a
    bucket (see
    [Dataproc staging and temp
    buckets](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)).
    **This field requires a Cloud Storage bucket name, not a `gs://...` URI to
    a Cloud Storage bucket.**
    """

    @property
    def gce_cluster_config(self) -> global___GceClusterConfig:
        """Optional. The shared Compute Engine config settings for
        all instances in a cluster.
        """
        pass
    @property
    def master_config(self) -> global___InstanceGroupConfig:
        """Optional. The Compute Engine config settings for
        the cluster's master instance.
        """
        pass
    @property
    def worker_config(self) -> global___InstanceGroupConfig:
        """Optional. The Compute Engine config settings for
        the cluster's worker instances.
        """
        pass
    @property
    def secondary_worker_config(self) -> global___InstanceGroupConfig:
        """Optional. The Compute Engine config settings for
        a cluster's secondary worker instances
        """
        pass
    @property
    def software_config(self) -> global___SoftwareConfig:
        """Optional. The config settings for cluster software."""
        pass
    @property
    def initialization_actions(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___NodeInitializationAction]:
        """Optional. Commands to execute on each node after config is
        completed. By default, executables are run on master and all worker nodes.
        You can test a node's `role` metadata to run an executable on
        a master or worker node, as shown below using `curl` (you can also use
        `wget`):

            ROLE=$(curl -H Metadata-Flavor:Google
            http://metadata/computeMetadata/v1/instance/attributes/dataproc-role)
            if [[ "${ROLE}" == 'Master' ]]; then
              ... master specific actions ...
            else
              ... worker specific actions ...
            fi
        """
        pass
    @property
    def encryption_config(self) -> global___EncryptionConfig:
        """Optional. Encryption settings for the cluster."""
        pass
    @property
    def autoscaling_config(self) -> global___AutoscalingConfig:
        """Optional. Autoscaling config for the policy associated with the cluster.
        Cluster does not autoscale if this field is unset.
        """
        pass
    @property
    def security_config(self) -> global___SecurityConfig:
        """Optional. Security settings for the cluster."""
        pass
    @property
    def lifecycle_config(self) -> global___LifecycleConfig:
        """Optional. Lifecycle setting for the cluster."""
        pass
    @property
    def endpoint_config(self) -> global___EndpointConfig:
        """Optional. Port/endpoint configuration for this cluster"""
        pass
    @property
    def metastore_config(self) -> global___MetastoreConfig:
        """Optional. Metastore configuration."""
        pass
    @property
    def gke_cluster_config(self) -> global___GkeClusterConfig:
        """Optional. BETA. The Kubernetes Engine config for Dataproc clusters deployed to
        Kubernetes. Setting this is considered mutually exclusive with Compute
        Engine-based options such as `gce_cluster_config`, `master_config`,
        `worker_config`, `secondary_worker_config`, and `autoscaling_config`.
        """
        pass
    def __init__(self,
        *,
        config_bucket : typing.Text = ...,
        temp_bucket : typing.Text = ...,
        gce_cluster_config : typing.Optional[global___GceClusterConfig] = ...,
        master_config : typing.Optional[global___InstanceGroupConfig] = ...,
        worker_config : typing.Optional[global___InstanceGroupConfig] = ...,
        secondary_worker_config : typing.Optional[global___InstanceGroupConfig] = ...,
        software_config : typing.Optional[global___SoftwareConfig] = ...,
        initialization_actions : typing.Optional[typing.Iterable[global___NodeInitializationAction]] = ...,
        encryption_config : typing.Optional[global___EncryptionConfig] = ...,
        autoscaling_config : typing.Optional[global___AutoscalingConfig] = ...,
        security_config : typing.Optional[global___SecurityConfig] = ...,
        lifecycle_config : typing.Optional[global___LifecycleConfig] = ...,
        endpoint_config : typing.Optional[global___EndpointConfig] = ...,
        metastore_config : typing.Optional[global___MetastoreConfig] = ...,
        gke_cluster_config : typing.Optional[global___GkeClusterConfig] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["autoscaling_config",b"autoscaling_config","encryption_config",b"encryption_config","endpoint_config",b"endpoint_config","gce_cluster_config",b"gce_cluster_config","gke_cluster_config",b"gke_cluster_config","lifecycle_config",b"lifecycle_config","master_config",b"master_config","metastore_config",b"metastore_config","secondary_worker_config",b"secondary_worker_config","security_config",b"security_config","software_config",b"software_config","worker_config",b"worker_config"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["autoscaling_config",b"autoscaling_config","config_bucket",b"config_bucket","encryption_config",b"encryption_config","endpoint_config",b"endpoint_config","gce_cluster_config",b"gce_cluster_config","gke_cluster_config",b"gke_cluster_config","initialization_actions",b"initialization_actions","lifecycle_config",b"lifecycle_config","master_config",b"master_config","metastore_config",b"metastore_config","secondary_worker_config",b"secondary_worker_config","security_config",b"security_config","software_config",b"software_config","temp_bucket",b"temp_bucket","worker_config",b"worker_config"]) -> None: ...
global___ClusterConfig = ClusterConfig

class GkeClusterConfig(google.protobuf.message.Message):
    """The GKE config for this cluster."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class NamespacedGkeDeploymentTarget(google.protobuf.message.Message):
        """A full, namespace-isolated deployment target for an existing GKE cluster."""
        DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
        TARGET_GKE_CLUSTER_FIELD_NUMBER: builtins.int
        CLUSTER_NAMESPACE_FIELD_NUMBER: builtins.int
        target_gke_cluster: typing.Text = ...
        """Optional. The target GKE cluster to deploy to.
        Format: 'projects/{project}/locations/{location}/clusters/{cluster_id}'
        """

        cluster_namespace: typing.Text = ...
        """Optional. A namespace within the GKE cluster to deploy into."""

        def __init__(self,
            *,
            target_gke_cluster : typing.Text = ...,
            cluster_namespace : typing.Text = ...,
            ) -> None: ...
        def ClearField(self, field_name: typing_extensions.Literal["cluster_namespace",b"cluster_namespace","target_gke_cluster",b"target_gke_cluster"]) -> None: ...

    NAMESPACED_GKE_DEPLOYMENT_TARGET_FIELD_NUMBER: builtins.int
    @property
    def namespaced_gke_deployment_target(self) -> global___GkeClusterConfig.NamespacedGkeDeploymentTarget:
        """Optional. A target for the deployment."""
        pass
    def __init__(self,
        *,
        namespaced_gke_deployment_target : typing.Optional[global___GkeClusterConfig.NamespacedGkeDeploymentTarget] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["namespaced_gke_deployment_target",b"namespaced_gke_deployment_target"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["namespaced_gke_deployment_target",b"namespaced_gke_deployment_target"]) -> None: ...
global___GkeClusterConfig = GkeClusterConfig

class EndpointConfig(google.protobuf.message.Message):
    """Endpoint config for this cluster"""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class HttpPortsEntry(google.protobuf.message.Message):
        DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
        KEY_FIELD_NUMBER: builtins.int
        VALUE_FIELD_NUMBER: builtins.int
        key: typing.Text = ...
        value: typing.Text = ...
        def __init__(self,
            *,
            key : typing.Text = ...,
            value : typing.Text = ...,
            ) -> None: ...
        def ClearField(self, field_name: typing_extensions.Literal["key",b"key","value",b"value"]) -> None: ...

    HTTP_PORTS_FIELD_NUMBER: builtins.int
    ENABLE_HTTP_PORT_ACCESS_FIELD_NUMBER: builtins.int
    @property
    def http_ports(self) -> google.protobuf.internal.containers.ScalarMap[typing.Text, typing.Text]:
        """Output only. The map of port descriptions to URLs. Will only be populated
        if enable_http_port_access is true.
        """
        pass
    enable_http_port_access: builtins.bool = ...
    """Optional. If true, enable http access to specific ports on the cluster
    from external sources. Defaults to false.
    """

    def __init__(self,
        *,
        http_ports : typing.Optional[typing.Mapping[typing.Text, typing.Text]] = ...,
        enable_http_port_access : builtins.bool = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["enable_http_port_access",b"enable_http_port_access","http_ports",b"http_ports"]) -> None: ...
global___EndpointConfig = EndpointConfig

class AutoscalingConfig(google.protobuf.message.Message):
    """Autoscaling Policy config associated with the cluster."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    POLICY_URI_FIELD_NUMBER: builtins.int
    policy_uri: typing.Text = ...
    """Optional. The autoscaling policy used by the cluster.

    Only resource names including projectid and location (region) are valid.
    Examples:

    * `https://www.googleapis.com/compute/v1/projects/[project_id]/locations/[dataproc_region]/autoscalingPolicies/[policy_id]`
    * `projects/[project_id]/locations/[dataproc_region]/autoscalingPolicies/[policy_id]`

    Note that the policy must be in the same project and Dataproc region.
    """

    def __init__(self,
        *,
        policy_uri : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["policy_uri",b"policy_uri"]) -> None: ...
global___AutoscalingConfig = AutoscalingConfig

class EncryptionConfig(google.protobuf.message.Message):
    """Encryption settings for the cluster."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    GCE_PD_KMS_KEY_NAME_FIELD_NUMBER: builtins.int
    gce_pd_kms_key_name: typing.Text = ...
    """Optional. The Cloud KMS key name to use for PD disk encryption for all
    instances in the cluster.
    """

    def __init__(self,
        *,
        gce_pd_kms_key_name : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["gce_pd_kms_key_name",b"gce_pd_kms_key_name"]) -> None: ...
global___EncryptionConfig = EncryptionConfig

class GceClusterConfig(google.protobuf.message.Message):
    """Common config settings for resources of Compute Engine cluster
    instances, applicable to all instances in the cluster.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class _PrivateIpv6GoogleAccess:
        ValueType = typing.NewType('ValueType', builtins.int)
        V: typing_extensions.TypeAlias = ValueType
    class _PrivateIpv6GoogleAccessEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_PrivateIpv6GoogleAccess.ValueType], builtins.type):
        DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor = ...
        PRIVATE_IPV6_GOOGLE_ACCESS_UNSPECIFIED: GceClusterConfig.PrivateIpv6GoogleAccess.ValueType = ...  # 0
        """If unspecified, Compute Engine default behavior will apply, which
        is the same as [INHERIT_FROM_SUBNETWORK][google.cloud.dataproc.v1.GceClusterConfig.PrivateIpv6GoogleAccess.INHERIT_FROM_SUBNETWORK].
        """

        INHERIT_FROM_SUBNETWORK: GceClusterConfig.PrivateIpv6GoogleAccess.ValueType = ...  # 1
        """Private access to and from Google Services configuration
        inherited from the subnetwork configuration. This is the
        default Compute Engine behavior.
        """

        OUTBOUND: GceClusterConfig.PrivateIpv6GoogleAccess.ValueType = ...  # 2
        """Enables outbound private IPv6 access to Google Services from the Dataproc
        cluster.
        """

        BIDIRECTIONAL: GceClusterConfig.PrivateIpv6GoogleAccess.ValueType = ...  # 3
        """Enables bidirectional private IPv6 access between Google Services and the
        Dataproc cluster.
        """

    class PrivateIpv6GoogleAccess(_PrivateIpv6GoogleAccess, metaclass=_PrivateIpv6GoogleAccessEnumTypeWrapper):
        """`PrivateIpv6GoogleAccess` controls whether and how Dataproc cluster nodes
        can communicate with Google Services through gRPC over IPv6.
        These values are directly mapped to corresponding values in the
        [Compute Engine Instance
        fields](https://cloud.google.com/compute/docs/reference/rest/v1/instances).
        """
        pass

    PRIVATE_IPV6_GOOGLE_ACCESS_UNSPECIFIED: GceClusterConfig.PrivateIpv6GoogleAccess.ValueType = ...  # 0
    """If unspecified, Compute Engine default behavior will apply, which
    is the same as [INHERIT_FROM_SUBNETWORK][google.cloud.dataproc.v1.GceClusterConfig.PrivateIpv6GoogleAccess.INHERIT_FROM_SUBNETWORK].
    """

    INHERIT_FROM_SUBNETWORK: GceClusterConfig.PrivateIpv6GoogleAccess.ValueType = ...  # 1
    """Private access to and from Google Services configuration
    inherited from the subnetwork configuration. This is the
    default Compute Engine behavior.
    """

    OUTBOUND: GceClusterConfig.PrivateIpv6GoogleAccess.ValueType = ...  # 2
    """Enables outbound private IPv6 access to Google Services from the Dataproc
    cluster.
    """

    BIDIRECTIONAL: GceClusterConfig.PrivateIpv6GoogleAccess.ValueType = ...  # 3
    """Enables bidirectional private IPv6 access between Google Services and the
    Dataproc cluster.
    """


    class MetadataEntry(google.protobuf.message.Message):
        DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
        KEY_FIELD_NUMBER: builtins.int
        VALUE_FIELD_NUMBER: builtins.int
        key: typing.Text = ...
        value: typing.Text = ...
        def __init__(self,
            *,
            key : typing.Text = ...,
            value : typing.Text = ...,
            ) -> None: ...
        def ClearField(self, field_name: typing_extensions.Literal["key",b"key","value",b"value"]) -> None: ...

    ZONE_URI_FIELD_NUMBER: builtins.int
    NETWORK_URI_FIELD_NUMBER: builtins.int
    SUBNETWORK_URI_FIELD_NUMBER: builtins.int
    INTERNAL_IP_ONLY_FIELD_NUMBER: builtins.int
    PRIVATE_IPV6_GOOGLE_ACCESS_FIELD_NUMBER: builtins.int
    SERVICE_ACCOUNT_FIELD_NUMBER: builtins.int
    SERVICE_ACCOUNT_SCOPES_FIELD_NUMBER: builtins.int
    TAGS_FIELD_NUMBER: builtins.int
    METADATA_FIELD_NUMBER: builtins.int
    RESERVATION_AFFINITY_FIELD_NUMBER: builtins.int
    NODE_GROUP_AFFINITY_FIELD_NUMBER: builtins.int
    SHIELDED_INSTANCE_CONFIG_FIELD_NUMBER: builtins.int
    CONFIDENTIAL_INSTANCE_CONFIG_FIELD_NUMBER: builtins.int
    zone_uri: typing.Text = ...
    """Optional. The zone where the Compute Engine cluster will be located.
    On a create request, it is required in the "global" region. If omitted
    in a non-global Dataproc region, the service will pick a zone in the
    corresponding Compute Engine region. On a get request, zone will
    always be present.

    A full URL, partial URI, or short name are valid. Examples:

    * `https://www.googleapis.com/compute/v1/projects/[project_id]/zones/[zone]`
    * `projects/[project_id]/zones/[zone]`
    * `us-central1-f`
    """

    network_uri: typing.Text = ...
    """Optional. The Compute Engine network to be used for machine
    communications. Cannot be specified with subnetwork_uri. If neither
    `network_uri` nor `subnetwork_uri` is specified, the "default" network of
    the project is used, if it exists. Cannot be a "Custom Subnet Network" (see
    [Using Subnetworks](https://cloud.google.com/compute/docs/subnetworks) for
    more information).

    A full URL, partial URI, or short name are valid. Examples:

    * `https://www.googleapis.com/compute/v1/projects/[project_id]/regions/global/default`
    * `projects/[project_id]/regions/global/default`
    * `default`
    """

    subnetwork_uri: typing.Text = ...
    """Optional. The Compute Engine subnetwork to be used for machine
    communications. Cannot be specified with network_uri.

    A full URL, partial URI, or short name are valid. Examples:

    * `https://www.googleapis.com/compute/v1/projects/[project_id]/regions/us-east1/subnetworks/sub0`
    * `projects/[project_id]/regions/us-east1/subnetworks/sub0`
    * `sub0`
    """

    internal_ip_only: builtins.bool = ...
    """Optional. If true, all instances in the cluster will only have internal IP
    addresses. By default, clusters are not restricted to internal IP
    addresses, and will have ephemeral external IP addresses assigned to each
    instance. This `internal_ip_only` restriction can only be enabled for
    subnetwork enabled networks, and all off-cluster dependencies must be
    configured to be accessible without external IP addresses.
    """

    private_ipv6_google_access: global___GceClusterConfig.PrivateIpv6GoogleAccess.ValueType = ...
    """Optional. The type of IPv6 access for a cluster."""

    service_account: typing.Text = ...
    """Optional. The [Dataproc service
    account](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/service-accounts#service_accounts_in_dataproc)
    (also see [VM Data Plane
    identity](https://cloud.google.com/dataproc/docs/concepts/iam/dataproc-principals#vm_service_account_data_plane_identity))
    used by Dataproc cluster VM instances to access Google Cloud Platform
    services.

    If not specified, the
    [Compute Engine default service
    account](https://cloud.google.com/compute/docs/access/service-accounts#default_service_account)
    is used.
    """

    @property
    def service_account_scopes(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """Optional. The URIs of service account scopes to be included in
        Compute Engine instances. The following base set of scopes is always
        included:

        * https://www.googleapis.com/auth/cloud.useraccounts.readonly
        * https://www.googleapis.com/auth/devstorage.read_write
        * https://www.googleapis.com/auth/logging.write

        If no scopes are specified, the following defaults are also provided:

        * https://www.googleapis.com/auth/bigquery
        * https://www.googleapis.com/auth/bigtable.admin.table
        * https://www.googleapis.com/auth/bigtable.data
        * https://www.googleapis.com/auth/devstorage.full_control
        """
        pass
    @property
    def tags(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """The Compute Engine tags to add to all instances (see [Tagging
        instances](https://cloud.google.com/compute/docs/label-or-tag-resources#tags)).
        """
        pass
    @property
    def metadata(self) -> google.protobuf.internal.containers.ScalarMap[typing.Text, typing.Text]:
        """The Compute Engine metadata entries to add to all instances (see
        [Project and instance
        metadata](https://cloud.google.com/compute/docs/storing-retrieving-metadata#project_and_instance_metadata)).
        """
        pass
    @property
    def reservation_affinity(self) -> global___ReservationAffinity:
        """Optional. Reservation Affinity for consuming Zonal reservation."""
        pass
    @property
    def node_group_affinity(self) -> global___NodeGroupAffinity:
        """Optional. Node Group Affinity for sole-tenant clusters."""
        pass
    @property
    def shielded_instance_config(self) -> global___ShieldedInstanceConfig:
        """Optional. Shielded Instance Config for clusters using [Compute Engine Shielded
        VMs](https://cloud.google.com/security/shielded-cloud/shielded-vm).
        """
        pass
    @property
    def confidential_instance_config(self) -> global___ConfidentialInstanceConfig:
        """Optional. Confidential Instance Config for clusters using [Confidential
        VMs](https://cloud.google.com/compute/confidential-vm/docs).
        """
        pass
    def __init__(self,
        *,
        zone_uri : typing.Text = ...,
        network_uri : typing.Text = ...,
        subnetwork_uri : typing.Text = ...,
        internal_ip_only : builtins.bool = ...,
        private_ipv6_google_access : global___GceClusterConfig.PrivateIpv6GoogleAccess.ValueType = ...,
        service_account : typing.Text = ...,
        service_account_scopes : typing.Optional[typing.Iterable[typing.Text]] = ...,
        tags : typing.Optional[typing.Iterable[typing.Text]] = ...,
        metadata : typing.Optional[typing.Mapping[typing.Text, typing.Text]] = ...,
        reservation_affinity : typing.Optional[global___ReservationAffinity] = ...,
        node_group_affinity : typing.Optional[global___NodeGroupAffinity] = ...,
        shielded_instance_config : typing.Optional[global___ShieldedInstanceConfig] = ...,
        confidential_instance_config : typing.Optional[global___ConfidentialInstanceConfig] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["confidential_instance_config",b"confidential_instance_config","node_group_affinity",b"node_group_affinity","reservation_affinity",b"reservation_affinity","shielded_instance_config",b"shielded_instance_config"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["confidential_instance_config",b"confidential_instance_config","internal_ip_only",b"internal_ip_only","metadata",b"metadata","network_uri",b"network_uri","node_group_affinity",b"node_group_affinity","private_ipv6_google_access",b"private_ipv6_google_access","reservation_affinity",b"reservation_affinity","service_account",b"service_account","service_account_scopes",b"service_account_scopes","shielded_instance_config",b"shielded_instance_config","subnetwork_uri",b"subnetwork_uri","tags",b"tags","zone_uri",b"zone_uri"]) -> None: ...
global___GceClusterConfig = GceClusterConfig

class NodeGroupAffinity(google.protobuf.message.Message):
    """Node Group Affinity for clusters using sole-tenant node groups."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    NODE_GROUP_URI_FIELD_NUMBER: builtins.int
    node_group_uri: typing.Text = ...
    """Required. The URI of a
    sole-tenant [node group
    resource](https://cloud.google.com/compute/docs/reference/rest/v1/nodeGroups)
    that the cluster will be created on.

    A full URL, partial URI, or node group name are valid. Examples:

    * `https://www.googleapis.com/compute/v1/projects/[project_id]/zones/us-central1-a/nodeGroups/node-group-1`
    * `projects/[project_id]/zones/us-central1-a/nodeGroups/node-group-1`
    * `node-group-1`
    """

    def __init__(self,
        *,
        node_group_uri : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["node_group_uri",b"node_group_uri"]) -> None: ...
global___NodeGroupAffinity = NodeGroupAffinity

class ShieldedInstanceConfig(google.protobuf.message.Message):
    """Shielded Instance Config for clusters using [Compute Engine Shielded
    VMs](https://cloud.google.com/security/shielded-cloud/shielded-vm).
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    ENABLE_SECURE_BOOT_FIELD_NUMBER: builtins.int
    ENABLE_VTPM_FIELD_NUMBER: builtins.int
    ENABLE_INTEGRITY_MONITORING_FIELD_NUMBER: builtins.int
    enable_secure_boot: builtins.bool = ...
    """Optional. Defines whether instances have Secure Boot enabled."""

    enable_vtpm: builtins.bool = ...
    """Optional. Defines whether instances have the vTPM enabled."""

    enable_integrity_monitoring: builtins.bool = ...
    """Optional. Defines whether instances have integrity monitoring enabled."""

    def __init__(self,
        *,
        enable_secure_boot : builtins.bool = ...,
        enable_vtpm : builtins.bool = ...,
        enable_integrity_monitoring : builtins.bool = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["enable_integrity_monitoring",b"enable_integrity_monitoring","enable_secure_boot",b"enable_secure_boot","enable_vtpm",b"enable_vtpm"]) -> None: ...
global___ShieldedInstanceConfig = ShieldedInstanceConfig

class ConfidentialInstanceConfig(google.protobuf.message.Message):
    """Confidential Instance Config for clusters using [Confidential
    VMs](https://cloud.google.com/compute/confidential-vm/docs)
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    ENABLE_CONFIDENTIAL_COMPUTE_FIELD_NUMBER: builtins.int
    enable_confidential_compute: builtins.bool = ...
    """Optional. Defines whether the instance should have confidential compute enabled."""

    def __init__(self,
        *,
        enable_confidential_compute : builtins.bool = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["enable_confidential_compute",b"enable_confidential_compute"]) -> None: ...
global___ConfidentialInstanceConfig = ConfidentialInstanceConfig

class InstanceGroupConfig(google.protobuf.message.Message):
    """The config settings for Compute Engine resources in
    an instance group, such as a master or worker group.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class _Preemptibility:
        ValueType = typing.NewType('ValueType', builtins.int)
        V: typing_extensions.TypeAlias = ValueType
    class _PreemptibilityEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_Preemptibility.ValueType], builtins.type):
        DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor = ...
        PREEMPTIBILITY_UNSPECIFIED: InstanceGroupConfig.Preemptibility.ValueType = ...  # 0
        """Preemptibility is unspecified, the system will choose the
        appropriate setting for each instance group.
        """

        NON_PREEMPTIBLE: InstanceGroupConfig.Preemptibility.ValueType = ...  # 1
        """Instances are non-preemptible.

        This option is allowed for all instance groups and is the only valid
        value for Master and Worker instance groups.
        """

        PREEMPTIBLE: InstanceGroupConfig.Preemptibility.ValueType = ...  # 2
        """Instances are preemptible.

        This option is allowed only for secondary worker groups.
        """

    class Preemptibility(_Preemptibility, metaclass=_PreemptibilityEnumTypeWrapper):
        """Controls the use of
        [preemptible instances]
        (https://cloud.google.com/compute/docs/instances/preemptible)
        within the group.
        """
        pass

    PREEMPTIBILITY_UNSPECIFIED: InstanceGroupConfig.Preemptibility.ValueType = ...  # 0
    """Preemptibility is unspecified, the system will choose the
    appropriate setting for each instance group.
    """

    NON_PREEMPTIBLE: InstanceGroupConfig.Preemptibility.ValueType = ...  # 1
    """Instances are non-preemptible.

    This option is allowed for all instance groups and is the only valid
    value for Master and Worker instance groups.
    """

    PREEMPTIBLE: InstanceGroupConfig.Preemptibility.ValueType = ...  # 2
    """Instances are preemptible.

    This option is allowed only for secondary worker groups.
    """


    NUM_INSTANCES_FIELD_NUMBER: builtins.int
    INSTANCE_NAMES_FIELD_NUMBER: builtins.int
    IMAGE_URI_FIELD_NUMBER: builtins.int
    MACHINE_TYPE_URI_FIELD_NUMBER: builtins.int
    DISK_CONFIG_FIELD_NUMBER: builtins.int
    IS_PREEMPTIBLE_FIELD_NUMBER: builtins.int
    PREEMPTIBILITY_FIELD_NUMBER: builtins.int
    MANAGED_GROUP_CONFIG_FIELD_NUMBER: builtins.int
    ACCELERATORS_FIELD_NUMBER: builtins.int
    MIN_CPU_PLATFORM_FIELD_NUMBER: builtins.int
    num_instances: builtins.int = ...
    """Optional. The number of VM instances in the instance group.
    For [HA
    cluster](/dataproc/docs/concepts/configuring-clusters/high-availability)
    [master_config](#FIELDS.master_config) groups, **must be set to 3**.
    For standard cluster [master_config](#FIELDS.master_config) groups,
    **must be set to 1**.
    """

    @property
    def instance_names(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """Output only. The list of instance names. Dataproc derives the names
        from `cluster_name`, `num_instances`, and the instance group.
        """
        pass
    image_uri: typing.Text = ...
    """Optional. The Compute Engine image resource used for cluster instances.

    The URI can represent an image or image family.

    Image examples:

    * `https://www.googleapis.com/compute/beta/projects/[project_id]/global/images/[image-id]`
    * `projects/[project_id]/global/images/[image-id]`
    * `image-id`

    Image family examples. Dataproc will use the most recent
    image from the family:

    * `https://www.googleapis.com/compute/beta/projects/[project_id]/global/images/family/[custom-image-family-name]`
    * `projects/[project_id]/global/images/family/[custom-image-family-name]`

    If the URI is unspecified, it will be inferred from
    `SoftwareConfig.image_version` or the system default.
    """

    machine_type_uri: typing.Text = ...
    """Optional. The Compute Engine machine type used for cluster instances.

    A full URL, partial URI, or short name are valid. Examples:

    * `https://www.googleapis.com/compute/v1/projects/[project_id]/zones/us-east1-a/machineTypes/n1-standard-2`
    * `projects/[project_id]/zones/us-east1-a/machineTypes/n1-standard-2`
    * `n1-standard-2`

    **Auto Zone Exception**: If you are using the Dataproc
    [Auto Zone
    Placement](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement)
    feature, you must use the short name of the machine type
    resource, for example, `n1-standard-2`.
    """

    @property
    def disk_config(self) -> global___DiskConfig:
        """Optional. Disk option config settings."""
        pass
    is_preemptible: builtins.bool = ...
    """Output only. Specifies that this instance group contains preemptible
    instances.
    """

    preemptibility: global___InstanceGroupConfig.Preemptibility.ValueType = ...
    """Optional. Specifies the preemptibility of the instance group.

    The default value for master and worker groups is
    `NON_PREEMPTIBLE`. This default cannot be changed.

    The default value for secondary instances is
    `PREEMPTIBLE`.
    """

    @property
    def managed_group_config(self) -> global___ManagedGroupConfig:
        """Output only. The config for Compute Engine Instance Group
        Manager that manages this group.
        This is only used for preemptible instance groups.
        """
        pass
    @property
    def accelerators(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___AcceleratorConfig]:
        """Optional. The Compute Engine accelerator configuration for these
        instances.
        """
        pass
    min_cpu_platform: typing.Text = ...
    """Optional. Specifies the minimum cpu platform for the Instance Group.
    See [Dataproc -> Minimum CPU
    Platform](https://cloud.google.com/dataproc/docs/concepts/compute/dataproc-min-cpu).
    """

    def __init__(self,
        *,
        num_instances : builtins.int = ...,
        instance_names : typing.Optional[typing.Iterable[typing.Text]] = ...,
        image_uri : typing.Text = ...,
        machine_type_uri : typing.Text = ...,
        disk_config : typing.Optional[global___DiskConfig] = ...,
        is_preemptible : builtins.bool = ...,
        preemptibility : global___InstanceGroupConfig.Preemptibility.ValueType = ...,
        managed_group_config : typing.Optional[global___ManagedGroupConfig] = ...,
        accelerators : typing.Optional[typing.Iterable[global___AcceleratorConfig]] = ...,
        min_cpu_platform : typing.Text = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["disk_config",b"disk_config","managed_group_config",b"managed_group_config"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["accelerators",b"accelerators","disk_config",b"disk_config","image_uri",b"image_uri","instance_names",b"instance_names","is_preemptible",b"is_preemptible","machine_type_uri",b"machine_type_uri","managed_group_config",b"managed_group_config","min_cpu_platform",b"min_cpu_platform","num_instances",b"num_instances","preemptibility",b"preemptibility"]) -> None: ...
global___InstanceGroupConfig = InstanceGroupConfig

class ManagedGroupConfig(google.protobuf.message.Message):
    """Specifies the resources used to actively manage an instance group."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    INSTANCE_TEMPLATE_NAME_FIELD_NUMBER: builtins.int
    INSTANCE_GROUP_MANAGER_NAME_FIELD_NUMBER: builtins.int
    instance_template_name: typing.Text = ...
    """Output only. The name of the Instance Template used for the Managed
    Instance Group.
    """

    instance_group_manager_name: typing.Text = ...
    """Output only. The name of the Instance Group Manager for this group."""

    def __init__(self,
        *,
        instance_template_name : typing.Text = ...,
        instance_group_manager_name : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["instance_group_manager_name",b"instance_group_manager_name","instance_template_name",b"instance_template_name"]) -> None: ...
global___ManagedGroupConfig = ManagedGroupConfig

class AcceleratorConfig(google.protobuf.message.Message):
    """Specifies the type and number of accelerator cards attached to the instances
    of an instance. See [GPUs on Compute
    Engine](https://cloud.google.com/compute/docs/gpus/).
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    ACCELERATOR_TYPE_URI_FIELD_NUMBER: builtins.int
    ACCELERATOR_COUNT_FIELD_NUMBER: builtins.int
    accelerator_type_uri: typing.Text = ...
    """Full URL, partial URI, or short name of the accelerator type resource to
    expose to this instance. See
    [Compute Engine
    AcceleratorTypes](https://cloud.google.com/compute/docs/reference/beta/acceleratorTypes).

    Examples:

    * `https://www.googleapis.com/compute/beta/projects/[project_id]/zones/us-east1-a/acceleratorTypes/nvidia-tesla-k80`
    * `projects/[project_id]/zones/us-east1-a/acceleratorTypes/nvidia-tesla-k80`
    * `nvidia-tesla-k80`

    **Auto Zone Exception**: If you are using the Dataproc
    [Auto Zone
    Placement](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement)
    feature, you must use the short name of the accelerator type
    resource, for example, `nvidia-tesla-k80`.
    """

    accelerator_count: builtins.int = ...
    """The number of the accelerator cards of this type exposed to this instance."""

    def __init__(self,
        *,
        accelerator_type_uri : typing.Text = ...,
        accelerator_count : builtins.int = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["accelerator_count",b"accelerator_count","accelerator_type_uri",b"accelerator_type_uri"]) -> None: ...
global___AcceleratorConfig = AcceleratorConfig

class DiskConfig(google.protobuf.message.Message):
    """Specifies the config of disk options for a group of VM instances."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    BOOT_DISK_TYPE_FIELD_NUMBER: builtins.int
    BOOT_DISK_SIZE_GB_FIELD_NUMBER: builtins.int
    NUM_LOCAL_SSDS_FIELD_NUMBER: builtins.int
    LOCAL_SSD_INTERFACE_FIELD_NUMBER: builtins.int
    boot_disk_type: typing.Text = ...
    """Optional. Type of the boot disk (default is "pd-standard").
    Valid values: "pd-balanced" (Persistent Disk Balanced Solid State Drive),
    "pd-ssd" (Persistent Disk Solid State Drive),
    or "pd-standard" (Persistent Disk Hard Disk Drive).
    See [Disk types](https://cloud.google.com/compute/docs/disks#disk-types).
    """

    boot_disk_size_gb: builtins.int = ...
    """Optional. Size in GB of the boot disk (default is 500GB)."""

    num_local_ssds: builtins.int = ...
    """Optional. Number of attached SSDs, from 0 to 4 (default is 0).
    If SSDs are not attached, the boot disk is used to store runtime logs and
    [HDFS](https://hadoop.apache.org/docs/r1.2.1/hdfs_user_guide.html) data.
    If one or more SSDs are attached, this runtime bulk
    data is spread across them, and the boot disk contains only basic
    config and installed binaries.
    """

    local_ssd_interface: typing.Text = ...
    """Optional. Interface type of local SSDs (default is "scsi").
    Valid values: "scsi" (Small Computer System Interface),
    "nvme" (Non-Volatile Memory Express).
    See [SSD Interface
    types](https://cloud.google.com/compute/docs/disks/local-ssd#performance).
    """

    def __init__(self,
        *,
        boot_disk_type : typing.Text = ...,
        boot_disk_size_gb : builtins.int = ...,
        num_local_ssds : builtins.int = ...,
        local_ssd_interface : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["boot_disk_size_gb",b"boot_disk_size_gb","boot_disk_type",b"boot_disk_type","local_ssd_interface",b"local_ssd_interface","num_local_ssds",b"num_local_ssds"]) -> None: ...
global___DiskConfig = DiskConfig

class NodeInitializationAction(google.protobuf.message.Message):
    """Specifies an executable to run on a fully configured node and a
    timeout period for executable completion.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    EXECUTABLE_FILE_FIELD_NUMBER: builtins.int
    EXECUTION_TIMEOUT_FIELD_NUMBER: builtins.int
    executable_file: typing.Text = ...
    """Required. Cloud Storage URI of executable file."""

    @property
    def execution_timeout(self) -> google.protobuf.duration_pb2.Duration:
        """Optional. Amount of time executable has to complete. Default is
        10 minutes (see JSON representation of
        [Duration](https://developers.google.com/protocol-buffers/docs/proto3#json)).

        Cluster creation fails with an explanatory error message (the
        name of the executable that caused the error and the exceeded timeout
        period) if the executable is not completed at end of the timeout period.
        """
        pass
    def __init__(self,
        *,
        executable_file : typing.Text = ...,
        execution_timeout : typing.Optional[google.protobuf.duration_pb2.Duration] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["execution_timeout",b"execution_timeout"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["executable_file",b"executable_file","execution_timeout",b"execution_timeout"]) -> None: ...
global___NodeInitializationAction = NodeInitializationAction

class ClusterStatus(google.protobuf.message.Message):
    """The status of a cluster and its instances."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class _State:
        ValueType = typing.NewType('ValueType', builtins.int)
        V: typing_extensions.TypeAlias = ValueType
    class _StateEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_State.ValueType], builtins.type):
        DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor = ...
        UNKNOWN: ClusterStatus.State.ValueType = ...  # 0
        """The cluster state is unknown."""

        CREATING: ClusterStatus.State.ValueType = ...  # 1
        """The cluster is being created and set up. It is not ready for use."""

        RUNNING: ClusterStatus.State.ValueType = ...  # 2
        """The cluster is currently running and healthy. It is ready for use."""

        ERROR: ClusterStatus.State.ValueType = ...  # 3
        """The cluster encountered an error. It is not ready for use."""

        ERROR_DUE_TO_UPDATE: ClusterStatus.State.ValueType = ...  # 9
        """The cluster has encountered an error while being updated. Jobs can
        be submitted to the cluster, but the cluster cannot be updated.
        """

        DELETING: ClusterStatus.State.ValueType = ...  # 4
        """The cluster is being deleted. It cannot be used."""

        UPDATING: ClusterStatus.State.ValueType = ...  # 5
        """The cluster is being updated. It continues to accept and process jobs."""

        STOPPING: ClusterStatus.State.ValueType = ...  # 6
        """The cluster is being stopped. It cannot be used."""

        STOPPED: ClusterStatus.State.ValueType = ...  # 7
        """The cluster is currently stopped. It is not ready for use."""

        STARTING: ClusterStatus.State.ValueType = ...  # 8
        """The cluster is being started. It is not ready for use."""

    class State(_State, metaclass=_StateEnumTypeWrapper):
        """The cluster state."""
        pass

    UNKNOWN: ClusterStatus.State.ValueType = ...  # 0
    """The cluster state is unknown."""

    CREATING: ClusterStatus.State.ValueType = ...  # 1
    """The cluster is being created and set up. It is not ready for use."""

    RUNNING: ClusterStatus.State.ValueType = ...  # 2
    """The cluster is currently running and healthy. It is ready for use."""

    ERROR: ClusterStatus.State.ValueType = ...  # 3
    """The cluster encountered an error. It is not ready for use."""

    ERROR_DUE_TO_UPDATE: ClusterStatus.State.ValueType = ...  # 9
    """The cluster has encountered an error while being updated. Jobs can
    be submitted to the cluster, but the cluster cannot be updated.
    """

    DELETING: ClusterStatus.State.ValueType = ...  # 4
    """The cluster is being deleted. It cannot be used."""

    UPDATING: ClusterStatus.State.ValueType = ...  # 5
    """The cluster is being updated. It continues to accept and process jobs."""

    STOPPING: ClusterStatus.State.ValueType = ...  # 6
    """The cluster is being stopped. It cannot be used."""

    STOPPED: ClusterStatus.State.ValueType = ...  # 7
    """The cluster is currently stopped. It is not ready for use."""

    STARTING: ClusterStatus.State.ValueType = ...  # 8
    """The cluster is being started. It is not ready for use."""


    class _Substate:
        ValueType = typing.NewType('ValueType', builtins.int)
        V: typing_extensions.TypeAlias = ValueType
    class _SubstateEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_Substate.ValueType], builtins.type):
        DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor = ...
        UNSPECIFIED: ClusterStatus.Substate.ValueType = ...  # 0
        """The cluster substate is unknown."""

        UNHEALTHY: ClusterStatus.Substate.ValueType = ...  # 1
        """The cluster is known to be in an unhealthy state
        (for example, critical daemons are not running or HDFS capacity is
        exhausted).

        Applies to RUNNING state.
        """

        STALE_STATUS: ClusterStatus.Substate.ValueType = ...  # 2
        """The agent-reported status is out of date (may occur if
        Dataproc loses communication with Agent).

        Applies to RUNNING state.
        """

    class Substate(_Substate, metaclass=_SubstateEnumTypeWrapper):
        """The cluster substate."""
        pass

    UNSPECIFIED: ClusterStatus.Substate.ValueType = ...  # 0
    """The cluster substate is unknown."""

    UNHEALTHY: ClusterStatus.Substate.ValueType = ...  # 1
    """The cluster is known to be in an unhealthy state
    (for example, critical daemons are not running or HDFS capacity is
    exhausted).

    Applies to RUNNING state.
    """

    STALE_STATUS: ClusterStatus.Substate.ValueType = ...  # 2
    """The agent-reported status is out of date (may occur if
    Dataproc loses communication with Agent).

    Applies to RUNNING state.
    """


    STATE_FIELD_NUMBER: builtins.int
    DETAIL_FIELD_NUMBER: builtins.int
    STATE_START_TIME_FIELD_NUMBER: builtins.int
    SUBSTATE_FIELD_NUMBER: builtins.int
    state: global___ClusterStatus.State.ValueType = ...
    """Output only. The cluster's state."""

    detail: typing.Text = ...
    """Optional. Output only. Details of cluster's state."""

    @property
    def state_start_time(self) -> google.protobuf.timestamp_pb2.Timestamp:
        """Output only. Time when this state was entered (see JSON representation of
        [Timestamp](https://developers.google.com/protocol-buffers/docs/proto3#json)).
        """
        pass
    substate: global___ClusterStatus.Substate.ValueType = ...
    """Output only. Additional state information that includes
    status reported by the agent.
    """

    def __init__(self,
        *,
        state : global___ClusterStatus.State.ValueType = ...,
        detail : typing.Text = ...,
        state_start_time : typing.Optional[google.protobuf.timestamp_pb2.Timestamp] = ...,
        substate : global___ClusterStatus.Substate.ValueType = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["state_start_time",b"state_start_time"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["detail",b"detail","state",b"state","state_start_time",b"state_start_time","substate",b"substate"]) -> None: ...
global___ClusterStatus = ClusterStatus

class SecurityConfig(google.protobuf.message.Message):
    """Security related configuration, including encryption, Kerberos, etc."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    KERBEROS_CONFIG_FIELD_NUMBER: builtins.int
    IDENTITY_CONFIG_FIELD_NUMBER: builtins.int
    @property
    def kerberos_config(self) -> global___KerberosConfig:
        """Optional. Kerberos related configuration."""
        pass
    @property
    def identity_config(self) -> global___IdentityConfig:
        """Optional. Identity related configuration, including service account based
        secure multi-tenancy user mappings.
        """
        pass
    def __init__(self,
        *,
        kerberos_config : typing.Optional[global___KerberosConfig] = ...,
        identity_config : typing.Optional[global___IdentityConfig] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["identity_config",b"identity_config","kerberos_config",b"kerberos_config"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["identity_config",b"identity_config","kerberos_config",b"kerberos_config"]) -> None: ...
global___SecurityConfig = SecurityConfig

class KerberosConfig(google.protobuf.message.Message):
    """Specifies Kerberos related configuration."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    ENABLE_KERBEROS_FIELD_NUMBER: builtins.int
    ROOT_PRINCIPAL_PASSWORD_URI_FIELD_NUMBER: builtins.int
    KMS_KEY_URI_FIELD_NUMBER: builtins.int
    KEYSTORE_URI_FIELD_NUMBER: builtins.int
    TRUSTSTORE_URI_FIELD_NUMBER: builtins.int
    KEYSTORE_PASSWORD_URI_FIELD_NUMBER: builtins.int
    KEY_PASSWORD_URI_FIELD_NUMBER: builtins.int
    TRUSTSTORE_PASSWORD_URI_FIELD_NUMBER: builtins.int
    CROSS_REALM_TRUST_REALM_FIELD_NUMBER: builtins.int
    CROSS_REALM_TRUST_KDC_FIELD_NUMBER: builtins.int
    CROSS_REALM_TRUST_ADMIN_SERVER_FIELD_NUMBER: builtins.int
    CROSS_REALM_TRUST_SHARED_PASSWORD_URI_FIELD_NUMBER: builtins.int
    KDC_DB_KEY_URI_FIELD_NUMBER: builtins.int
    TGT_LIFETIME_HOURS_FIELD_NUMBER: builtins.int
    REALM_FIELD_NUMBER: builtins.int
    enable_kerberos: builtins.bool = ...
    """Optional. Flag to indicate whether to Kerberize the cluster (default: false). Set
    this field to true to enable Kerberos on a cluster.
    """

    root_principal_password_uri: typing.Text = ...
    """Optional. The Cloud Storage URI of a KMS encrypted file containing the root
    principal password.
    """

    kms_key_uri: typing.Text = ...
    """Optional. The uri of the KMS key used to encrypt various sensitive
    files.
    """

    keystore_uri: typing.Text = ...
    """Optional. The Cloud Storage URI of the keystore file used for SSL
    encryption. If not provided, Dataproc will provide a self-signed
    certificate.
    """

    truststore_uri: typing.Text = ...
    """Optional. The Cloud Storage URI of the truststore file used for SSL
    encryption. If not provided, Dataproc will provide a self-signed
    certificate.
    """

    keystore_password_uri: typing.Text = ...
    """Optional. The Cloud Storage URI of a KMS encrypted file containing the
    password to the user provided keystore. For the self-signed certificate,
    this password is generated by Dataproc.
    """

    key_password_uri: typing.Text = ...
    """Optional. The Cloud Storage URI of a KMS encrypted file containing the
    password to the user provided key. For the self-signed certificate, this
    password is generated by Dataproc.
    """

    truststore_password_uri: typing.Text = ...
    """Optional. The Cloud Storage URI of a KMS encrypted file containing the
    password to the user provided truststore. For the self-signed certificate,
    this password is generated by Dataproc.
    """

    cross_realm_trust_realm: typing.Text = ...
    """Optional. The remote realm the Dataproc on-cluster KDC will trust, should
    the user enable cross realm trust.
    """

    cross_realm_trust_kdc: typing.Text = ...
    """Optional. The KDC (IP or hostname) for the remote trusted realm in a cross
    realm trust relationship.
    """

    cross_realm_trust_admin_server: typing.Text = ...
    """Optional. The admin server (IP or hostname) for the remote trusted realm in
    a cross realm trust relationship.
    """

    cross_realm_trust_shared_password_uri: typing.Text = ...
    """Optional. The Cloud Storage URI of a KMS encrypted file containing the
    shared password between the on-cluster Kerberos realm and the remote
    trusted realm, in a cross realm trust relationship.
    """

    kdc_db_key_uri: typing.Text = ...
    """Optional. The Cloud Storage URI of a KMS encrypted file containing the
    master key of the KDC database.
    """

    tgt_lifetime_hours: builtins.int = ...
    """Optional. The lifetime of the ticket granting ticket, in hours.
    If not specified, or user specifies 0, then default value 10
    will be used.
    """

    realm: typing.Text = ...
    """Optional. The name of the on-cluster Kerberos realm.
    If not specified, the uppercased domain of hostnames will be the realm.
    """

    def __init__(self,
        *,
        enable_kerberos : builtins.bool = ...,
        root_principal_password_uri : typing.Text = ...,
        kms_key_uri : typing.Text = ...,
        keystore_uri : typing.Text = ...,
        truststore_uri : typing.Text = ...,
        keystore_password_uri : typing.Text = ...,
        key_password_uri : typing.Text = ...,
        truststore_password_uri : typing.Text = ...,
        cross_realm_trust_realm : typing.Text = ...,
        cross_realm_trust_kdc : typing.Text = ...,
        cross_realm_trust_admin_server : typing.Text = ...,
        cross_realm_trust_shared_password_uri : typing.Text = ...,
        kdc_db_key_uri : typing.Text = ...,
        tgt_lifetime_hours : builtins.int = ...,
        realm : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["cross_realm_trust_admin_server",b"cross_realm_trust_admin_server","cross_realm_trust_kdc",b"cross_realm_trust_kdc","cross_realm_trust_realm",b"cross_realm_trust_realm","cross_realm_trust_shared_password_uri",b"cross_realm_trust_shared_password_uri","enable_kerberos",b"enable_kerberos","kdc_db_key_uri",b"kdc_db_key_uri","key_password_uri",b"key_password_uri","keystore_password_uri",b"keystore_password_uri","keystore_uri",b"keystore_uri","kms_key_uri",b"kms_key_uri","realm",b"realm","root_principal_password_uri",b"root_principal_password_uri","tgt_lifetime_hours",b"tgt_lifetime_hours","truststore_password_uri",b"truststore_password_uri","truststore_uri",b"truststore_uri"]) -> None: ...
global___KerberosConfig = KerberosConfig

class IdentityConfig(google.protobuf.message.Message):
    """Identity related configuration, including service account based
    secure multi-tenancy user mappings.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class UserServiceAccountMappingEntry(google.protobuf.message.Message):
        DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
        KEY_FIELD_NUMBER: builtins.int
        VALUE_FIELD_NUMBER: builtins.int
        key: typing.Text = ...
        value: typing.Text = ...
        def __init__(self,
            *,
            key : typing.Text = ...,
            value : typing.Text = ...,
            ) -> None: ...
        def ClearField(self, field_name: typing_extensions.Literal["key",b"key","value",b"value"]) -> None: ...

    USER_SERVICE_ACCOUNT_MAPPING_FIELD_NUMBER: builtins.int
    @property
    def user_service_account_mapping(self) -> google.protobuf.internal.containers.ScalarMap[typing.Text, typing.Text]:
        """Required. Map of user to service account."""
        pass
    def __init__(self,
        *,
        user_service_account_mapping : typing.Optional[typing.Mapping[typing.Text, typing.Text]] = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["user_service_account_mapping",b"user_service_account_mapping"]) -> None: ...
global___IdentityConfig = IdentityConfig

class SoftwareConfig(google.protobuf.message.Message):
    """Specifies the selection and config of software inside the cluster."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class PropertiesEntry(google.protobuf.message.Message):
        DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
        KEY_FIELD_NUMBER: builtins.int
        VALUE_FIELD_NUMBER: builtins.int
        key: typing.Text = ...
        value: typing.Text = ...
        def __init__(self,
            *,
            key : typing.Text = ...,
            value : typing.Text = ...,
            ) -> None: ...
        def ClearField(self, field_name: typing_extensions.Literal["key",b"key","value",b"value"]) -> None: ...

    IMAGE_VERSION_FIELD_NUMBER: builtins.int
    PROPERTIES_FIELD_NUMBER: builtins.int
    OPTIONAL_COMPONENTS_FIELD_NUMBER: builtins.int
    image_version: typing.Text = ...
    """Optional. The version of software inside the cluster. It must be one of the
    supported [Dataproc
    Versions](https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-versions#supported_dataproc_versions),
    such as "1.2" (including a subminor version, such as "1.2.29"), or the
    ["preview"
    version](https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-versions#other_versions).
    If unspecified, it defaults to the latest Debian version.
    """

    @property
    def properties(self) -> google.protobuf.internal.containers.ScalarMap[typing.Text, typing.Text]:
        """Optional. The properties to set on daemon config files.

        Property keys are specified in `prefix:property` format, for example
        `core:hadoop.tmp.dir`. The following are supported prefixes
        and their mappings:

        * capacity-scheduler: `capacity-scheduler.xml`
        * core:   `core-site.xml`
        * distcp: `distcp-default.xml`
        * hdfs:   `hdfs-site.xml`
        * hive:   `hive-site.xml`
        * mapred: `mapred-site.xml`
        * pig:    `pig.properties`
        * spark:  `spark-defaults.conf`
        * yarn:   `yarn-site.xml`

        For more information, see [Cluster
        properties](https://cloud.google.com/dataproc/docs/concepts/cluster-properties).
        """
        pass
    @property
    def optional_components(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[google.cloud.dataproc.v1.shared_pb2.Component.ValueType]:
        """Optional. The set of components to activate on the cluster."""
        pass
    def __init__(self,
        *,
        image_version : typing.Text = ...,
        properties : typing.Optional[typing.Mapping[typing.Text, typing.Text]] = ...,
        optional_components : typing.Optional[typing.Iterable[google.cloud.dataproc.v1.shared_pb2.Component.ValueType]] = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["image_version",b"image_version","optional_components",b"optional_components","properties",b"properties"]) -> None: ...
global___SoftwareConfig = SoftwareConfig

class LifecycleConfig(google.protobuf.message.Message):
    """Specifies the cluster auto-delete schedule configuration."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    IDLE_DELETE_TTL_FIELD_NUMBER: builtins.int
    AUTO_DELETE_TIME_FIELD_NUMBER: builtins.int
    AUTO_DELETE_TTL_FIELD_NUMBER: builtins.int
    IDLE_START_TIME_FIELD_NUMBER: builtins.int
    @property
    def idle_delete_ttl(self) -> google.protobuf.duration_pb2.Duration:
        """Optional. The duration to keep the cluster alive while idling (when no jobs
        are running). Passing this threshold will cause the cluster to be
        deleted. Minimum value is 5 minutes; maximum value is 14 days (see JSON
        representation of
        [Duration](https://developers.google.com/protocol-buffers/docs/proto3#json)).
        """
        pass
    @property
    def auto_delete_time(self) -> google.protobuf.timestamp_pb2.Timestamp:
        """Optional. The time when cluster will be auto-deleted (see JSON representation of
        [Timestamp](https://developers.google.com/protocol-buffers/docs/proto3#json)).
        """
        pass
    @property
    def auto_delete_ttl(self) -> google.protobuf.duration_pb2.Duration:
        """Optional. The lifetime duration of cluster. The cluster will be
        auto-deleted at the end of this period. Minimum value is 10 minutes;
        maximum value is 14 days (see JSON representation of
        [Duration](https://developers.google.com/protocol-buffers/docs/proto3#json)).
        """
        pass
    @property
    def idle_start_time(self) -> google.protobuf.timestamp_pb2.Timestamp:
        """Output only. The time when cluster became idle (most recent job finished)
        and became eligible for deletion due to idleness (see JSON representation
        of
        [Timestamp](https://developers.google.com/protocol-buffers/docs/proto3#json)).
        """
        pass
    def __init__(self,
        *,
        idle_delete_ttl : typing.Optional[google.protobuf.duration_pb2.Duration] = ...,
        auto_delete_time : typing.Optional[google.protobuf.timestamp_pb2.Timestamp] = ...,
        auto_delete_ttl : typing.Optional[google.protobuf.duration_pb2.Duration] = ...,
        idle_start_time : typing.Optional[google.protobuf.timestamp_pb2.Timestamp] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["auto_delete_time",b"auto_delete_time","auto_delete_ttl",b"auto_delete_ttl","idle_delete_ttl",b"idle_delete_ttl","idle_start_time",b"idle_start_time","ttl",b"ttl"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["auto_delete_time",b"auto_delete_time","auto_delete_ttl",b"auto_delete_ttl","idle_delete_ttl",b"idle_delete_ttl","idle_start_time",b"idle_start_time","ttl",b"ttl"]) -> None: ...
    def WhichOneof(self, oneof_group: typing_extensions.Literal["ttl",b"ttl"]) -> typing.Optional[typing_extensions.Literal["auto_delete_time","auto_delete_ttl"]]: ...
global___LifecycleConfig = LifecycleConfig

class MetastoreConfig(google.protobuf.message.Message):
    """Specifies a Metastore configuration."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    DATAPROC_METASTORE_SERVICE_FIELD_NUMBER: builtins.int
    dataproc_metastore_service: typing.Text = ...
    """Required. Resource name of an existing Dataproc Metastore service.

    Example:

    * `projects/[project_id]/locations/[dataproc_region]/services/[service-name]`
    """

    def __init__(self,
        *,
        dataproc_metastore_service : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["dataproc_metastore_service",b"dataproc_metastore_service"]) -> None: ...
global___MetastoreConfig = MetastoreConfig

class ClusterMetrics(google.protobuf.message.Message):
    """Contains cluster daemon metrics, such as HDFS and YARN stats.

    **Beta Feature**: This report is available for testing purposes only. It may
    be changed before final release.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class HdfsMetricsEntry(google.protobuf.message.Message):
        DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
        KEY_FIELD_NUMBER: builtins.int
        VALUE_FIELD_NUMBER: builtins.int
        key: typing.Text = ...
        value: builtins.int = ...
        def __init__(self,
            *,
            key : typing.Text = ...,
            value : builtins.int = ...,
            ) -> None: ...
        def ClearField(self, field_name: typing_extensions.Literal["key",b"key","value",b"value"]) -> None: ...

    class YarnMetricsEntry(google.protobuf.message.Message):
        DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
        KEY_FIELD_NUMBER: builtins.int
        VALUE_FIELD_NUMBER: builtins.int
        key: typing.Text = ...
        value: builtins.int = ...
        def __init__(self,
            *,
            key : typing.Text = ...,
            value : builtins.int = ...,
            ) -> None: ...
        def ClearField(self, field_name: typing_extensions.Literal["key",b"key","value",b"value"]) -> None: ...

    HDFS_METRICS_FIELD_NUMBER: builtins.int
    YARN_METRICS_FIELD_NUMBER: builtins.int
    @property
    def hdfs_metrics(self) -> google.protobuf.internal.containers.ScalarMap[typing.Text, builtins.int]:
        """The HDFS metrics."""
        pass
    @property
    def yarn_metrics(self) -> google.protobuf.internal.containers.ScalarMap[typing.Text, builtins.int]:
        """The YARN metrics."""
        pass
    def __init__(self,
        *,
        hdfs_metrics : typing.Optional[typing.Mapping[typing.Text, builtins.int]] = ...,
        yarn_metrics : typing.Optional[typing.Mapping[typing.Text, builtins.int]] = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["hdfs_metrics",b"hdfs_metrics","yarn_metrics",b"yarn_metrics"]) -> None: ...
global___ClusterMetrics = ClusterMetrics

class CreateClusterRequest(google.protobuf.message.Message):
    """A request to create a cluster."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    PROJECT_ID_FIELD_NUMBER: builtins.int
    REGION_FIELD_NUMBER: builtins.int
    CLUSTER_FIELD_NUMBER: builtins.int
    REQUEST_ID_FIELD_NUMBER: builtins.int
    ACTION_ON_FAILED_PRIMARY_WORKERS_FIELD_NUMBER: builtins.int
    project_id: typing.Text = ...
    """Required. The ID of the Google Cloud Platform project that the cluster
    belongs to.
    """

    region: typing.Text = ...
    """Required. The Dataproc region in which to handle the request."""

    @property
    def cluster(self) -> global___Cluster:
        """Required. The cluster to create."""
        pass
    request_id: typing.Text = ...
    """Optional. A unique ID used to identify the request. If the server receives two
    [CreateClusterRequest](https://cloud.google.com/dataproc/docs/reference/rpc/google.cloud.dataproc.v1#google.cloud.dataproc.v1.CreateClusterRequest)s
    with the same id, then the second request will be ignored and the
    first [google.longrunning.Operation][google.longrunning.Operation] created and stored in the backend
    is returned.

    It is recommended to always set this value to a
    [UUID](https://en.wikipedia.org/wiki/Universally_unique_identifier).

    The ID must contain only letters (a-z, A-Z), numbers (0-9),
    underscores (_), and hyphens (-). The maximum length is 40 characters.
    """

    action_on_failed_primary_workers: google.cloud.dataproc.v1.shared_pb2.FailureAction.ValueType = ...
    """Optional. Failure action when primary worker creation fails."""

    def __init__(self,
        *,
        project_id : typing.Text = ...,
        region : typing.Text = ...,
        cluster : typing.Optional[global___Cluster] = ...,
        request_id : typing.Text = ...,
        action_on_failed_primary_workers : google.cloud.dataproc.v1.shared_pb2.FailureAction.ValueType = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["cluster",b"cluster"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["action_on_failed_primary_workers",b"action_on_failed_primary_workers","cluster",b"cluster","project_id",b"project_id","region",b"region","request_id",b"request_id"]) -> None: ...
global___CreateClusterRequest = CreateClusterRequest

class UpdateClusterRequest(google.protobuf.message.Message):
    """A request to update a cluster."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    PROJECT_ID_FIELD_NUMBER: builtins.int
    REGION_FIELD_NUMBER: builtins.int
    CLUSTER_NAME_FIELD_NUMBER: builtins.int
    CLUSTER_FIELD_NUMBER: builtins.int
    GRACEFUL_DECOMMISSION_TIMEOUT_FIELD_NUMBER: builtins.int
    UPDATE_MASK_FIELD_NUMBER: builtins.int
    REQUEST_ID_FIELD_NUMBER: builtins.int
    project_id: typing.Text = ...
    """Required. The ID of the Google Cloud Platform project the
    cluster belongs to.
    """

    region: typing.Text = ...
    """Required. The Dataproc region in which to handle the request."""

    cluster_name: typing.Text = ...
    """Required. The cluster name."""

    @property
    def cluster(self) -> global___Cluster:
        """Required. The changes to the cluster."""
        pass
    @property
    def graceful_decommission_timeout(self) -> google.protobuf.duration_pb2.Duration:
        """Optional. Timeout for graceful YARN decomissioning. Graceful
        decommissioning allows removing nodes from the cluster without
        interrupting jobs in progress. Timeout specifies how long to wait for jobs
        in progress to finish before forcefully removing nodes (and potentially
        interrupting jobs). Default timeout is 0 (for forceful decommission), and
        the maximum allowed timeout is 1 day. (see JSON representation of
        [Duration](https://developers.google.com/protocol-buffers/docs/proto3#json)).

        Only supported on Dataproc image versions 1.2 and higher.
        """
        pass
    @property
    def update_mask(self) -> google.protobuf.field_mask_pb2.FieldMask:
        """Required. Specifies the path, relative to `Cluster`, of
        the field to update. For example, to change the number of workers
        in a cluster to 5, the `update_mask` parameter would be
        specified as `config.worker_config.num_instances`,
        and the `PATCH` request body would specify the new value, as follows:

            {
              "config":{
                "workerConfig":{
                  "numInstances":"5"
                }
              }
            }
        Similarly, to change the number of preemptible workers in a cluster to 5,
        the `update_mask` parameter would be
        `config.secondary_worker_config.num_instances`, and the `PATCH` request
        body would be set as follows:

            {
              "config":{
                "secondaryWorkerConfig":{
                  "numInstances":"5"
                }
              }
            }
        <strong>Note:</strong> Currently, only the following fields can be updated:

         <table>
         <tbody>
         <tr>
         <td><strong>Mask</strong></td>
         <td><strong>Purpose</strong></td>
         </tr>
         <tr>
         <td><strong><em>labels</em></strong></td>
         <td>Update labels</td>
         </tr>
         <tr>
         <td><strong><em>config.worker_config.num_instances</em></strong></td>
         <td>Resize primary worker group</td>
         </tr>
         <tr>
         <td><strong><em>config.secondary_worker_config.num_instances</em></strong></td>
         <td>Resize secondary worker group</td>
         </tr>
         <tr>
         <td>config.autoscaling_config.policy_uri</td><td>Use, stop using, or
         change autoscaling policies</td>
         </tr>
         </tbody>
         </table>
        """
        pass
    request_id: typing.Text = ...
    """Optional. A unique ID used to identify the request. If the server
    receives two
    [UpdateClusterRequest](https://cloud.google.com/dataproc/docs/reference/rpc/google.cloud.dataproc.v1#google.cloud.dataproc.v1.UpdateClusterRequest)s
    with the same id, then the second request will be ignored and the
    first [google.longrunning.Operation][google.longrunning.Operation] created and stored in the
    backend is returned.

    It is recommended to always set this value to a
    [UUID](https://en.wikipedia.org/wiki/Universally_unique_identifier).

    The ID must contain only letters (a-z, A-Z), numbers (0-9),
    underscores (_), and hyphens (-). The maximum length is 40 characters.
    """

    def __init__(self,
        *,
        project_id : typing.Text = ...,
        region : typing.Text = ...,
        cluster_name : typing.Text = ...,
        cluster : typing.Optional[global___Cluster] = ...,
        graceful_decommission_timeout : typing.Optional[google.protobuf.duration_pb2.Duration] = ...,
        update_mask : typing.Optional[google.protobuf.field_mask_pb2.FieldMask] = ...,
        request_id : typing.Text = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["cluster",b"cluster","graceful_decommission_timeout",b"graceful_decommission_timeout","update_mask",b"update_mask"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["cluster",b"cluster","cluster_name",b"cluster_name","graceful_decommission_timeout",b"graceful_decommission_timeout","project_id",b"project_id","region",b"region","request_id",b"request_id","update_mask",b"update_mask"]) -> None: ...
global___UpdateClusterRequest = UpdateClusterRequest

class StopClusterRequest(google.protobuf.message.Message):
    """A request to stop a cluster."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    PROJECT_ID_FIELD_NUMBER: builtins.int
    REGION_FIELD_NUMBER: builtins.int
    CLUSTER_NAME_FIELD_NUMBER: builtins.int
    CLUSTER_UUID_FIELD_NUMBER: builtins.int
    REQUEST_ID_FIELD_NUMBER: builtins.int
    project_id: typing.Text = ...
    """Required. The ID of the Google Cloud Platform project the
    cluster belongs to.
    """

    region: typing.Text = ...
    """Required. The Dataproc region in which to handle the request."""

    cluster_name: typing.Text = ...
    """Required. The cluster name."""

    cluster_uuid: typing.Text = ...
    """Optional. Specifying the `cluster_uuid` means the RPC will fail
    (with error NOT_FOUND) if a cluster with the specified UUID does not exist.
    """

    request_id: typing.Text = ...
    """Optional. A unique ID used to identify the request. If the server
    receives two
    [StopClusterRequest](https://cloud.google.com/dataproc/docs/reference/rpc/google.cloud.dataproc.v1#google.cloud.dataproc.v1.StopClusterRequest)s
    with the same id, then the second request will be ignored and the
    first [google.longrunning.Operation][google.longrunning.Operation] created and stored in the
    backend is returned.

    Recommendation: Set this value to a
    [UUID](https://en.wikipedia.org/wiki/Universally_unique_identifier).

    The ID must contain only letters (a-z, A-Z), numbers (0-9),
    underscores (_), and hyphens (-). The maximum length is 40 characters.
    """

    def __init__(self,
        *,
        project_id : typing.Text = ...,
        region : typing.Text = ...,
        cluster_name : typing.Text = ...,
        cluster_uuid : typing.Text = ...,
        request_id : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["cluster_name",b"cluster_name","cluster_uuid",b"cluster_uuid","project_id",b"project_id","region",b"region","request_id",b"request_id"]) -> None: ...
global___StopClusterRequest = StopClusterRequest

class StartClusterRequest(google.protobuf.message.Message):
    """A request to start a cluster."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    PROJECT_ID_FIELD_NUMBER: builtins.int
    REGION_FIELD_NUMBER: builtins.int
    CLUSTER_NAME_FIELD_NUMBER: builtins.int
    CLUSTER_UUID_FIELD_NUMBER: builtins.int
    REQUEST_ID_FIELD_NUMBER: builtins.int
    project_id: typing.Text = ...
    """Required. The ID of the Google Cloud Platform project the
    cluster belongs to.
    """

    region: typing.Text = ...
    """Required. The Dataproc region in which to handle the request."""

    cluster_name: typing.Text = ...
    """Required. The cluster name."""

    cluster_uuid: typing.Text = ...
    """Optional. Specifying the `cluster_uuid` means the RPC will fail
    (with error NOT_FOUND) if a cluster with the specified UUID does not exist.
    """

    request_id: typing.Text = ...
    """Optional. A unique ID used to identify the request. If the server
    receives two
    [StartClusterRequest](https://cloud.google.com/dataproc/docs/reference/rpc/google.cloud.dataproc.v1#google.cloud.dataproc.v1.StartClusterRequest)s
    with the same id, then the second request will be ignored and the
    first [google.longrunning.Operation][google.longrunning.Operation] created and stored in the
    backend is returned.

    Recommendation: Set this value to a
    [UUID](https://en.wikipedia.org/wiki/Universally_unique_identifier).

    The ID must contain only letters (a-z, A-Z), numbers (0-9),
    underscores (_), and hyphens (-). The maximum length is 40 characters.
    """

    def __init__(self,
        *,
        project_id : typing.Text = ...,
        region : typing.Text = ...,
        cluster_name : typing.Text = ...,
        cluster_uuid : typing.Text = ...,
        request_id : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["cluster_name",b"cluster_name","cluster_uuid",b"cluster_uuid","project_id",b"project_id","region",b"region","request_id",b"request_id"]) -> None: ...
global___StartClusterRequest = StartClusterRequest

class DeleteClusterRequest(google.protobuf.message.Message):
    """A request to delete a cluster."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    PROJECT_ID_FIELD_NUMBER: builtins.int
    REGION_FIELD_NUMBER: builtins.int
    CLUSTER_NAME_FIELD_NUMBER: builtins.int
    CLUSTER_UUID_FIELD_NUMBER: builtins.int
    REQUEST_ID_FIELD_NUMBER: builtins.int
    project_id: typing.Text = ...
    """Required. The ID of the Google Cloud Platform project that the cluster
    belongs to.
    """

    region: typing.Text = ...
    """Required. The Dataproc region in which to handle the request."""

    cluster_name: typing.Text = ...
    """Required. The cluster name."""

    cluster_uuid: typing.Text = ...
    """Optional. Specifying the `cluster_uuid` means the RPC should fail
    (with error NOT_FOUND) if cluster with specified UUID does not exist.
    """

    request_id: typing.Text = ...
    """Optional. A unique ID used to identify the request. If the server
    receives two
    [DeleteClusterRequest](https://cloud.google.com/dataproc/docs/reference/rpc/google.cloud.dataproc.v1#google.cloud.dataproc.v1.DeleteClusterRequest)s
    with the same id, then the second request will be ignored and the
    first [google.longrunning.Operation][google.longrunning.Operation] created and stored in the
    backend is returned.

    It is recommended to always set this value to a
    [UUID](https://en.wikipedia.org/wiki/Universally_unique_identifier).

    The ID must contain only letters (a-z, A-Z), numbers (0-9),
    underscores (_), and hyphens (-). The maximum length is 40 characters.
    """

    def __init__(self,
        *,
        project_id : typing.Text = ...,
        region : typing.Text = ...,
        cluster_name : typing.Text = ...,
        cluster_uuid : typing.Text = ...,
        request_id : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["cluster_name",b"cluster_name","cluster_uuid",b"cluster_uuid","project_id",b"project_id","region",b"region","request_id",b"request_id"]) -> None: ...
global___DeleteClusterRequest = DeleteClusterRequest

class GetClusterRequest(google.protobuf.message.Message):
    """Request to get the resource representation for a cluster in a project."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    PROJECT_ID_FIELD_NUMBER: builtins.int
    REGION_FIELD_NUMBER: builtins.int
    CLUSTER_NAME_FIELD_NUMBER: builtins.int
    project_id: typing.Text = ...
    """Required. The ID of the Google Cloud Platform project that the cluster
    belongs to.
    """

    region: typing.Text = ...
    """Required. The Dataproc region in which to handle the request."""

    cluster_name: typing.Text = ...
    """Required. The cluster name."""

    def __init__(self,
        *,
        project_id : typing.Text = ...,
        region : typing.Text = ...,
        cluster_name : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["cluster_name",b"cluster_name","project_id",b"project_id","region",b"region"]) -> None: ...
global___GetClusterRequest = GetClusterRequest

class ListClustersRequest(google.protobuf.message.Message):
    """A request to list the clusters in a project."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    PROJECT_ID_FIELD_NUMBER: builtins.int
    REGION_FIELD_NUMBER: builtins.int
    FILTER_FIELD_NUMBER: builtins.int
    PAGE_SIZE_FIELD_NUMBER: builtins.int
    PAGE_TOKEN_FIELD_NUMBER: builtins.int
    project_id: typing.Text = ...
    """Required. The ID of the Google Cloud Platform project that the cluster
    belongs to.
    """

    region: typing.Text = ...
    """Required. The Dataproc region in which to handle the request."""

    filter: typing.Text = ...
    """Optional. A filter constraining the clusters to list. Filters are
    case-sensitive and have the following syntax:

    field = value [AND [field = value]] ...

    where **field** is one of `status.state`, `clusterName`, or `labels.[KEY]`,
    and `[KEY]` is a label key. **value** can be `*` to match all values.
    `status.state` can be one of the following: `ACTIVE`, `INACTIVE`,
    `CREATING`, `RUNNING`, `ERROR`, `DELETING`, or `UPDATING`. `ACTIVE`
    contains the `CREATING`, `UPDATING`, and `RUNNING` states. `INACTIVE`
    contains the `DELETING` and `ERROR` states.
    `clusterName` is the name of the cluster provided at creation time.
    Only the logical `AND` operator is supported; space-separated items are
    treated as having an implicit `AND` operator.

    Example filter:

    status.state = ACTIVE AND clusterName = mycluster
    AND labels.env = staging AND labels.starred = *
    """

    page_size: builtins.int = ...
    """Optional. The standard List page size."""

    page_token: typing.Text = ...
    """Optional. The standard List page token."""

    def __init__(self,
        *,
        project_id : typing.Text = ...,
        region : typing.Text = ...,
        filter : typing.Text = ...,
        page_size : builtins.int = ...,
        page_token : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["filter",b"filter","page_size",b"page_size","page_token",b"page_token","project_id",b"project_id","region",b"region"]) -> None: ...
global___ListClustersRequest = ListClustersRequest

class ListClustersResponse(google.protobuf.message.Message):
    """The list of all clusters in a project."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    CLUSTERS_FIELD_NUMBER: builtins.int
    NEXT_PAGE_TOKEN_FIELD_NUMBER: builtins.int
    @property
    def clusters(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___Cluster]:
        """Output only. The clusters in the project."""
        pass
    next_page_token: typing.Text = ...
    """Output only. This token is included in the response if there are more
    results to fetch. To fetch additional results, provide this value as the
    `page_token` in a subsequent `ListClustersRequest`.
    """

    def __init__(self,
        *,
        clusters : typing.Optional[typing.Iterable[global___Cluster]] = ...,
        next_page_token : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["clusters",b"clusters","next_page_token",b"next_page_token"]) -> None: ...
global___ListClustersResponse = ListClustersResponse

class DiagnoseClusterRequest(google.protobuf.message.Message):
    """A request to collect cluster diagnostic information."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    PROJECT_ID_FIELD_NUMBER: builtins.int
    REGION_FIELD_NUMBER: builtins.int
    CLUSTER_NAME_FIELD_NUMBER: builtins.int
    project_id: typing.Text = ...
    """Required. The ID of the Google Cloud Platform project that the cluster
    belongs to.
    """

    region: typing.Text = ...
    """Required. The Dataproc region in which to handle the request."""

    cluster_name: typing.Text = ...
    """Required. The cluster name."""

    def __init__(self,
        *,
        project_id : typing.Text = ...,
        region : typing.Text = ...,
        cluster_name : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["cluster_name",b"cluster_name","project_id",b"project_id","region",b"region"]) -> None: ...
global___DiagnoseClusterRequest = DiagnoseClusterRequest

class DiagnoseClusterResults(google.protobuf.message.Message):
    """The location of diagnostic output."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    OUTPUT_URI_FIELD_NUMBER: builtins.int
    output_uri: typing.Text = ...
    """Output only. The Cloud Storage URI of the diagnostic output.
    The output report is a plain text file with a summary of collected
    diagnostics.
    """

    def __init__(self,
        *,
        output_uri : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["output_uri",b"output_uri"]) -> None: ...
global___DiagnoseClusterResults = DiagnoseClusterResults

class ReservationAffinity(google.protobuf.message.Message):
    """Reservation Affinity for consuming Zonal reservation."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class _Type:
        ValueType = typing.NewType('ValueType', builtins.int)
        V: typing_extensions.TypeAlias = ValueType
    class _TypeEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_Type.ValueType], builtins.type):
        DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor = ...
        TYPE_UNSPECIFIED: ReservationAffinity.Type.ValueType = ...  # 0
        NO_RESERVATION: ReservationAffinity.Type.ValueType = ...  # 1
        """Do not consume from any allocated capacity."""

        ANY_RESERVATION: ReservationAffinity.Type.ValueType = ...  # 2
        """Consume any reservation available."""

        SPECIFIC_RESERVATION: ReservationAffinity.Type.ValueType = ...  # 3
        """Must consume from a specific reservation. Must specify key value fields
        for specifying the reservations.
        """

    class Type(_Type, metaclass=_TypeEnumTypeWrapper):
        """Indicates whether to consume capacity from an reservation or not."""
        pass

    TYPE_UNSPECIFIED: ReservationAffinity.Type.ValueType = ...  # 0
    NO_RESERVATION: ReservationAffinity.Type.ValueType = ...  # 1
    """Do not consume from any allocated capacity."""

    ANY_RESERVATION: ReservationAffinity.Type.ValueType = ...  # 2
    """Consume any reservation available."""

    SPECIFIC_RESERVATION: ReservationAffinity.Type.ValueType = ...  # 3
    """Must consume from a specific reservation. Must specify key value fields
    for specifying the reservations.
    """


    CONSUME_RESERVATION_TYPE_FIELD_NUMBER: builtins.int
    KEY_FIELD_NUMBER: builtins.int
    VALUES_FIELD_NUMBER: builtins.int
    consume_reservation_type: global___ReservationAffinity.Type.ValueType = ...
    """Optional. Type of reservation to consume"""

    key: typing.Text = ...
    """Optional. Corresponds to the label key of reservation resource."""

    @property
    def values(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """Optional. Corresponds to the label values of reservation resource."""
        pass
    def __init__(self,
        *,
        consume_reservation_type : global___ReservationAffinity.Type.ValueType = ...,
        key : typing.Text = ...,
        values : typing.Optional[typing.Iterable[typing.Text]] = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["consume_reservation_type",b"consume_reservation_type","key",b"key","values",b"values"]) -> None: ...
global___ReservationAffinity = ReservationAffinity
