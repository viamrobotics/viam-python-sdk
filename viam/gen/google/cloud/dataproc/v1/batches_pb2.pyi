"""
@generated by mypy-protobuf.  Do not edit manually!
isort:skip_file
"""
import builtins
import google.cloud.dataproc.v1.shared_pb2
import google.protobuf.descriptor
import google.protobuf.internal.containers
import google.protobuf.internal.enum_type_wrapper
import google.protobuf.message
import google.protobuf.timestamp_pb2
import typing
import typing_extensions

DESCRIPTOR: google.protobuf.descriptor.FileDescriptor = ...

class CreateBatchRequest(google.protobuf.message.Message):
    """A request to create a batch workload."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    PARENT_FIELD_NUMBER: builtins.int
    BATCH_FIELD_NUMBER: builtins.int
    BATCH_ID_FIELD_NUMBER: builtins.int
    REQUEST_ID_FIELD_NUMBER: builtins.int
    parent: typing.Text = ...
    """Required. The parent resource where this batch will be created."""

    @property
    def batch(self) -> global___Batch:
        """Required. The batch to create."""
        pass
    batch_id: typing.Text = ...
    """Optional. The ID to use for the batch, which will become the final component of
    the batch's resource name.

    This value must be 4-63 characters. Valid characters are `/[a-z][0-9]-/`.
    """

    request_id: typing.Text = ...
    """Optional. A unique ID used to identify the request. If the service
    receives two
    [CreateBatchRequest](https://cloud.google.com/dataproc/docs/reference/rpc/google.cloud.dataproc.v1#google.cloud.dataproc.v1.CreateBatchRequest)s
    with the same request_id, the second request is ignored and the
    Operation that corresponds to the first Batch created and stored
    in the backend is returned.

    Recommendation: Set this value to a
    [UUID](https://en.wikipedia.org/wiki/Universally_unique_identifier).

    The value must contain only letters (a-z, A-Z), numbers (0-9),
    underscores (_), and hyphens (-). The maximum length is 40 characters.
    """

    def __init__(self,
        *,
        parent : typing.Text = ...,
        batch : typing.Optional[global___Batch] = ...,
        batch_id : typing.Text = ...,
        request_id : typing.Text = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["batch",b"batch"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["batch",b"batch","batch_id",b"batch_id","parent",b"parent","request_id",b"request_id"]) -> None: ...
global___CreateBatchRequest = CreateBatchRequest

class GetBatchRequest(google.protobuf.message.Message):
    """A request to get the resource representation for a batch workload."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    NAME_FIELD_NUMBER: builtins.int
    name: typing.Text = ...
    """Required. The name of the batch to retrieve."""

    def __init__(self,
        *,
        name : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["name",b"name"]) -> None: ...
global___GetBatchRequest = GetBatchRequest

class ListBatchesRequest(google.protobuf.message.Message):
    """A request to list batch workloads in a project."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    PARENT_FIELD_NUMBER: builtins.int
    PAGE_SIZE_FIELD_NUMBER: builtins.int
    PAGE_TOKEN_FIELD_NUMBER: builtins.int
    parent: typing.Text = ...
    """Required. The parent, which owns this collection of batches."""

    page_size: builtins.int = ...
    """Optional. The maximum number of batches to return in each response.
    The service may return fewer than this value.
    The default page size is 20; the maximum page size is 1000.
    """

    page_token: typing.Text = ...
    """Optional. A page token received from a previous `ListBatches` call.
    Provide this token to retrieve the subsequent page.
    """

    def __init__(self,
        *,
        parent : typing.Text = ...,
        page_size : builtins.int = ...,
        page_token : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["page_size",b"page_size","page_token",b"page_token","parent",b"parent"]) -> None: ...
global___ListBatchesRequest = ListBatchesRequest

class ListBatchesResponse(google.protobuf.message.Message):
    """A list of batch workloads."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    BATCHES_FIELD_NUMBER: builtins.int
    NEXT_PAGE_TOKEN_FIELD_NUMBER: builtins.int
    @property
    def batches(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___Batch]:
        """The batches from the specified collection."""
        pass
    next_page_token: typing.Text = ...
    """A token, which can be sent as `page_token` to retrieve the next page.
    If this field is omitted, there are no subsequent pages.
    """

    def __init__(self,
        *,
        batches : typing.Optional[typing.Iterable[global___Batch]] = ...,
        next_page_token : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["batches",b"batches","next_page_token",b"next_page_token"]) -> None: ...
global___ListBatchesResponse = ListBatchesResponse

class DeleteBatchRequest(google.protobuf.message.Message):
    """A request to delete a batch workload."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    NAME_FIELD_NUMBER: builtins.int
    name: typing.Text = ...
    """Required. The name of the batch resource to delete."""

    def __init__(self,
        *,
        name : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["name",b"name"]) -> None: ...
global___DeleteBatchRequest = DeleteBatchRequest

class Batch(google.protobuf.message.Message):
    """A representation of a batch workload in the service."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class _State:
        ValueType = typing.NewType('ValueType', builtins.int)
        V: typing_extensions.TypeAlias = ValueType
    class _StateEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_State.ValueType], builtins.type):
        DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor = ...
        STATE_UNSPECIFIED: Batch.State.ValueType = ...  # 0
        """The batch state is unknown."""

        PENDING: Batch.State.ValueType = ...  # 1
        """The batch is created before running."""

        RUNNING: Batch.State.ValueType = ...  # 2
        """The batch is running."""

        CANCELLING: Batch.State.ValueType = ...  # 3
        """The batch is cancelling."""

        CANCELLED: Batch.State.ValueType = ...  # 4
        """The batch cancellation was successful."""

        SUCCEEDED: Batch.State.ValueType = ...  # 5
        """The batch completed successfully."""

        FAILED: Batch.State.ValueType = ...  # 6
        """The batch is no longer running due to an error."""

    class State(_State, metaclass=_StateEnumTypeWrapper):
        """The batch state."""
        pass

    STATE_UNSPECIFIED: Batch.State.ValueType = ...  # 0
    """The batch state is unknown."""

    PENDING: Batch.State.ValueType = ...  # 1
    """The batch is created before running."""

    RUNNING: Batch.State.ValueType = ...  # 2
    """The batch is running."""

    CANCELLING: Batch.State.ValueType = ...  # 3
    """The batch is cancelling."""

    CANCELLED: Batch.State.ValueType = ...  # 4
    """The batch cancellation was successful."""

    SUCCEEDED: Batch.State.ValueType = ...  # 5
    """The batch completed successfully."""

    FAILED: Batch.State.ValueType = ...  # 6
    """The batch is no longer running due to an error."""


    class StateHistory(google.protobuf.message.Message):
        """Historical state information."""
        DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
        STATE_FIELD_NUMBER: builtins.int
        STATE_MESSAGE_FIELD_NUMBER: builtins.int
        STATE_START_TIME_FIELD_NUMBER: builtins.int
        state: global___Batch.State.ValueType = ...
        """Output only. The state of the batch at this point in history."""

        state_message: typing.Text = ...
        """Output only. Details about the state at this point in history."""

        @property
        def state_start_time(self) -> google.protobuf.timestamp_pb2.Timestamp:
            """Output only. The time when the batch entered the historical state."""
            pass
        def __init__(self,
            *,
            state : global___Batch.State.ValueType = ...,
            state_message : typing.Text = ...,
            state_start_time : typing.Optional[google.protobuf.timestamp_pb2.Timestamp] = ...,
            ) -> None: ...
        def HasField(self, field_name: typing_extensions.Literal["state_start_time",b"state_start_time"]) -> builtins.bool: ...
        def ClearField(self, field_name: typing_extensions.Literal["state",b"state","state_message",b"state_message","state_start_time",b"state_start_time"]) -> None: ...

    class LabelsEntry(google.protobuf.message.Message):
        DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
        KEY_FIELD_NUMBER: builtins.int
        VALUE_FIELD_NUMBER: builtins.int
        key: typing.Text = ...
        value: typing.Text = ...
        def __init__(self,
            *,
            key : typing.Text = ...,
            value : typing.Text = ...,
            ) -> None: ...
        def ClearField(self, field_name: typing_extensions.Literal["key",b"key","value",b"value"]) -> None: ...

    NAME_FIELD_NUMBER: builtins.int
    UUID_FIELD_NUMBER: builtins.int
    CREATE_TIME_FIELD_NUMBER: builtins.int
    PYSPARK_BATCH_FIELD_NUMBER: builtins.int
    SPARK_BATCH_FIELD_NUMBER: builtins.int
    SPARK_R_BATCH_FIELD_NUMBER: builtins.int
    SPARK_SQL_BATCH_FIELD_NUMBER: builtins.int
    RUNTIME_INFO_FIELD_NUMBER: builtins.int
    STATE_FIELD_NUMBER: builtins.int
    STATE_MESSAGE_FIELD_NUMBER: builtins.int
    STATE_TIME_FIELD_NUMBER: builtins.int
    CREATOR_FIELD_NUMBER: builtins.int
    LABELS_FIELD_NUMBER: builtins.int
    RUNTIME_CONFIG_FIELD_NUMBER: builtins.int
    ENVIRONMENT_CONFIG_FIELD_NUMBER: builtins.int
    OPERATION_FIELD_NUMBER: builtins.int
    STATE_HISTORY_FIELD_NUMBER: builtins.int
    name: typing.Text = ...
    """Output only. The resource name of the batch."""

    uuid: typing.Text = ...
    """Output only. A batch UUID (Unique Universal Identifier). The service
    generates this value when it creates the batch.
    """

    @property
    def create_time(self) -> google.protobuf.timestamp_pb2.Timestamp:
        """Output only. The time when the batch was created."""
        pass
    @property
    def pyspark_batch(self) -> global___PySparkBatch:
        """Optional. PySpark batch config."""
        pass
    @property
    def spark_batch(self) -> global___SparkBatch:
        """Optional. Spark batch config."""
        pass
    @property
    def spark_r_batch(self) -> global___SparkRBatch:
        """Optional. SparkR batch config."""
        pass
    @property
    def spark_sql_batch(self) -> global___SparkSqlBatch:
        """Optional. SparkSql batch config."""
        pass
    @property
    def runtime_info(self) -> google.cloud.dataproc.v1.shared_pb2.RuntimeInfo:
        """Output only. Runtime information about batch execution."""
        pass
    state: global___Batch.State.ValueType = ...
    """Output only. The state of the batch."""

    state_message: typing.Text = ...
    """Output only. Batch state details, such as a failure
    description if the state is `FAILED`.
    """

    @property
    def state_time(self) -> google.protobuf.timestamp_pb2.Timestamp:
        """Output only. The time when the batch entered a current state."""
        pass
    creator: typing.Text = ...
    """Output only. The email address of the user who created the batch."""

    @property
    def labels(self) -> google.protobuf.internal.containers.ScalarMap[typing.Text, typing.Text]:
        """Optional. The labels to associate with this batch.
        Label **keys** must contain 1 to 63 characters, and must conform to
        [RFC 1035](https://www.ietf.org/rfc/rfc1035.txt).
        Label **values** may be empty, but, if present, must contain 1 to 63
        characters, and must conform to [RFC
        1035](https://www.ietf.org/rfc/rfc1035.txt). No more than 32 labels can be
        associated with a batch.
        """
        pass
    @property
    def runtime_config(self) -> google.cloud.dataproc.v1.shared_pb2.RuntimeConfig:
        """Optional. Runtime configuration for the batch execution."""
        pass
    @property
    def environment_config(self) -> google.cloud.dataproc.v1.shared_pb2.EnvironmentConfig:
        """Optional. Environment configuration for the batch execution."""
        pass
    operation: typing.Text = ...
    """Output only. The resource name of the operation associated with this batch."""

    @property
    def state_history(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___Batch.StateHistory]:
        """Output only. Historical state information for the batch."""
        pass
    def __init__(self,
        *,
        name : typing.Text = ...,
        uuid : typing.Text = ...,
        create_time : typing.Optional[google.protobuf.timestamp_pb2.Timestamp] = ...,
        pyspark_batch : typing.Optional[global___PySparkBatch] = ...,
        spark_batch : typing.Optional[global___SparkBatch] = ...,
        spark_r_batch : typing.Optional[global___SparkRBatch] = ...,
        spark_sql_batch : typing.Optional[global___SparkSqlBatch] = ...,
        runtime_info : typing.Optional[google.cloud.dataproc.v1.shared_pb2.RuntimeInfo] = ...,
        state : global___Batch.State.ValueType = ...,
        state_message : typing.Text = ...,
        state_time : typing.Optional[google.protobuf.timestamp_pb2.Timestamp] = ...,
        creator : typing.Text = ...,
        labels : typing.Optional[typing.Mapping[typing.Text, typing.Text]] = ...,
        runtime_config : typing.Optional[google.cloud.dataproc.v1.shared_pb2.RuntimeConfig] = ...,
        environment_config : typing.Optional[google.cloud.dataproc.v1.shared_pb2.EnvironmentConfig] = ...,
        operation : typing.Text = ...,
        state_history : typing.Optional[typing.Iterable[global___Batch.StateHistory]] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["batch_config",b"batch_config","create_time",b"create_time","environment_config",b"environment_config","pyspark_batch",b"pyspark_batch","runtime_config",b"runtime_config","runtime_info",b"runtime_info","spark_batch",b"spark_batch","spark_r_batch",b"spark_r_batch","spark_sql_batch",b"spark_sql_batch","state_time",b"state_time"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["batch_config",b"batch_config","create_time",b"create_time","creator",b"creator","environment_config",b"environment_config","labels",b"labels","name",b"name","operation",b"operation","pyspark_batch",b"pyspark_batch","runtime_config",b"runtime_config","runtime_info",b"runtime_info","spark_batch",b"spark_batch","spark_r_batch",b"spark_r_batch","spark_sql_batch",b"spark_sql_batch","state",b"state","state_history",b"state_history","state_message",b"state_message","state_time",b"state_time","uuid",b"uuid"]) -> None: ...
    def WhichOneof(self, oneof_group: typing_extensions.Literal["batch_config",b"batch_config"]) -> typing.Optional[typing_extensions.Literal["pyspark_batch","spark_batch","spark_r_batch","spark_sql_batch"]]: ...
global___Batch = Batch

class PySparkBatch(google.protobuf.message.Message):
    """A configuration for running an
    [Apache
    PySpark](https://spark.apache.org/docs/latest/api/python/getting_started/quickstart.html)
    batch workload.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    MAIN_PYTHON_FILE_URI_FIELD_NUMBER: builtins.int
    ARGS_FIELD_NUMBER: builtins.int
    PYTHON_FILE_URIS_FIELD_NUMBER: builtins.int
    JAR_FILE_URIS_FIELD_NUMBER: builtins.int
    FILE_URIS_FIELD_NUMBER: builtins.int
    ARCHIVE_URIS_FIELD_NUMBER: builtins.int
    main_python_file_uri: typing.Text = ...
    """Required. The HCFS URI of the main Python file to use as the Spark driver. Must
    be a .py file.
    """

    @property
    def args(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """Optional. The arguments to pass to the driver. Do not include arguments
        that can be set as batch properties, such as `--conf`, since a collision
        can occur that causes an incorrect batch submission.
        """
        pass
    @property
    def python_file_uris(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """Optional. HCFS file URIs of Python files to pass to the PySpark
        framework. Supported file types: `.py`, `.egg`, and `.zip`.
        """
        pass
    @property
    def jar_file_uris(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """Optional. HCFS URIs of jar files to add to the classpath of the
        Spark driver and tasks.
        """
        pass
    @property
    def file_uris(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """Optional. HCFS URIs of files to be placed in the working directory of
        each executor.
        """
        pass
    @property
    def archive_uris(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """Optional. HCFS URIs of archives to be extracted into the working directory
        of each executor. Supported file types:
        `.jar`, `.tar`, `.tar.gz`, `.tgz`, and `.zip`.
        """
        pass
    def __init__(self,
        *,
        main_python_file_uri : typing.Text = ...,
        args : typing.Optional[typing.Iterable[typing.Text]] = ...,
        python_file_uris : typing.Optional[typing.Iterable[typing.Text]] = ...,
        jar_file_uris : typing.Optional[typing.Iterable[typing.Text]] = ...,
        file_uris : typing.Optional[typing.Iterable[typing.Text]] = ...,
        archive_uris : typing.Optional[typing.Iterable[typing.Text]] = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["archive_uris",b"archive_uris","args",b"args","file_uris",b"file_uris","jar_file_uris",b"jar_file_uris","main_python_file_uri",b"main_python_file_uri","python_file_uris",b"python_file_uris"]) -> None: ...
global___PySparkBatch = PySparkBatch

class SparkBatch(google.protobuf.message.Message):
    """A configuration for running an [Apache Spark](http://spark.apache.org/)
    batch workload.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    MAIN_JAR_FILE_URI_FIELD_NUMBER: builtins.int
    MAIN_CLASS_FIELD_NUMBER: builtins.int
    ARGS_FIELD_NUMBER: builtins.int
    JAR_FILE_URIS_FIELD_NUMBER: builtins.int
    FILE_URIS_FIELD_NUMBER: builtins.int
    ARCHIVE_URIS_FIELD_NUMBER: builtins.int
    main_jar_file_uri: typing.Text = ...
    """Optional. The HCFS URI of the jar file that contains the main class."""

    main_class: typing.Text = ...
    """Optional. The name of the driver main class. The jar file that contains the class
    must be in the classpath or specified in `jar_file_uris`.
    """

    @property
    def args(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """Optional. The arguments to pass to the driver. Do not include arguments
        that can be set as batch properties, such as `--conf`, since a collision
        can occur that causes an incorrect batch submission.
        """
        pass
    @property
    def jar_file_uris(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """Optional. HCFS URIs of jar files to add to the classpath of the
        Spark driver and tasks.
        """
        pass
    @property
    def file_uris(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """Optional. HCFS URIs of files to be placed in the working directory of
        each executor.
        """
        pass
    @property
    def archive_uris(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """Optional. HCFS URIs of archives to be extracted into the working directory
        of each executor. Supported file types:
        `.jar`, `.tar`, `.tar.gz`, `.tgz`, and `.zip`.
        """
        pass
    def __init__(self,
        *,
        main_jar_file_uri : typing.Text = ...,
        main_class : typing.Text = ...,
        args : typing.Optional[typing.Iterable[typing.Text]] = ...,
        jar_file_uris : typing.Optional[typing.Iterable[typing.Text]] = ...,
        file_uris : typing.Optional[typing.Iterable[typing.Text]] = ...,
        archive_uris : typing.Optional[typing.Iterable[typing.Text]] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["driver",b"driver","main_class",b"main_class","main_jar_file_uri",b"main_jar_file_uri"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["archive_uris",b"archive_uris","args",b"args","driver",b"driver","file_uris",b"file_uris","jar_file_uris",b"jar_file_uris","main_class",b"main_class","main_jar_file_uri",b"main_jar_file_uri"]) -> None: ...
    def WhichOneof(self, oneof_group: typing_extensions.Literal["driver",b"driver"]) -> typing.Optional[typing_extensions.Literal["main_jar_file_uri","main_class"]]: ...
global___SparkBatch = SparkBatch

class SparkRBatch(google.protobuf.message.Message):
    """A configuration for running an
    [Apache SparkR](https://spark.apache.org/docs/latest/sparkr.html)
    batch workload.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    MAIN_R_FILE_URI_FIELD_NUMBER: builtins.int
    ARGS_FIELD_NUMBER: builtins.int
    FILE_URIS_FIELD_NUMBER: builtins.int
    ARCHIVE_URIS_FIELD_NUMBER: builtins.int
    main_r_file_uri: typing.Text = ...
    """Required. The HCFS URI of the main R file to use as the driver.
    Must be a `.R` or `.r` file.
    """

    @property
    def args(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """Optional. The arguments to pass to the Spark driver. Do not include arguments
        that can be set as batch properties, such as `--conf`, since a collision
        can occur that causes an incorrect batch submission.
        """
        pass
    @property
    def file_uris(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """Optional. HCFS URIs of files to be placed in the working directory of
        each executor.
        """
        pass
    @property
    def archive_uris(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """Optional. HCFS URIs of archives to be extracted into the working directory
        of each executor. Supported file types:
        `.jar`, `.tar`, `.tar.gz`, `.tgz`, and `.zip`.
        """
        pass
    def __init__(self,
        *,
        main_r_file_uri : typing.Text = ...,
        args : typing.Optional[typing.Iterable[typing.Text]] = ...,
        file_uris : typing.Optional[typing.Iterable[typing.Text]] = ...,
        archive_uris : typing.Optional[typing.Iterable[typing.Text]] = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["archive_uris",b"archive_uris","args",b"args","file_uris",b"file_uris","main_r_file_uri",b"main_r_file_uri"]) -> None: ...
global___SparkRBatch = SparkRBatch

class SparkSqlBatch(google.protobuf.message.Message):
    """A configuration for running
    [Apache Spark SQL](http://spark.apache.org/sql/) queries as a batch workload.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class QueryVariablesEntry(google.protobuf.message.Message):
        DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
        KEY_FIELD_NUMBER: builtins.int
        VALUE_FIELD_NUMBER: builtins.int
        key: typing.Text = ...
        value: typing.Text = ...
        def __init__(self,
            *,
            key : typing.Text = ...,
            value : typing.Text = ...,
            ) -> None: ...
        def ClearField(self, field_name: typing_extensions.Literal["key",b"key","value",b"value"]) -> None: ...

    QUERY_FILE_URI_FIELD_NUMBER: builtins.int
    QUERY_VARIABLES_FIELD_NUMBER: builtins.int
    JAR_FILE_URIS_FIELD_NUMBER: builtins.int
    query_file_uri: typing.Text = ...
    """Required. The HCFS URI of the script that contains Spark SQL queries to execute."""

    @property
    def query_variables(self) -> google.protobuf.internal.containers.ScalarMap[typing.Text, typing.Text]:
        """Optional. Mapping of query variable names to values (equivalent to the
        Spark SQL command: `SET name="value";`).
        """
        pass
    @property
    def jar_file_uris(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """Optional. HCFS URIs of jar files to be added to the Spark CLASSPATH."""
        pass
    def __init__(self,
        *,
        query_file_uri : typing.Text = ...,
        query_variables : typing.Optional[typing.Mapping[typing.Text, typing.Text]] = ...,
        jar_file_uris : typing.Optional[typing.Iterable[typing.Text]] = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["jar_file_uris",b"jar_file_uris","query_file_uri",b"query_file_uri","query_variables",b"query_variables"]) -> None: ...
global___SparkSqlBatch = SparkSqlBatch
