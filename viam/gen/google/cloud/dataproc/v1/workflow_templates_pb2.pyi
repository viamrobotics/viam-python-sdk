"""
@generated by mypy-protobuf.  Do not edit manually!
isort:skip_file
"""
import builtins
import google.cloud.dataproc.v1.clusters_pb2
import google.cloud.dataproc.v1.jobs_pb2
import google.protobuf.descriptor
import google.protobuf.duration_pb2
import google.protobuf.internal.containers
import google.protobuf.internal.enum_type_wrapper
import google.protobuf.message
import google.protobuf.timestamp_pb2
import typing
import typing_extensions

DESCRIPTOR: google.protobuf.descriptor.FileDescriptor = ...

class WorkflowTemplate(google.protobuf.message.Message):
    """A Dataproc workflow template resource."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class LabelsEntry(google.protobuf.message.Message):
        DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
        KEY_FIELD_NUMBER: builtins.int
        VALUE_FIELD_NUMBER: builtins.int
        key: typing.Text = ...
        value: typing.Text = ...
        def __init__(self,
            *,
            key : typing.Text = ...,
            value : typing.Text = ...,
            ) -> None: ...
        def ClearField(self, field_name: typing_extensions.Literal["key",b"key","value",b"value"]) -> None: ...

    ID_FIELD_NUMBER: builtins.int
    NAME_FIELD_NUMBER: builtins.int
    VERSION_FIELD_NUMBER: builtins.int
    CREATE_TIME_FIELD_NUMBER: builtins.int
    UPDATE_TIME_FIELD_NUMBER: builtins.int
    LABELS_FIELD_NUMBER: builtins.int
    PLACEMENT_FIELD_NUMBER: builtins.int
    JOBS_FIELD_NUMBER: builtins.int
    PARAMETERS_FIELD_NUMBER: builtins.int
    DAG_TIMEOUT_FIELD_NUMBER: builtins.int
    id: typing.Text = ...
    name: typing.Text = ...
    """Output only. The resource name of the workflow template, as described
    in https://cloud.google.com/apis/design/resource_names.

    * For `projects.regions.workflowTemplates`, the resource name of the
      template has the following format:
      `projects/{project_id}/regions/{region}/workflowTemplates/{template_id}`

    * For `projects.locations.workflowTemplates`, the resource name of the
      template has the following format:
      `projects/{project_id}/locations/{location}/workflowTemplates/{template_id}`
    """

    version: builtins.int = ...
    """Optional. Used to perform a consistent read-modify-write.

    This field should be left blank for a `CreateWorkflowTemplate` request. It
    is required for an `UpdateWorkflowTemplate` request, and must match the
    current server version. A typical update template flow would fetch the
    current template with a `GetWorkflowTemplate` request, which will return
    the current template with the `version` field filled in with the
    current server version. The user updates other fields in the template,
    then returns it as part of the `UpdateWorkflowTemplate` request.
    """

    @property
    def create_time(self) -> google.protobuf.timestamp_pb2.Timestamp:
        """Output only. The time template was created."""
        pass
    @property
    def update_time(self) -> google.protobuf.timestamp_pb2.Timestamp:
        """Output only. The time template was last updated."""
        pass
    @property
    def labels(self) -> google.protobuf.internal.containers.ScalarMap[typing.Text, typing.Text]:
        """Optional. The labels to associate with this template. These labels
        will be propagated to all jobs and clusters created by the workflow
        instance.

        Label **keys** must contain 1 to 63 characters, and must conform to
        [RFC 1035](https://www.ietf.org/rfc/rfc1035.txt).

        Label **values** may be empty, but, if present, must contain 1 to 63
        characters, and must conform to
        [RFC 1035](https://www.ietf.org/rfc/rfc1035.txt).

        No more than 32 labels can be associated with a template.
        """
        pass
    @property
    def placement(self) -> global___WorkflowTemplatePlacement:
        """Required. WorkflowTemplate scheduling information."""
        pass
    @property
    def jobs(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___OrderedJob]:
        """Required. The Directed Acyclic Graph of Jobs to submit."""
        pass
    @property
    def parameters(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___TemplateParameter]:
        """Optional. Template parameters whose values are substituted into the
        template. Values for parameters must be provided when the template is
        instantiated.
        """
        pass
    @property
    def dag_timeout(self) -> google.protobuf.duration_pb2.Duration:
        """Optional. Timeout duration for the DAG of jobs, expressed in seconds (see
        [JSON representation of
        duration](https://developers.google.com/protocol-buffers/docs/proto3#json)).
        The timeout duration must be from 10 minutes ("600s") to 24 hours
        ("86400s"). The timer begins when the first job is submitted. If the
        workflow is running at the end of the timeout period, any remaining jobs
        are cancelled, the workflow is ended, and if the workflow was running on a
        [managed
        cluster](/dataproc/docs/concepts/workflows/using-workflows#configuring_or_selecting_a_cluster),
        the cluster is deleted.
        """
        pass
    def __init__(self,
        *,
        id : typing.Text = ...,
        name : typing.Text = ...,
        version : builtins.int = ...,
        create_time : typing.Optional[google.protobuf.timestamp_pb2.Timestamp] = ...,
        update_time : typing.Optional[google.protobuf.timestamp_pb2.Timestamp] = ...,
        labels : typing.Optional[typing.Mapping[typing.Text, typing.Text]] = ...,
        placement : typing.Optional[global___WorkflowTemplatePlacement] = ...,
        jobs : typing.Optional[typing.Iterable[global___OrderedJob]] = ...,
        parameters : typing.Optional[typing.Iterable[global___TemplateParameter]] = ...,
        dag_timeout : typing.Optional[google.protobuf.duration_pb2.Duration] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["create_time",b"create_time","dag_timeout",b"dag_timeout","placement",b"placement","update_time",b"update_time"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["create_time",b"create_time","dag_timeout",b"dag_timeout","id",b"id","jobs",b"jobs","labels",b"labels","name",b"name","parameters",b"parameters","placement",b"placement","update_time",b"update_time","version",b"version"]) -> None: ...
global___WorkflowTemplate = WorkflowTemplate

class WorkflowTemplatePlacement(google.protobuf.message.Message):
    """Specifies workflow execution target.

    Either `managed_cluster` or `cluster_selector` is required.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    MANAGED_CLUSTER_FIELD_NUMBER: builtins.int
    CLUSTER_SELECTOR_FIELD_NUMBER: builtins.int
    @property
    def managed_cluster(self) -> global___ManagedCluster:
        """A cluster that is managed by the workflow."""
        pass
    @property
    def cluster_selector(self) -> global___ClusterSelector:
        """Optional. A selector that chooses target cluster for jobs based
        on metadata.

        The selector is evaluated at the time each job is submitted.
        """
        pass
    def __init__(self,
        *,
        managed_cluster : typing.Optional[global___ManagedCluster] = ...,
        cluster_selector : typing.Optional[global___ClusterSelector] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["cluster_selector",b"cluster_selector","managed_cluster",b"managed_cluster","placement",b"placement"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["cluster_selector",b"cluster_selector","managed_cluster",b"managed_cluster","placement",b"placement"]) -> None: ...
    def WhichOneof(self, oneof_group: typing_extensions.Literal["placement",b"placement"]) -> typing.Optional[typing_extensions.Literal["managed_cluster","cluster_selector"]]: ...
global___WorkflowTemplatePlacement = WorkflowTemplatePlacement

class ManagedCluster(google.protobuf.message.Message):
    """Cluster that is managed by the workflow."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class LabelsEntry(google.protobuf.message.Message):
        DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
        KEY_FIELD_NUMBER: builtins.int
        VALUE_FIELD_NUMBER: builtins.int
        key: typing.Text = ...
        value: typing.Text = ...
        def __init__(self,
            *,
            key : typing.Text = ...,
            value : typing.Text = ...,
            ) -> None: ...
        def ClearField(self, field_name: typing_extensions.Literal["key",b"key","value",b"value"]) -> None: ...

    CLUSTER_NAME_FIELD_NUMBER: builtins.int
    CONFIG_FIELD_NUMBER: builtins.int
    LABELS_FIELD_NUMBER: builtins.int
    cluster_name: typing.Text = ...
    """Required. The cluster name prefix. A unique cluster name will be formed by
    appending a random suffix.

    The name must contain only lower-case letters (a-z), numbers (0-9),
    and hyphens (-). Must begin with a letter. Cannot begin or end with
    hyphen. Must consist of between 2 and 35 characters.
    """

    @property
    def config(self) -> google.cloud.dataproc.v1.clusters_pb2.ClusterConfig:
        """Required. The cluster configuration."""
        pass
    @property
    def labels(self) -> google.protobuf.internal.containers.ScalarMap[typing.Text, typing.Text]:
        """Optional. The labels to associate with this cluster.

        Label keys must be between 1 and 63 characters long, and must conform to
        the following PCRE regular expression:
        [\\p{Ll}\\p{Lo}][\\p{Ll}\\p{Lo}\\p{N}_-]{0,62}

        Label values must be between 1 and 63 characters long, and must conform to
        the following PCRE regular expression: [\\p{Ll}\\p{Lo}\\p{N}_-]{0,63}

        No more than 32 labels can be associated with a given cluster.
        """
        pass
    def __init__(self,
        *,
        cluster_name : typing.Text = ...,
        config : typing.Optional[google.cloud.dataproc.v1.clusters_pb2.ClusterConfig] = ...,
        labels : typing.Optional[typing.Mapping[typing.Text, typing.Text]] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["config",b"config"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["cluster_name",b"cluster_name","config",b"config","labels",b"labels"]) -> None: ...
global___ManagedCluster = ManagedCluster

class ClusterSelector(google.protobuf.message.Message):
    """A selector that chooses target cluster for jobs based on metadata."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class ClusterLabelsEntry(google.protobuf.message.Message):
        DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
        KEY_FIELD_NUMBER: builtins.int
        VALUE_FIELD_NUMBER: builtins.int
        key: typing.Text = ...
        value: typing.Text = ...
        def __init__(self,
            *,
            key : typing.Text = ...,
            value : typing.Text = ...,
            ) -> None: ...
        def ClearField(self, field_name: typing_extensions.Literal["key",b"key","value",b"value"]) -> None: ...

    ZONE_FIELD_NUMBER: builtins.int
    CLUSTER_LABELS_FIELD_NUMBER: builtins.int
    zone: typing.Text = ...
    """Optional. The zone where workflow process executes. This parameter does not
    affect the selection of the cluster.

    If unspecified, the zone of the first cluster matching the selector
    is used.
    """

    @property
    def cluster_labels(self) -> google.protobuf.internal.containers.ScalarMap[typing.Text, typing.Text]:
        """Required. The cluster labels. Cluster must have all labels
        to match.
        """
        pass
    def __init__(self,
        *,
        zone : typing.Text = ...,
        cluster_labels : typing.Optional[typing.Mapping[typing.Text, typing.Text]] = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["cluster_labels",b"cluster_labels","zone",b"zone"]) -> None: ...
global___ClusterSelector = ClusterSelector

class OrderedJob(google.protobuf.message.Message):
    """A job executed by the workflow."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class LabelsEntry(google.protobuf.message.Message):
        DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
        KEY_FIELD_NUMBER: builtins.int
        VALUE_FIELD_NUMBER: builtins.int
        key: typing.Text = ...
        value: typing.Text = ...
        def __init__(self,
            *,
            key : typing.Text = ...,
            value : typing.Text = ...,
            ) -> None: ...
        def ClearField(self, field_name: typing_extensions.Literal["key",b"key","value",b"value"]) -> None: ...

    STEP_ID_FIELD_NUMBER: builtins.int
    HADOOP_JOB_FIELD_NUMBER: builtins.int
    SPARK_JOB_FIELD_NUMBER: builtins.int
    PYSPARK_JOB_FIELD_NUMBER: builtins.int
    HIVE_JOB_FIELD_NUMBER: builtins.int
    PIG_JOB_FIELD_NUMBER: builtins.int
    SPARK_R_JOB_FIELD_NUMBER: builtins.int
    SPARK_SQL_JOB_FIELD_NUMBER: builtins.int
    PRESTO_JOB_FIELD_NUMBER: builtins.int
    LABELS_FIELD_NUMBER: builtins.int
    SCHEDULING_FIELD_NUMBER: builtins.int
    PREREQUISITE_STEP_IDS_FIELD_NUMBER: builtins.int
    step_id: typing.Text = ...
    """Required. The step id. The id must be unique among all jobs
    within the template.

    The step id is used as prefix for job id, as job
    `goog-dataproc-workflow-step-id` label, and in
    [prerequisiteStepIds][google.cloud.dataproc.v1.OrderedJob.prerequisite_step_ids] field from other
    steps.

    The id must contain only letters (a-z, A-Z), numbers (0-9),
    underscores (_), and hyphens (-). Cannot begin or end with underscore
    or hyphen. Must consist of between 3 and 50 characters.
    """

    @property
    def hadoop_job(self) -> google.cloud.dataproc.v1.jobs_pb2.HadoopJob:
        """Optional. Job is a Hadoop job."""
        pass
    @property
    def spark_job(self) -> google.cloud.dataproc.v1.jobs_pb2.SparkJob:
        """Optional. Job is a Spark job."""
        pass
    @property
    def pyspark_job(self) -> google.cloud.dataproc.v1.jobs_pb2.PySparkJob:
        """Optional. Job is a PySpark job."""
        pass
    @property
    def hive_job(self) -> google.cloud.dataproc.v1.jobs_pb2.HiveJob:
        """Optional. Job is a Hive job."""
        pass
    @property
    def pig_job(self) -> google.cloud.dataproc.v1.jobs_pb2.PigJob:
        """Optional. Job is a Pig job."""
        pass
    @property
    def spark_r_job(self) -> google.cloud.dataproc.v1.jobs_pb2.SparkRJob:
        """Optional. Job is a SparkR job."""
        pass
    @property
    def spark_sql_job(self) -> google.cloud.dataproc.v1.jobs_pb2.SparkSqlJob:
        """Optional. Job is a SparkSql job."""
        pass
    @property
    def presto_job(self) -> google.cloud.dataproc.v1.jobs_pb2.PrestoJob:
        """Optional. Job is a Presto job."""
        pass
    @property
    def labels(self) -> google.protobuf.internal.containers.ScalarMap[typing.Text, typing.Text]:
        """Optional. The labels to associate with this job.

        Label keys must be between 1 and 63 characters long, and must conform to
        the following regular expression:
        [\\p{Ll}\\p{Lo}][\\p{Ll}\\p{Lo}\\p{N}_-]{0,62}

        Label values must be between 1 and 63 characters long, and must conform to
        the following regular expression: [\\p{Ll}\\p{Lo}\\p{N}_-]{0,63}

        No more than 32 labels can be associated with a given job.
        """
        pass
    @property
    def scheduling(self) -> google.cloud.dataproc.v1.jobs_pb2.JobScheduling:
        """Optional. Job scheduling configuration."""
        pass
    @property
    def prerequisite_step_ids(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """Optional. The optional list of prerequisite job step_ids.
        If not specified, the job will start at the beginning of workflow.
        """
        pass
    def __init__(self,
        *,
        step_id : typing.Text = ...,
        hadoop_job : typing.Optional[google.cloud.dataproc.v1.jobs_pb2.HadoopJob] = ...,
        spark_job : typing.Optional[google.cloud.dataproc.v1.jobs_pb2.SparkJob] = ...,
        pyspark_job : typing.Optional[google.cloud.dataproc.v1.jobs_pb2.PySparkJob] = ...,
        hive_job : typing.Optional[google.cloud.dataproc.v1.jobs_pb2.HiveJob] = ...,
        pig_job : typing.Optional[google.cloud.dataproc.v1.jobs_pb2.PigJob] = ...,
        spark_r_job : typing.Optional[google.cloud.dataproc.v1.jobs_pb2.SparkRJob] = ...,
        spark_sql_job : typing.Optional[google.cloud.dataproc.v1.jobs_pb2.SparkSqlJob] = ...,
        presto_job : typing.Optional[google.cloud.dataproc.v1.jobs_pb2.PrestoJob] = ...,
        labels : typing.Optional[typing.Mapping[typing.Text, typing.Text]] = ...,
        scheduling : typing.Optional[google.cloud.dataproc.v1.jobs_pb2.JobScheduling] = ...,
        prerequisite_step_ids : typing.Optional[typing.Iterable[typing.Text]] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["hadoop_job",b"hadoop_job","hive_job",b"hive_job","job_type",b"job_type","pig_job",b"pig_job","presto_job",b"presto_job","pyspark_job",b"pyspark_job","scheduling",b"scheduling","spark_job",b"spark_job","spark_r_job",b"spark_r_job","spark_sql_job",b"spark_sql_job"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["hadoop_job",b"hadoop_job","hive_job",b"hive_job","job_type",b"job_type","labels",b"labels","pig_job",b"pig_job","prerequisite_step_ids",b"prerequisite_step_ids","presto_job",b"presto_job","pyspark_job",b"pyspark_job","scheduling",b"scheduling","spark_job",b"spark_job","spark_r_job",b"spark_r_job","spark_sql_job",b"spark_sql_job","step_id",b"step_id"]) -> None: ...
    def WhichOneof(self, oneof_group: typing_extensions.Literal["job_type",b"job_type"]) -> typing.Optional[typing_extensions.Literal["hadoop_job","spark_job","pyspark_job","hive_job","pig_job","spark_r_job","spark_sql_job","presto_job"]]: ...
global___OrderedJob = OrderedJob

class TemplateParameter(google.protobuf.message.Message):
    """A configurable parameter that replaces one or more fields in the template.
    Parameterizable fields:
    - Labels
    - File uris
    - Job properties
    - Job arguments
    - Script variables
    - Main class (in HadoopJob and SparkJob)
    - Zone (in ClusterSelector)
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    NAME_FIELD_NUMBER: builtins.int
    FIELDS_FIELD_NUMBER: builtins.int
    DESCRIPTION_FIELD_NUMBER: builtins.int
    VALIDATION_FIELD_NUMBER: builtins.int
    name: typing.Text = ...
    """Required. Parameter name.
    The parameter name is used as the key, and paired with the
    parameter value, which are passed to the template when the template
    is instantiated.
    The name must contain only capital letters (A-Z), numbers (0-9), and
    underscores (_), and must not start with a number. The maximum length is
    40 characters.
    """

    @property
    def fields(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """Required. Paths to all fields that the parameter replaces.
        A field is allowed to appear in at most one parameter's list of field
        paths.

        A field path is similar in syntax to a [google.protobuf.FieldMask][google.protobuf.FieldMask].
        For example, a field path that references the zone field of a workflow
        template's cluster selector would be specified as
        `placement.clusterSelector.zone`.

        Also, field paths can reference fields using the following syntax:

        * Values in maps can be referenced by key:
            * labels['key']
            * placement.clusterSelector.clusterLabels['key']
            * placement.managedCluster.labels['key']
            * placement.clusterSelector.clusterLabels['key']
            * jobs['step-id'].labels['key']

        * Jobs in the jobs list can be referenced by step-id:
            * jobs['step-id'].hadoopJob.mainJarFileUri
            * jobs['step-id'].hiveJob.queryFileUri
            * jobs['step-id'].pySparkJob.mainPythonFileUri
            * jobs['step-id'].hadoopJob.jarFileUris[0]
            * jobs['step-id'].hadoopJob.archiveUris[0]
            * jobs['step-id'].hadoopJob.fileUris[0]
            * jobs['step-id'].pySparkJob.pythonFileUris[0]

        * Items in repeated fields can be referenced by a zero-based index:
            * jobs['step-id'].sparkJob.args[0]

        * Other examples:
            * jobs['step-id'].hadoopJob.properties['key']
            * jobs['step-id'].hadoopJob.args[0]
            * jobs['step-id'].hiveJob.scriptVariables['key']
            * jobs['step-id'].hadoopJob.mainJarFileUri
            * placement.clusterSelector.zone

        It may not be possible to parameterize maps and repeated fields in their
        entirety since only individual map values and individual items in repeated
        fields can be referenced. For example, the following field paths are
        invalid:

        - placement.clusterSelector.clusterLabels
        - jobs['step-id'].sparkJob.args
        """
        pass
    description: typing.Text = ...
    """Optional. Brief description of the parameter.
    Must not exceed 1024 characters.
    """

    @property
    def validation(self) -> global___ParameterValidation:
        """Optional. Validation rules to be applied to this parameter's value."""
        pass
    def __init__(self,
        *,
        name : typing.Text = ...,
        fields : typing.Optional[typing.Iterable[typing.Text]] = ...,
        description : typing.Text = ...,
        validation : typing.Optional[global___ParameterValidation] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["validation",b"validation"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["description",b"description","fields",b"fields","name",b"name","validation",b"validation"]) -> None: ...
global___TemplateParameter = TemplateParameter

class ParameterValidation(google.protobuf.message.Message):
    """Configuration for parameter validation."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    REGEX_FIELD_NUMBER: builtins.int
    VALUES_FIELD_NUMBER: builtins.int
    @property
    def regex(self) -> global___RegexValidation:
        """Validation based on regular expressions."""
        pass
    @property
    def values(self) -> global___ValueValidation:
        """Validation based on a list of allowed values."""
        pass
    def __init__(self,
        *,
        regex : typing.Optional[global___RegexValidation] = ...,
        values : typing.Optional[global___ValueValidation] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["regex",b"regex","validation_type",b"validation_type","values",b"values"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["regex",b"regex","validation_type",b"validation_type","values",b"values"]) -> None: ...
    def WhichOneof(self, oneof_group: typing_extensions.Literal["validation_type",b"validation_type"]) -> typing.Optional[typing_extensions.Literal["regex","values"]]: ...
global___ParameterValidation = ParameterValidation

class RegexValidation(google.protobuf.message.Message):
    """Validation based on regular expressions."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    REGEXES_FIELD_NUMBER: builtins.int
    @property
    def regexes(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """Required. RE2 regular expressions used to validate the parameter's value.
        The value must match the regex in its entirety (substring
        matches are not sufficient).
        """
        pass
    def __init__(self,
        *,
        regexes : typing.Optional[typing.Iterable[typing.Text]] = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["regexes",b"regexes"]) -> None: ...
global___RegexValidation = RegexValidation

class ValueValidation(google.protobuf.message.Message):
    """Validation based on a list of allowed values."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    VALUES_FIELD_NUMBER: builtins.int
    @property
    def values(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """Required. List of allowed values for the parameter."""
        pass
    def __init__(self,
        *,
        values : typing.Optional[typing.Iterable[typing.Text]] = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["values",b"values"]) -> None: ...
global___ValueValidation = ValueValidation

class WorkflowMetadata(google.protobuf.message.Message):
    """A Dataproc workflow template resource."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class _State:
        ValueType = typing.NewType('ValueType', builtins.int)
        V: typing_extensions.TypeAlias = ValueType
    class _StateEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_State.ValueType], builtins.type):
        DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor = ...
        UNKNOWN: WorkflowMetadata.State.ValueType = ...  # 0
        """Unused."""

        PENDING: WorkflowMetadata.State.ValueType = ...  # 1
        """The operation has been created."""

        RUNNING: WorkflowMetadata.State.ValueType = ...  # 2
        """The operation is running."""

        DONE: WorkflowMetadata.State.ValueType = ...  # 3
        """The operation is done; either cancelled or completed."""

    class State(_State, metaclass=_StateEnumTypeWrapper):
        """The operation state."""
        pass

    UNKNOWN: WorkflowMetadata.State.ValueType = ...  # 0
    """Unused."""

    PENDING: WorkflowMetadata.State.ValueType = ...  # 1
    """The operation has been created."""

    RUNNING: WorkflowMetadata.State.ValueType = ...  # 2
    """The operation is running."""

    DONE: WorkflowMetadata.State.ValueType = ...  # 3
    """The operation is done; either cancelled or completed."""


    class ParametersEntry(google.protobuf.message.Message):
        DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
        KEY_FIELD_NUMBER: builtins.int
        VALUE_FIELD_NUMBER: builtins.int
        key: typing.Text = ...
        value: typing.Text = ...
        def __init__(self,
            *,
            key : typing.Text = ...,
            value : typing.Text = ...,
            ) -> None: ...
        def ClearField(self, field_name: typing_extensions.Literal["key",b"key","value",b"value"]) -> None: ...

    TEMPLATE_FIELD_NUMBER: builtins.int
    VERSION_FIELD_NUMBER: builtins.int
    CREATE_CLUSTER_FIELD_NUMBER: builtins.int
    GRAPH_FIELD_NUMBER: builtins.int
    DELETE_CLUSTER_FIELD_NUMBER: builtins.int
    STATE_FIELD_NUMBER: builtins.int
    CLUSTER_NAME_FIELD_NUMBER: builtins.int
    PARAMETERS_FIELD_NUMBER: builtins.int
    START_TIME_FIELD_NUMBER: builtins.int
    END_TIME_FIELD_NUMBER: builtins.int
    CLUSTER_UUID_FIELD_NUMBER: builtins.int
    DAG_TIMEOUT_FIELD_NUMBER: builtins.int
    DAG_START_TIME_FIELD_NUMBER: builtins.int
    DAG_END_TIME_FIELD_NUMBER: builtins.int
    template: typing.Text = ...
    """Output only. The resource name of the workflow template as described
    in https://cloud.google.com/apis/design/resource_names.

    * For `projects.regions.workflowTemplates`, the resource name of the
      template has the following format:
      `projects/{project_id}/regions/{region}/workflowTemplates/{template_id}`

    * For `projects.locations.workflowTemplates`, the resource name of the
      template has the following format:
      `projects/{project_id}/locations/{location}/workflowTemplates/{template_id}`
    """

    version: builtins.int = ...
    """Output only. The version of template at the time of
    workflow instantiation.
    """

    @property
    def create_cluster(self) -> global___ClusterOperation:
        """Output only. The create cluster operation metadata."""
        pass
    @property
    def graph(self) -> global___WorkflowGraph:
        """Output only. The workflow graph."""
        pass
    @property
    def delete_cluster(self) -> global___ClusterOperation:
        """Output only. The delete cluster operation metadata."""
        pass
    state: global___WorkflowMetadata.State.ValueType = ...
    """Output only. The workflow state."""

    cluster_name: typing.Text = ...
    """Output only. The name of the target cluster."""

    @property
    def parameters(self) -> google.protobuf.internal.containers.ScalarMap[typing.Text, typing.Text]:
        """Map from parameter names to values that were used for those parameters."""
        pass
    @property
    def start_time(self) -> google.protobuf.timestamp_pb2.Timestamp:
        """Output only. Workflow start time."""
        pass
    @property
    def end_time(self) -> google.protobuf.timestamp_pb2.Timestamp:
        """Output only. Workflow end time."""
        pass
    cluster_uuid: typing.Text = ...
    """Output only. The UUID of target cluster."""

    @property
    def dag_timeout(self) -> google.protobuf.duration_pb2.Duration:
        """Output only. The timeout duration for the DAG of jobs, expressed in seconds (see
        [JSON representation of
        duration](https://developers.google.com/protocol-buffers/docs/proto3#json)).
        """
        pass
    @property
    def dag_start_time(self) -> google.protobuf.timestamp_pb2.Timestamp:
        """Output only. DAG start time, only set for workflows with [dag_timeout][google.cloud.dataproc.v1.WorkflowMetadata.dag_timeout] when DAG
        begins.
        """
        pass
    @property
    def dag_end_time(self) -> google.protobuf.timestamp_pb2.Timestamp:
        """Output only. DAG end time, only set for workflows with [dag_timeout][google.cloud.dataproc.v1.WorkflowMetadata.dag_timeout] when DAG ends."""
        pass
    def __init__(self,
        *,
        template : typing.Text = ...,
        version : builtins.int = ...,
        create_cluster : typing.Optional[global___ClusterOperation] = ...,
        graph : typing.Optional[global___WorkflowGraph] = ...,
        delete_cluster : typing.Optional[global___ClusterOperation] = ...,
        state : global___WorkflowMetadata.State.ValueType = ...,
        cluster_name : typing.Text = ...,
        parameters : typing.Optional[typing.Mapping[typing.Text, typing.Text]] = ...,
        start_time : typing.Optional[google.protobuf.timestamp_pb2.Timestamp] = ...,
        end_time : typing.Optional[google.protobuf.timestamp_pb2.Timestamp] = ...,
        cluster_uuid : typing.Text = ...,
        dag_timeout : typing.Optional[google.protobuf.duration_pb2.Duration] = ...,
        dag_start_time : typing.Optional[google.protobuf.timestamp_pb2.Timestamp] = ...,
        dag_end_time : typing.Optional[google.protobuf.timestamp_pb2.Timestamp] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["create_cluster",b"create_cluster","dag_end_time",b"dag_end_time","dag_start_time",b"dag_start_time","dag_timeout",b"dag_timeout","delete_cluster",b"delete_cluster","end_time",b"end_time","graph",b"graph","start_time",b"start_time"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["cluster_name",b"cluster_name","cluster_uuid",b"cluster_uuid","create_cluster",b"create_cluster","dag_end_time",b"dag_end_time","dag_start_time",b"dag_start_time","dag_timeout",b"dag_timeout","delete_cluster",b"delete_cluster","end_time",b"end_time","graph",b"graph","parameters",b"parameters","start_time",b"start_time","state",b"state","template",b"template","version",b"version"]) -> None: ...
global___WorkflowMetadata = WorkflowMetadata

class ClusterOperation(google.protobuf.message.Message):
    """The cluster operation triggered by a workflow."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    OPERATION_ID_FIELD_NUMBER: builtins.int
    ERROR_FIELD_NUMBER: builtins.int
    DONE_FIELD_NUMBER: builtins.int
    operation_id: typing.Text = ...
    """Output only. The id of the cluster operation."""

    error: typing.Text = ...
    """Output only. Error, if operation failed."""

    done: builtins.bool = ...
    """Output only. Indicates the operation is done."""

    def __init__(self,
        *,
        operation_id : typing.Text = ...,
        error : typing.Text = ...,
        done : builtins.bool = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["done",b"done","error",b"error","operation_id",b"operation_id"]) -> None: ...
global___ClusterOperation = ClusterOperation

class WorkflowGraph(google.protobuf.message.Message):
    """The workflow graph."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    NODES_FIELD_NUMBER: builtins.int
    @property
    def nodes(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___WorkflowNode]:
        """Output only. The workflow nodes."""
        pass
    def __init__(self,
        *,
        nodes : typing.Optional[typing.Iterable[global___WorkflowNode]] = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["nodes",b"nodes"]) -> None: ...
global___WorkflowGraph = WorkflowGraph

class WorkflowNode(google.protobuf.message.Message):
    """The workflow node."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class _NodeState:
        ValueType = typing.NewType('ValueType', builtins.int)
        V: typing_extensions.TypeAlias = ValueType
    class _NodeStateEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_NodeState.ValueType], builtins.type):
        DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor = ...
        NODE_STATE_UNSPECIFIED: WorkflowNode.NodeState.ValueType = ...  # 0
        """State is unspecified."""

        BLOCKED: WorkflowNode.NodeState.ValueType = ...  # 1
        """The node is awaiting prerequisite node to finish."""

        RUNNABLE: WorkflowNode.NodeState.ValueType = ...  # 2
        """The node is runnable but not running."""

        RUNNING: WorkflowNode.NodeState.ValueType = ...  # 3
        """The node is running."""

        COMPLETED: WorkflowNode.NodeState.ValueType = ...  # 4
        """The node completed successfully."""

        FAILED: WorkflowNode.NodeState.ValueType = ...  # 5
        """The node failed. A node can be marked FAILED because
        its ancestor or peer failed.
        """

    class NodeState(_NodeState, metaclass=_NodeStateEnumTypeWrapper):
        """The workflow node state."""
        pass

    NODE_STATE_UNSPECIFIED: WorkflowNode.NodeState.ValueType = ...  # 0
    """State is unspecified."""

    BLOCKED: WorkflowNode.NodeState.ValueType = ...  # 1
    """The node is awaiting prerequisite node to finish."""

    RUNNABLE: WorkflowNode.NodeState.ValueType = ...  # 2
    """The node is runnable but not running."""

    RUNNING: WorkflowNode.NodeState.ValueType = ...  # 3
    """The node is running."""

    COMPLETED: WorkflowNode.NodeState.ValueType = ...  # 4
    """The node completed successfully."""

    FAILED: WorkflowNode.NodeState.ValueType = ...  # 5
    """The node failed. A node can be marked FAILED because
    its ancestor or peer failed.
    """


    STEP_ID_FIELD_NUMBER: builtins.int
    PREREQUISITE_STEP_IDS_FIELD_NUMBER: builtins.int
    JOB_ID_FIELD_NUMBER: builtins.int
    STATE_FIELD_NUMBER: builtins.int
    ERROR_FIELD_NUMBER: builtins.int
    step_id: typing.Text = ...
    """Output only. The name of the node."""

    @property
    def prerequisite_step_ids(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """Output only. Node's prerequisite nodes."""
        pass
    job_id: typing.Text = ...
    """Output only. The job id; populated after the node enters RUNNING state."""

    state: global___WorkflowNode.NodeState.ValueType = ...
    """Output only. The node state."""

    error: typing.Text = ...
    """Output only. The error detail."""

    def __init__(self,
        *,
        step_id : typing.Text = ...,
        prerequisite_step_ids : typing.Optional[typing.Iterable[typing.Text]] = ...,
        job_id : typing.Text = ...,
        state : global___WorkflowNode.NodeState.ValueType = ...,
        error : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["error",b"error","job_id",b"job_id","prerequisite_step_ids",b"prerequisite_step_ids","state",b"state","step_id",b"step_id"]) -> None: ...
global___WorkflowNode = WorkflowNode

class CreateWorkflowTemplateRequest(google.protobuf.message.Message):
    """A request to create a workflow template."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    PARENT_FIELD_NUMBER: builtins.int
    TEMPLATE_FIELD_NUMBER: builtins.int
    parent: typing.Text = ...
    """Required. The resource name of the region or location, as described
    in https://cloud.google.com/apis/design/resource_names.

    * For `projects.regions.workflowTemplates.create`, the resource name of the
      region has the following format:
      `projects/{project_id}/regions/{region}`

    * For `projects.locations.workflowTemplates.create`, the resource name of
      the location has the following format:
      `projects/{project_id}/locations/{location}`
    """

    @property
    def template(self) -> global___WorkflowTemplate:
        """Required. The Dataproc workflow template to create."""
        pass
    def __init__(self,
        *,
        parent : typing.Text = ...,
        template : typing.Optional[global___WorkflowTemplate] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["template",b"template"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["parent",b"parent","template",b"template"]) -> None: ...
global___CreateWorkflowTemplateRequest = CreateWorkflowTemplateRequest

class GetWorkflowTemplateRequest(google.protobuf.message.Message):
    """A request to fetch a workflow template."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    NAME_FIELD_NUMBER: builtins.int
    VERSION_FIELD_NUMBER: builtins.int
    name: typing.Text = ...
    """Required. The resource name of the workflow template, as described
    in https://cloud.google.com/apis/design/resource_names.

    * For `projects.regions.workflowTemplates.get`, the resource name of the
      template has the following format:
      `projects/{project_id}/regions/{region}/workflowTemplates/{template_id}`

    * For `projects.locations.workflowTemplates.get`, the resource name of the
      template has the following format:
      `projects/{project_id}/locations/{location}/workflowTemplates/{template_id}`
    """

    version: builtins.int = ...
    """Optional. The version of workflow template to retrieve. Only previously
    instantiated versions can be retrieved.

    If unspecified, retrieves the current version.
    """

    def __init__(self,
        *,
        name : typing.Text = ...,
        version : builtins.int = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["name",b"name","version",b"version"]) -> None: ...
global___GetWorkflowTemplateRequest = GetWorkflowTemplateRequest

class InstantiateWorkflowTemplateRequest(google.protobuf.message.Message):
    """A request to instantiate a workflow template."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class ParametersEntry(google.protobuf.message.Message):
        DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
        KEY_FIELD_NUMBER: builtins.int
        VALUE_FIELD_NUMBER: builtins.int
        key: typing.Text = ...
        value: typing.Text = ...
        def __init__(self,
            *,
            key : typing.Text = ...,
            value : typing.Text = ...,
            ) -> None: ...
        def ClearField(self, field_name: typing_extensions.Literal["key",b"key","value",b"value"]) -> None: ...

    NAME_FIELD_NUMBER: builtins.int
    VERSION_FIELD_NUMBER: builtins.int
    REQUEST_ID_FIELD_NUMBER: builtins.int
    PARAMETERS_FIELD_NUMBER: builtins.int
    name: typing.Text = ...
    """Required. The resource name of the workflow template, as described
    in https://cloud.google.com/apis/design/resource_names.

    * For `projects.regions.workflowTemplates.instantiate`, the resource name
    of the template has the following format:
      `projects/{project_id}/regions/{region}/workflowTemplates/{template_id}`

    * For `projects.locations.workflowTemplates.instantiate`, the resource name
      of the template has the following format:
      `projects/{project_id}/locations/{location}/workflowTemplates/{template_id}`
    """

    version: builtins.int = ...
    """Optional. The version of workflow template to instantiate. If specified,
    the workflow will be instantiated only if the current version of
    the workflow template has the supplied version.

    This option cannot be used to instantiate a previous version of
    workflow template.
    """

    request_id: typing.Text = ...
    """Optional. A tag that prevents multiple concurrent workflow
    instances with the same tag from running. This mitigates risk of
    concurrent instances started due to retries.

    It is recommended to always set this value to a
    [UUID](https://en.wikipedia.org/wiki/Universally_unique_identifier).

    The tag must contain only letters (a-z, A-Z), numbers (0-9),
    underscores (_), and hyphens (-). The maximum length is 40 characters.
    """

    @property
    def parameters(self) -> google.protobuf.internal.containers.ScalarMap[typing.Text, typing.Text]:
        """Optional. Map from parameter names to values that should be used for those
        parameters. Values may not exceed 1000 characters.
        """
        pass
    def __init__(self,
        *,
        name : typing.Text = ...,
        version : builtins.int = ...,
        request_id : typing.Text = ...,
        parameters : typing.Optional[typing.Mapping[typing.Text, typing.Text]] = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["name",b"name","parameters",b"parameters","request_id",b"request_id","version",b"version"]) -> None: ...
global___InstantiateWorkflowTemplateRequest = InstantiateWorkflowTemplateRequest

class InstantiateInlineWorkflowTemplateRequest(google.protobuf.message.Message):
    """A request to instantiate an inline workflow template."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    PARENT_FIELD_NUMBER: builtins.int
    TEMPLATE_FIELD_NUMBER: builtins.int
    REQUEST_ID_FIELD_NUMBER: builtins.int
    parent: typing.Text = ...
    """Required. The resource name of the region or location, as described
    in https://cloud.google.com/apis/design/resource_names.

    * For `projects.regions.workflowTemplates,instantiateinline`, the resource
      name of the region has the following format:
      `projects/{project_id}/regions/{region}`

    * For `projects.locations.workflowTemplates.instantiateinline`, the
      resource name of the location has the following format:
      `projects/{project_id}/locations/{location}`
    """

    @property
    def template(self) -> global___WorkflowTemplate:
        """Required. The workflow template to instantiate."""
        pass
    request_id: typing.Text = ...
    """Optional. A tag that prevents multiple concurrent workflow
    instances with the same tag from running. This mitigates risk of
    concurrent instances started due to retries.

    It is recommended to always set this value to a
    [UUID](https://en.wikipedia.org/wiki/Universally_unique_identifier).

    The tag must contain only letters (a-z, A-Z), numbers (0-9),
    underscores (_), and hyphens (-). The maximum length is 40 characters.
    """

    def __init__(self,
        *,
        parent : typing.Text = ...,
        template : typing.Optional[global___WorkflowTemplate] = ...,
        request_id : typing.Text = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["template",b"template"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["parent",b"parent","request_id",b"request_id","template",b"template"]) -> None: ...
global___InstantiateInlineWorkflowTemplateRequest = InstantiateInlineWorkflowTemplateRequest

class UpdateWorkflowTemplateRequest(google.protobuf.message.Message):
    """A request to update a workflow template."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    TEMPLATE_FIELD_NUMBER: builtins.int
    @property
    def template(self) -> global___WorkflowTemplate:
        """Required. The updated workflow template.

        The `template.version` field must match the current version.
        """
        pass
    def __init__(self,
        *,
        template : typing.Optional[global___WorkflowTemplate] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["template",b"template"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["template",b"template"]) -> None: ...
global___UpdateWorkflowTemplateRequest = UpdateWorkflowTemplateRequest

class ListWorkflowTemplatesRequest(google.protobuf.message.Message):
    """A request to list workflow templates in a project."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    PARENT_FIELD_NUMBER: builtins.int
    PAGE_SIZE_FIELD_NUMBER: builtins.int
    PAGE_TOKEN_FIELD_NUMBER: builtins.int
    parent: typing.Text = ...
    """Required. The resource name of the region or location, as described
    in https://cloud.google.com/apis/design/resource_names.

    * For `projects.regions.workflowTemplates,list`, the resource
      name of the region has the following format:
      `projects/{project_id}/regions/{region}`

    * For `projects.locations.workflowTemplates.list`, the
      resource name of the location has the following format:
      `projects/{project_id}/locations/{location}`
    """

    page_size: builtins.int = ...
    """Optional. The maximum number of results to return in each response."""

    page_token: typing.Text = ...
    """Optional. The page token, returned by a previous call, to request the
    next page of results.
    """

    def __init__(self,
        *,
        parent : typing.Text = ...,
        page_size : builtins.int = ...,
        page_token : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["page_size",b"page_size","page_token",b"page_token","parent",b"parent"]) -> None: ...
global___ListWorkflowTemplatesRequest = ListWorkflowTemplatesRequest

class ListWorkflowTemplatesResponse(google.protobuf.message.Message):
    """A response to a request to list workflow templates in a project."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    TEMPLATES_FIELD_NUMBER: builtins.int
    NEXT_PAGE_TOKEN_FIELD_NUMBER: builtins.int
    @property
    def templates(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___WorkflowTemplate]:
        """Output only. WorkflowTemplates list."""
        pass
    next_page_token: typing.Text = ...
    """Output only. This token is included in the response if there are more
    results to fetch. To fetch additional results, provide this value as the
    page_token in a subsequent <code>ListWorkflowTemplatesRequest</code>.
    """

    def __init__(self,
        *,
        templates : typing.Optional[typing.Iterable[global___WorkflowTemplate]] = ...,
        next_page_token : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["next_page_token",b"next_page_token","templates",b"templates"]) -> None: ...
global___ListWorkflowTemplatesResponse = ListWorkflowTemplatesResponse

class DeleteWorkflowTemplateRequest(google.protobuf.message.Message):
    """A request to delete a workflow template.

    Currently started workflows will remain running.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    NAME_FIELD_NUMBER: builtins.int
    VERSION_FIELD_NUMBER: builtins.int
    name: typing.Text = ...
    """Required. The resource name of the workflow template, as described
    in https://cloud.google.com/apis/design/resource_names.

    * For `projects.regions.workflowTemplates.delete`, the resource name
    of the template has the following format:
      `projects/{project_id}/regions/{region}/workflowTemplates/{template_id}`

    * For `projects.locations.workflowTemplates.instantiate`, the resource name
      of the template has the following format:
      `projects/{project_id}/locations/{location}/workflowTemplates/{template_id}`
    """

    version: builtins.int = ...
    """Optional. The version of workflow template to delete. If specified,
    will only delete the template if the current server version matches
    specified version.
    """

    def __init__(self,
        *,
        name : typing.Text = ...,
        version : builtins.int = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["name",b"name","version",b"version"]) -> None: ...
global___DeleteWorkflowTemplateRequest = DeleteWorkflowTemplateRequest
