"""
@generated by mypy-protobuf.  Do not edit manually!
isort:skip_file
"""
import builtins
import google.cloud.automl.v1beta1.temporal_pb2
import google.protobuf.descriptor
import google.protobuf.internal.containers
import google.protobuf.internal.enum_type_wrapper
import google.protobuf.message
import typing
import typing_extensions

DESCRIPTOR: google.protobuf.descriptor.FileDescriptor = ...

class _ClassificationType:
    ValueType = typing.NewType('ValueType', builtins.int)
    V: typing_extensions.TypeAlias = ValueType
class _ClassificationTypeEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_ClassificationType.ValueType], builtins.type):
    DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor = ...
    CLASSIFICATION_TYPE_UNSPECIFIED: ClassificationType.ValueType = ...  # 0
    """An un-set value of this enum."""

    MULTICLASS: ClassificationType.ValueType = ...  # 1
    """At most one label is allowed per example."""

    MULTILABEL: ClassificationType.ValueType = ...  # 2
    """Multiple labels are allowed for one example."""

class ClassificationType(_ClassificationType, metaclass=_ClassificationTypeEnumTypeWrapper):
    """Type of the classification problem."""
    pass

CLASSIFICATION_TYPE_UNSPECIFIED: ClassificationType.ValueType = ...  # 0
"""An un-set value of this enum."""

MULTICLASS: ClassificationType.ValueType = ...  # 1
"""At most one label is allowed per example."""

MULTILABEL: ClassificationType.ValueType = ...  # 2
"""Multiple labels are allowed for one example."""

global___ClassificationType = ClassificationType


class ClassificationAnnotation(google.protobuf.message.Message):
    """Contains annotation details specific to classification."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    SCORE_FIELD_NUMBER: builtins.int
    score: builtins.float = ...
    """Output only. A confidence estimate between 0.0 and 1.0. A higher value
    means greater confidence that the annotation is positive. If a user
    approves an annotation as negative or positive, the score value remains
    unchanged. If a user creates an annotation, the score is 0 for negative or
    1 for positive.
    """

    def __init__(self,
        *,
        score : builtins.float = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["score",b"score"]) -> None: ...
global___ClassificationAnnotation = ClassificationAnnotation

class VideoClassificationAnnotation(google.protobuf.message.Message):
    """Contains annotation details specific to video classification."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    TYPE_FIELD_NUMBER: builtins.int
    CLASSIFICATION_ANNOTATION_FIELD_NUMBER: builtins.int
    TIME_SEGMENT_FIELD_NUMBER: builtins.int
    type: typing.Text = ...
    """Output only. Expresses the type of video classification. Possible values:

    *  `segment` - Classification done on a specified by user
           time segment of a video. AnnotationSpec is answered to be present
           in that time segment, if it is present in any part of it. The video
           ML model evaluations are done only for this type of classification.

    *  `shot`- Shot-level classification.
           AutoML Video Intelligence determines the boundaries
           for each camera shot in the entire segment of the video that user
           specified in the request configuration. AutoML Video Intelligence
           then returns labels and their confidence scores for each detected
           shot, along with the start and end time of the shot.
           WARNING: Model evaluation is not done for this classification type,
           the quality of it depends on training data, but there are no
           metrics provided to describe that quality.

    *  `1s_interval` - AutoML Video Intelligence returns labels and their
           confidence scores for each second of the entire segment of the video
           that user specified in the request configuration.
           WARNING: Model evaluation is not done for this classification type,
           the quality of it depends on training data, but there are no
           metrics provided to describe that quality.
    """

    @property
    def classification_annotation(self) -> global___ClassificationAnnotation:
        """Output only . The classification details of this annotation."""
        pass
    @property
    def time_segment(self) -> google.cloud.automl.v1beta1.temporal_pb2.TimeSegment:
        """Output only . The time segment of the video to which the
        annotation applies.
        """
        pass
    def __init__(self,
        *,
        type : typing.Text = ...,
        classification_annotation : typing.Optional[global___ClassificationAnnotation] = ...,
        time_segment : typing.Optional[google.cloud.automl.v1beta1.temporal_pb2.TimeSegment] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["classification_annotation",b"classification_annotation","time_segment",b"time_segment"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["classification_annotation",b"classification_annotation","time_segment",b"time_segment","type",b"type"]) -> None: ...
global___VideoClassificationAnnotation = VideoClassificationAnnotation

class ClassificationEvaluationMetrics(google.protobuf.message.Message):
    """Model evaluation metrics for classification problems.
    Note: For Video Classification this metrics only describe quality of the
    Video Classification predictions of "segment_classification" type.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class ConfidenceMetricsEntry(google.protobuf.message.Message):
        """Metrics for a single confidence threshold."""
        DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
        CONFIDENCE_THRESHOLD_FIELD_NUMBER: builtins.int
        POSITION_THRESHOLD_FIELD_NUMBER: builtins.int
        RECALL_FIELD_NUMBER: builtins.int
        PRECISION_FIELD_NUMBER: builtins.int
        FALSE_POSITIVE_RATE_FIELD_NUMBER: builtins.int
        F1_SCORE_FIELD_NUMBER: builtins.int
        RECALL_AT1_FIELD_NUMBER: builtins.int
        PRECISION_AT1_FIELD_NUMBER: builtins.int
        FALSE_POSITIVE_RATE_AT1_FIELD_NUMBER: builtins.int
        F1_SCORE_AT1_FIELD_NUMBER: builtins.int
        TRUE_POSITIVE_COUNT_FIELD_NUMBER: builtins.int
        FALSE_POSITIVE_COUNT_FIELD_NUMBER: builtins.int
        FALSE_NEGATIVE_COUNT_FIELD_NUMBER: builtins.int
        TRUE_NEGATIVE_COUNT_FIELD_NUMBER: builtins.int
        confidence_threshold: builtins.float = ...
        """Output only. Metrics are computed with an assumption that the model
        never returns predictions with score lower than this value.
        """

        position_threshold: builtins.int = ...
        """Output only. Metrics are computed with an assumption that the model
        always returns at most this many predictions (ordered by their score,
        descendingly), but they all still need to meet the confidence_threshold.
        """

        recall: builtins.float = ...
        """Output only. Recall (True Positive Rate) for the given confidence
        threshold.
        """

        precision: builtins.float = ...
        """Output only. Precision for the given confidence threshold."""

        false_positive_rate: builtins.float = ...
        """Output only. False Positive Rate for the given confidence threshold."""

        f1_score: builtins.float = ...
        """Output only. The harmonic mean of recall and precision."""

        recall_at1: builtins.float = ...
        """Output only. The Recall (True Positive Rate) when only considering the
        label that has the highest prediction score and not below the confidence
        threshold for each example.
        """

        precision_at1: builtins.float = ...
        """Output only. The precision when only considering the label that has the
        highest prediction score and not below the confidence threshold for each
        example.
        """

        false_positive_rate_at1: builtins.float = ...
        """Output only. The False Positive Rate when only considering the label that
        has the highest prediction score and not below the confidence threshold
        for each example.
        """

        f1_score_at1: builtins.float = ...
        """Output only. The harmonic mean of [recall_at1][google.cloud.automl.v1beta1.ClassificationEvaluationMetrics.ConfidenceMetricsEntry.recall_at1] and [precision_at1][google.cloud.automl.v1beta1.ClassificationEvaluationMetrics.ConfidenceMetricsEntry.precision_at1]."""

        true_positive_count: builtins.int = ...
        """Output only. The number of model created labels that match a ground truth
        label.
        """

        false_positive_count: builtins.int = ...
        """Output only. The number of model created labels that do not match a
        ground truth label.
        """

        false_negative_count: builtins.int = ...
        """Output only. The number of ground truth labels that are not matched
        by a model created label.
        """

        true_negative_count: builtins.int = ...
        """Output only. The number of labels that were not created by the model,
        but if they would, they would not match a ground truth label.
        """

        def __init__(self,
            *,
            confidence_threshold : builtins.float = ...,
            position_threshold : builtins.int = ...,
            recall : builtins.float = ...,
            precision : builtins.float = ...,
            false_positive_rate : builtins.float = ...,
            f1_score : builtins.float = ...,
            recall_at1 : builtins.float = ...,
            precision_at1 : builtins.float = ...,
            false_positive_rate_at1 : builtins.float = ...,
            f1_score_at1 : builtins.float = ...,
            true_positive_count : builtins.int = ...,
            false_positive_count : builtins.int = ...,
            false_negative_count : builtins.int = ...,
            true_negative_count : builtins.int = ...,
            ) -> None: ...
        def ClearField(self, field_name: typing_extensions.Literal["confidence_threshold",b"confidence_threshold","f1_score",b"f1_score","f1_score_at1",b"f1_score_at1","false_negative_count",b"false_negative_count","false_positive_count",b"false_positive_count","false_positive_rate",b"false_positive_rate","false_positive_rate_at1",b"false_positive_rate_at1","position_threshold",b"position_threshold","precision",b"precision","precision_at1",b"precision_at1","recall",b"recall","recall_at1",b"recall_at1","true_negative_count",b"true_negative_count","true_positive_count",b"true_positive_count"]) -> None: ...

    class ConfusionMatrix(google.protobuf.message.Message):
        """Confusion matrix of the model running the classification."""
        DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
        class Row(google.protobuf.message.Message):
            """Output only. A row in the confusion matrix."""
            DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
            EXAMPLE_COUNT_FIELD_NUMBER: builtins.int
            @property
            def example_count(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[builtins.int]:
                """Output only. Value of the specific cell in the confusion matrix.
                The number of values each row has (i.e. the length of the row) is equal
                to the length of the `annotation_spec_id` field or, if that one is not
                populated, length of the [display_name][google.cloud.automl.v1beta1.ClassificationEvaluationMetrics.ConfusionMatrix.display_name] field.
                """
                pass
            def __init__(self,
                *,
                example_count : typing.Optional[typing.Iterable[builtins.int]] = ...,
                ) -> None: ...
            def ClearField(self, field_name: typing_extensions.Literal["example_count",b"example_count"]) -> None: ...

        ANNOTATION_SPEC_ID_FIELD_NUMBER: builtins.int
        DISPLAY_NAME_FIELD_NUMBER: builtins.int
        ROW_FIELD_NUMBER: builtins.int
        @property
        def annotation_spec_id(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
            """Output only. IDs of the annotation specs used in the confusion matrix.
            For Tables CLASSIFICATION

            [prediction_type][google.cloud.automl.v1beta1.TablesModelMetadata.prediction_type]
            only list of [annotation_spec_display_name-s][] is populated.
            """
            pass
        @property
        def display_name(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
            """Output only. Display name of the annotation specs used in the confusion
            matrix, as they were at the moment of the evaluation. For Tables
            CLASSIFICATION

            [prediction_type-s][google.cloud.automl.v1beta1.TablesModelMetadata.prediction_type],
            distinct values of the target column at the moment of the model
            evaluation are populated here.
            """
            pass
        @property
        def row(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___ClassificationEvaluationMetrics.ConfusionMatrix.Row]:
            """Output only. Rows in the confusion matrix. The number of rows is equal to
            the size of `annotation_spec_id`.
            `row[i].example_count[j]` is the number of examples that have ground
            truth of the `annotation_spec_id[i]` and are predicted as
            `annotation_spec_id[j]` by the model being evaluated.
            """
            pass
        def __init__(self,
            *,
            annotation_spec_id : typing.Optional[typing.Iterable[typing.Text]] = ...,
            display_name : typing.Optional[typing.Iterable[typing.Text]] = ...,
            row : typing.Optional[typing.Iterable[global___ClassificationEvaluationMetrics.ConfusionMatrix.Row]] = ...,
            ) -> None: ...
        def ClearField(self, field_name: typing_extensions.Literal["annotation_spec_id",b"annotation_spec_id","display_name",b"display_name","row",b"row"]) -> None: ...

    AU_PRC_FIELD_NUMBER: builtins.int
    BASE_AU_PRC_FIELD_NUMBER: builtins.int
    AU_ROC_FIELD_NUMBER: builtins.int
    LOG_LOSS_FIELD_NUMBER: builtins.int
    CONFIDENCE_METRICS_ENTRY_FIELD_NUMBER: builtins.int
    CONFUSION_MATRIX_FIELD_NUMBER: builtins.int
    ANNOTATION_SPEC_ID_FIELD_NUMBER: builtins.int
    au_prc: builtins.float = ...
    """Output only. The Area Under Precision-Recall Curve metric. Micro-averaged
    for the overall evaluation.
    """

    base_au_prc: builtins.float = ...
    """Output only. The Area Under Precision-Recall Curve metric based on priors.
    Micro-averaged for the overall evaluation.
    Deprecated.
    """

    au_roc: builtins.float = ...
    """Output only. The Area Under Receiver Operating Characteristic curve metric.
    Micro-averaged for the overall evaluation.
    """

    log_loss: builtins.float = ...
    """Output only. The Log Loss metric."""

    @property
    def confidence_metrics_entry(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___ClassificationEvaluationMetrics.ConfidenceMetricsEntry]:
        """Output only. Metrics for each confidence_threshold in
        0.00,0.05,0.10,...,0.95,0.96,0.97,0.98,0.99 and
        position_threshold = INT32_MAX_VALUE.
        ROC and precision-recall curves, and other aggregated metrics are derived
        from them. The confidence metrics entries may also be supplied for
        additional values of position_threshold, but from these no aggregated
        metrics are computed.
        """
        pass
    @property
    def confusion_matrix(self) -> global___ClassificationEvaluationMetrics.ConfusionMatrix:
        """Output only. Confusion matrix of the evaluation.
        Only set for MULTICLASS classification problems where number
        of labels is no more than 10.
        Only set for model level evaluation, not for evaluation per label.
        """
        pass
    @property
    def annotation_spec_id(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """Output only. The annotation spec ids used for this evaluation."""
        pass
    def __init__(self,
        *,
        au_prc : builtins.float = ...,
        base_au_prc : builtins.float = ...,
        au_roc : builtins.float = ...,
        log_loss : builtins.float = ...,
        confidence_metrics_entry : typing.Optional[typing.Iterable[global___ClassificationEvaluationMetrics.ConfidenceMetricsEntry]] = ...,
        confusion_matrix : typing.Optional[global___ClassificationEvaluationMetrics.ConfusionMatrix] = ...,
        annotation_spec_id : typing.Optional[typing.Iterable[typing.Text]] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["confusion_matrix",b"confusion_matrix"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["annotation_spec_id",b"annotation_spec_id","au_prc",b"au_prc","au_roc",b"au_roc","base_au_prc",b"base_au_prc","confidence_metrics_entry",b"confidence_metrics_entry","confusion_matrix",b"confusion_matrix","log_loss",b"log_loss"]) -> None: ...
global___ClassificationEvaluationMetrics = ClassificationEvaluationMetrics
