"""
@generated by mypy-protobuf.  Do not edit manually!
isort:skip_file
"""
import builtins
import google.cloud.automl.v1beta1.classification_pb2
import google.cloud.automl.v1beta1.detection_pb2
import google.cloud.automl.v1beta1.regression_pb2
import google.cloud.automl.v1beta1.text_extraction_pb2
import google.cloud.automl.v1beta1.text_sentiment_pb2
import google.cloud.automl.v1beta1.translation_pb2
import google.protobuf.descriptor
import google.protobuf.message
import google.protobuf.timestamp_pb2
import typing
import typing_extensions

DESCRIPTOR: google.protobuf.descriptor.FileDescriptor = ...

class ModelEvaluation(google.protobuf.message.Message):
    """Evaluation results of a model."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    CLASSIFICATION_EVALUATION_METRICS_FIELD_NUMBER: builtins.int
    REGRESSION_EVALUATION_METRICS_FIELD_NUMBER: builtins.int
    TRANSLATION_EVALUATION_METRICS_FIELD_NUMBER: builtins.int
    IMAGE_OBJECT_DETECTION_EVALUATION_METRICS_FIELD_NUMBER: builtins.int
    VIDEO_OBJECT_TRACKING_EVALUATION_METRICS_FIELD_NUMBER: builtins.int
    TEXT_SENTIMENT_EVALUATION_METRICS_FIELD_NUMBER: builtins.int
    TEXT_EXTRACTION_EVALUATION_METRICS_FIELD_NUMBER: builtins.int
    NAME_FIELD_NUMBER: builtins.int
    ANNOTATION_SPEC_ID_FIELD_NUMBER: builtins.int
    DISPLAY_NAME_FIELD_NUMBER: builtins.int
    CREATE_TIME_FIELD_NUMBER: builtins.int
    EVALUATED_EXAMPLE_COUNT_FIELD_NUMBER: builtins.int
    @property
    def classification_evaluation_metrics(self) -> google.cloud.automl.v1beta1.classification_pb2.ClassificationEvaluationMetrics:
        """Model evaluation metrics for image, text, video and tables
        classification.
        Tables problem is considered a classification when the target column
        is CATEGORY DataType.
        """
        pass
    @property
    def regression_evaluation_metrics(self) -> google.cloud.automl.v1beta1.regression_pb2.RegressionEvaluationMetrics:
        """Model evaluation metrics for Tables regression.
        Tables problem is considered a regression when the target column
        has FLOAT64 DataType.
        """
        pass
    @property
    def translation_evaluation_metrics(self) -> google.cloud.automl.v1beta1.translation_pb2.TranslationEvaluationMetrics:
        """Model evaluation metrics for translation."""
        pass
    @property
    def image_object_detection_evaluation_metrics(self) -> google.cloud.automl.v1beta1.detection_pb2.ImageObjectDetectionEvaluationMetrics:
        """Model evaluation metrics for image object detection."""
        pass
    @property
    def video_object_tracking_evaluation_metrics(self) -> google.cloud.automl.v1beta1.detection_pb2.VideoObjectTrackingEvaluationMetrics:
        """Model evaluation metrics for video object tracking."""
        pass
    @property
    def text_sentiment_evaluation_metrics(self) -> google.cloud.automl.v1beta1.text_sentiment_pb2.TextSentimentEvaluationMetrics:
        """Evaluation metrics for text sentiment models."""
        pass
    @property
    def text_extraction_evaluation_metrics(self) -> google.cloud.automl.v1beta1.text_extraction_pb2.TextExtractionEvaluationMetrics:
        """Evaluation metrics for text extraction models."""
        pass
    name: typing.Text = ...
    """Output only. Resource name of the model evaluation.
    Format:

    `projects/{project_id}/locations/{location_id}/models/{model_id}/modelEvaluations/{model_evaluation_id}`
    """

    annotation_spec_id: typing.Text = ...
    """Output only. The ID of the annotation spec that the model evaluation applies to. The
    The ID is empty for the overall model evaluation.
    For Tables annotation specs in the dataset do not exist and this ID is
    always not set, but for CLASSIFICATION

    [prediction_type-s][google.cloud.automl.v1beta1.TablesModelMetadata.prediction_type]
    the
    [display_name][google.cloud.automl.v1beta1.ModelEvaluation.display_name]
    field is used.
    """

    display_name: typing.Text = ...
    """Output only. The value of
    [display_name][google.cloud.automl.v1beta1.AnnotationSpec.display_name] at
    the moment when the model was trained. Because this field returns a value
    at model training time, for different models trained from the same dataset,
    the values may differ, since display names could had been changed between
    the two model's trainings.
    For Tables CLASSIFICATION

    [prediction_type-s][google.cloud.automl.v1beta1.TablesModelMetadata.prediction_type]
    distinct values of the target column at the moment of the model evaluation
    are populated here.
    The display_name is empty for the overall model evaluation.
    """

    @property
    def create_time(self) -> google.protobuf.timestamp_pb2.Timestamp:
        """Output only. Timestamp when this model evaluation was created."""
        pass
    evaluated_example_count: builtins.int = ...
    """Output only. The number of examples used for model evaluation, i.e. for
    which ground truth from time of model creation is compared against the
    predicted annotations created by the model.
    For overall ModelEvaluation (i.e. with annotation_spec_id not set) this is
    the total number of all examples used for evaluation.
    Otherwise, this is the count of examples that according to the ground
    truth were annotated by the

    [annotation_spec_id][google.cloud.automl.v1beta1.ModelEvaluation.annotation_spec_id].
    """

    def __init__(self,
        *,
        classification_evaluation_metrics : typing.Optional[google.cloud.automl.v1beta1.classification_pb2.ClassificationEvaluationMetrics] = ...,
        regression_evaluation_metrics : typing.Optional[google.cloud.automl.v1beta1.regression_pb2.RegressionEvaluationMetrics] = ...,
        translation_evaluation_metrics : typing.Optional[google.cloud.automl.v1beta1.translation_pb2.TranslationEvaluationMetrics] = ...,
        image_object_detection_evaluation_metrics : typing.Optional[google.cloud.automl.v1beta1.detection_pb2.ImageObjectDetectionEvaluationMetrics] = ...,
        video_object_tracking_evaluation_metrics : typing.Optional[google.cloud.automl.v1beta1.detection_pb2.VideoObjectTrackingEvaluationMetrics] = ...,
        text_sentiment_evaluation_metrics : typing.Optional[google.cloud.automl.v1beta1.text_sentiment_pb2.TextSentimentEvaluationMetrics] = ...,
        text_extraction_evaluation_metrics : typing.Optional[google.cloud.automl.v1beta1.text_extraction_pb2.TextExtractionEvaluationMetrics] = ...,
        name : typing.Text = ...,
        annotation_spec_id : typing.Text = ...,
        display_name : typing.Text = ...,
        create_time : typing.Optional[google.protobuf.timestamp_pb2.Timestamp] = ...,
        evaluated_example_count : builtins.int = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["classification_evaluation_metrics",b"classification_evaluation_metrics","create_time",b"create_time","image_object_detection_evaluation_metrics",b"image_object_detection_evaluation_metrics","metrics",b"metrics","regression_evaluation_metrics",b"regression_evaluation_metrics","text_extraction_evaluation_metrics",b"text_extraction_evaluation_metrics","text_sentiment_evaluation_metrics",b"text_sentiment_evaluation_metrics","translation_evaluation_metrics",b"translation_evaluation_metrics","video_object_tracking_evaluation_metrics",b"video_object_tracking_evaluation_metrics"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["annotation_spec_id",b"annotation_spec_id","classification_evaluation_metrics",b"classification_evaluation_metrics","create_time",b"create_time","display_name",b"display_name","evaluated_example_count",b"evaluated_example_count","image_object_detection_evaluation_metrics",b"image_object_detection_evaluation_metrics","metrics",b"metrics","name",b"name","regression_evaluation_metrics",b"regression_evaluation_metrics","text_extraction_evaluation_metrics",b"text_extraction_evaluation_metrics","text_sentiment_evaluation_metrics",b"text_sentiment_evaluation_metrics","translation_evaluation_metrics",b"translation_evaluation_metrics","video_object_tracking_evaluation_metrics",b"video_object_tracking_evaluation_metrics"]) -> None: ...
    def WhichOneof(self, oneof_group: typing_extensions.Literal["metrics",b"metrics"]) -> typing.Optional[typing_extensions.Literal["classification_evaluation_metrics","regression_evaluation_metrics","translation_evaluation_metrics","image_object_detection_evaluation_metrics","video_object_tracking_evaluation_metrics","text_sentiment_evaluation_metrics","text_extraction_evaluation_metrics"]]: ...
global___ModelEvaluation = ModelEvaluation
