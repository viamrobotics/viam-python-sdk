"""
@generated by mypy-protobuf.  Do not edit manually!
isort:skip_file
"""
import builtins
import google.cloud.aiplatform.v1beta1.completion_stats_pb2
import google.cloud.aiplatform.v1beta1.encryption_spec_pb2
import google.cloud.aiplatform.v1beta1.explanation_pb2
import google.cloud.aiplatform.v1beta1.io_pb2
import google.cloud.aiplatform.v1beta1.job_state_pb2
import google.cloud.aiplatform.v1beta1.machine_resources_pb2
import google.cloud.aiplatform.v1beta1.manual_batch_tuning_parameters_pb2
import google.cloud.aiplatform.v1beta1.unmanaged_container_model_pb2
import google.protobuf.descriptor
import google.protobuf.internal.containers
import google.protobuf.message
import google.protobuf.struct_pb2
import google.protobuf.timestamp_pb2
import google.rpc.status_pb2
import typing
import typing_extensions

DESCRIPTOR: google.protobuf.descriptor.FileDescriptor = ...

class BatchPredictionJob(google.protobuf.message.Message):
    """A job that uses a [Model][google.cloud.aiplatform.v1beta1.BatchPredictionJob.model] to produce predictions
    on multiple [input instances][google.cloud.aiplatform.v1beta1.BatchPredictionJob.input_config]. If
    predictions for significant portion of the instances fail, the job may finish
    without attempting predictions for all remaining instances.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class InputConfig(google.protobuf.message.Message):
        """Configures the input to [BatchPredictionJob][google.cloud.aiplatform.v1beta1.BatchPredictionJob].
        See [Model.supported_input_storage_formats][google.cloud.aiplatform.v1beta1.Model.supported_input_storage_formats] for Model's supported input
        formats, and how instances should be expressed via any of them.
        """
        DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
        GCS_SOURCE_FIELD_NUMBER: builtins.int
        BIGQUERY_SOURCE_FIELD_NUMBER: builtins.int
        INSTANCES_FORMAT_FIELD_NUMBER: builtins.int
        @property
        def gcs_source(self) -> google.cloud.aiplatform.v1beta1.io_pb2.GcsSource:
            """The Cloud Storage location for the input instances."""
            pass
        @property
        def bigquery_source(self) -> google.cloud.aiplatform.v1beta1.io_pb2.BigQuerySource:
            """The BigQuery location of the input table.
            The schema of the table should be in the format described by the given
            context OpenAPI Schema, if one is provided. The table may contain
            additional columns that are not described by the schema, and they will
            be ignored.
            """
            pass
        instances_format: typing.Text = ...
        """Required. The format in which instances are given, must be one of the
        [Model's][google.cloud.aiplatform.v1beta1.BatchPredictionJob.model]
        [supported_input_storage_formats][google.cloud.aiplatform.v1beta1.Model.supported_input_storage_formats].
        """

        def __init__(self,
            *,
            gcs_source : typing.Optional[google.cloud.aiplatform.v1beta1.io_pb2.GcsSource] = ...,
            bigquery_source : typing.Optional[google.cloud.aiplatform.v1beta1.io_pb2.BigQuerySource] = ...,
            instances_format : typing.Text = ...,
            ) -> None: ...
        def HasField(self, field_name: typing_extensions.Literal["bigquery_source",b"bigquery_source","gcs_source",b"gcs_source","source",b"source"]) -> builtins.bool: ...
        def ClearField(self, field_name: typing_extensions.Literal["bigquery_source",b"bigquery_source","gcs_source",b"gcs_source","instances_format",b"instances_format","source",b"source"]) -> None: ...
        def WhichOneof(self, oneof_group: typing_extensions.Literal["source",b"source"]) -> typing.Optional[typing_extensions.Literal["gcs_source","bigquery_source"]]: ...

    class OutputConfig(google.protobuf.message.Message):
        """Configures the output of [BatchPredictionJob][google.cloud.aiplatform.v1beta1.BatchPredictionJob].
        See [Model.supported_output_storage_formats][google.cloud.aiplatform.v1beta1.Model.supported_output_storage_formats] for supported output
        formats, and how predictions are expressed via any of them.
        """
        DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
        GCS_DESTINATION_FIELD_NUMBER: builtins.int
        BIGQUERY_DESTINATION_FIELD_NUMBER: builtins.int
        PREDICTIONS_FORMAT_FIELD_NUMBER: builtins.int
        @property
        def gcs_destination(self) -> google.cloud.aiplatform.v1beta1.io_pb2.GcsDestination:
            """The Cloud Storage location of the directory where the output is
            to be written to. In the given directory a new directory is created.
            Its name is `prediction-<model-display-name>-<job-create-time>`,
            where timestamp is in YYYY-MM-DDThh:mm:ss.sssZ ISO-8601 format.
            Inside of it files `predictions_0001.<extension>`,
            `predictions_0002.<extension>`, ..., `predictions_N.<extension>`
            are created where `<extension>` depends on chosen
            [predictions_format][google.cloud.aiplatform.v1beta1.BatchPredictionJob.OutputConfig.predictions_format], and N may equal 0001 and depends on the total
            number of successfully predicted instances.
            If the Model has both [instance][google.cloud.aiplatform.v1beta1.PredictSchemata.instance_schema_uri]
            and [prediction][google.cloud.aiplatform.v1beta1.PredictSchemata.parameters_schema_uri] schemata
            defined then each such file contains predictions as per the
            [predictions_format][google.cloud.aiplatform.v1beta1.BatchPredictionJob.OutputConfig.predictions_format].
            If prediction for any instance failed (partially or completely), then
            an additional `errors_0001.<extension>`, `errors_0002.<extension>`,...,
            `errors_N.<extension>` files are created (N depends on total number
            of failed predictions). These files contain the failed instances,
            as per their schema, followed by an additional `error` field which as
            value has [google.rpc.Status][google.rpc.Status]
            containing only `code` and `message` fields.
            """
            pass
        @property
        def bigquery_destination(self) -> google.cloud.aiplatform.v1beta1.io_pb2.BigQueryDestination:
            """The BigQuery project or dataset location where the output is to be
            written to. If project is provided, a new dataset is created with name
            `prediction_<model-display-name>_<job-create-time>`
            where <model-display-name> is made
            BigQuery-dataset-name compatible (for example, most special characters
            become underscores), and timestamp is in
            YYYY_MM_DDThh_mm_ss_sssZ "based on ISO-8601" format. In the dataset
            two tables will be created, `predictions`, and `errors`.
            If the Model has both [instance][google.cloud.aiplatform.v1beta1.PredictSchemata.instance_schema_uri]
            and [prediction][google.cloud.aiplatform.v1beta1.PredictSchemata.parameters_schema_uri] schemata
            defined then the tables have columns as follows: The `predictions`
            table contains instances for which the prediction succeeded, it
            has columns as per a concatenation of the Model's instance and
            prediction schemata. The `errors` table contains rows for which the
            prediction has failed, it has instance columns, as per the
            instance schema, followed by a single "errors" column, which as values
            has [google.rpc.Status][google.rpc.Status]
            represented as a STRUCT, and containing only `code` and `message`.
            """
            pass
        predictions_format: typing.Text = ...
        """Required. The format in which Vertex AI gives the predictions, must be one of the
        [Model's][google.cloud.aiplatform.v1beta1.BatchPredictionJob.model]
        [supported_output_storage_formats][google.cloud.aiplatform.v1beta1.Model.supported_output_storage_formats].
        """

        def __init__(self,
            *,
            gcs_destination : typing.Optional[google.cloud.aiplatform.v1beta1.io_pb2.GcsDestination] = ...,
            bigquery_destination : typing.Optional[google.cloud.aiplatform.v1beta1.io_pb2.BigQueryDestination] = ...,
            predictions_format : typing.Text = ...,
            ) -> None: ...
        def HasField(self, field_name: typing_extensions.Literal["bigquery_destination",b"bigquery_destination","destination",b"destination","gcs_destination",b"gcs_destination"]) -> builtins.bool: ...
        def ClearField(self, field_name: typing_extensions.Literal["bigquery_destination",b"bigquery_destination","destination",b"destination","gcs_destination",b"gcs_destination","predictions_format",b"predictions_format"]) -> None: ...
        def WhichOneof(self, oneof_group: typing_extensions.Literal["destination",b"destination"]) -> typing.Optional[typing_extensions.Literal["gcs_destination","bigquery_destination"]]: ...

    class OutputInfo(google.protobuf.message.Message):
        """Further describes this job's output.
        Supplements [output_config][google.cloud.aiplatform.v1beta1.BatchPredictionJob.output_config].
        """
        DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
        GCS_OUTPUT_DIRECTORY_FIELD_NUMBER: builtins.int
        BIGQUERY_OUTPUT_DATASET_FIELD_NUMBER: builtins.int
        BIGQUERY_OUTPUT_TABLE_FIELD_NUMBER: builtins.int
        gcs_output_directory: typing.Text = ...
        """Output only. The full path of the Cloud Storage directory created, into which
        the prediction output is written.
        """

        bigquery_output_dataset: typing.Text = ...
        """Output only. The path of the BigQuery dataset created, in
        `bq://projectId.bqDatasetId`
        format, into which the prediction output is written.
        """

        bigquery_output_table: typing.Text = ...
        """Output only. The name of the BigQuery table created, in
        `predictions_<timestamp>`
        format, into which the prediction output is written.
        Can be used by UI to generate the BigQuery output path, for example.
        """

        def __init__(self,
            *,
            gcs_output_directory : typing.Text = ...,
            bigquery_output_dataset : typing.Text = ...,
            bigquery_output_table : typing.Text = ...,
            ) -> None: ...
        def HasField(self, field_name: typing_extensions.Literal["bigquery_output_dataset",b"bigquery_output_dataset","gcs_output_directory",b"gcs_output_directory","output_location",b"output_location"]) -> builtins.bool: ...
        def ClearField(self, field_name: typing_extensions.Literal["bigquery_output_dataset",b"bigquery_output_dataset","bigquery_output_table",b"bigquery_output_table","gcs_output_directory",b"gcs_output_directory","output_location",b"output_location"]) -> None: ...
        def WhichOneof(self, oneof_group: typing_extensions.Literal["output_location",b"output_location"]) -> typing.Optional[typing_extensions.Literal["gcs_output_directory","bigquery_output_dataset"]]: ...

    class LabelsEntry(google.protobuf.message.Message):
        DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
        KEY_FIELD_NUMBER: builtins.int
        VALUE_FIELD_NUMBER: builtins.int
        key: typing.Text = ...
        value: typing.Text = ...
        def __init__(self,
            *,
            key : typing.Text = ...,
            value : typing.Text = ...,
            ) -> None: ...
        def ClearField(self, field_name: typing_extensions.Literal["key",b"key","value",b"value"]) -> None: ...

    NAME_FIELD_NUMBER: builtins.int
    DISPLAY_NAME_FIELD_NUMBER: builtins.int
    MODEL_FIELD_NUMBER: builtins.int
    UNMANAGED_CONTAINER_MODEL_FIELD_NUMBER: builtins.int
    INPUT_CONFIG_FIELD_NUMBER: builtins.int
    MODEL_PARAMETERS_FIELD_NUMBER: builtins.int
    OUTPUT_CONFIG_FIELD_NUMBER: builtins.int
    DEDICATED_RESOURCES_FIELD_NUMBER: builtins.int
    MANUAL_BATCH_TUNING_PARAMETERS_FIELD_NUMBER: builtins.int
    GENERATE_EXPLANATION_FIELD_NUMBER: builtins.int
    EXPLANATION_SPEC_FIELD_NUMBER: builtins.int
    OUTPUT_INFO_FIELD_NUMBER: builtins.int
    STATE_FIELD_NUMBER: builtins.int
    ERROR_FIELD_NUMBER: builtins.int
    PARTIAL_FAILURES_FIELD_NUMBER: builtins.int
    RESOURCES_CONSUMED_FIELD_NUMBER: builtins.int
    COMPLETION_STATS_FIELD_NUMBER: builtins.int
    CREATE_TIME_FIELD_NUMBER: builtins.int
    START_TIME_FIELD_NUMBER: builtins.int
    END_TIME_FIELD_NUMBER: builtins.int
    UPDATE_TIME_FIELD_NUMBER: builtins.int
    LABELS_FIELD_NUMBER: builtins.int
    ENCRYPTION_SPEC_FIELD_NUMBER: builtins.int
    name: typing.Text = ...
    """Output only. Resource name of the BatchPredictionJob."""

    display_name: typing.Text = ...
    """Required. The user-defined name of this BatchPredictionJob."""

    model: typing.Text = ...
    """The name of the Model resoure that produces the predictions via this job,
    must share the same ancestor Location.
    Starting this job has no impact on any existing deployments of the Model
    and their resources.
    Exactly one of model and unmanaged_container_model must be set.
    """

    @property
    def unmanaged_container_model(self) -> google.cloud.aiplatform.v1beta1.unmanaged_container_model_pb2.UnmanagedContainerModel:
        """Contains model information necessary to perform batch prediction without
        requiring uploading to model registry.
        Exactly one of model and unmanaged_container_model must be set.
        """
        pass
    @property
    def input_config(self) -> global___BatchPredictionJob.InputConfig:
        """Required. Input configuration of the instances on which predictions are performed.
        The schema of any single instance may be specified via
        the [Model's][google.cloud.aiplatform.v1beta1.BatchPredictionJob.model]
        [PredictSchemata's][google.cloud.aiplatform.v1beta1.Model.predict_schemata]
        [instance_schema_uri][google.cloud.aiplatform.v1beta1.PredictSchemata.instance_schema_uri].
        """
        pass
    @property
    def model_parameters(self) -> google.protobuf.struct_pb2.Value:
        """The parameters that govern the predictions. The schema of the parameters
        may be specified via the [Model's][google.cloud.aiplatform.v1beta1.BatchPredictionJob.model]
        [PredictSchemata's][google.cloud.aiplatform.v1beta1.Model.predict_schemata]
        [parameters_schema_uri][google.cloud.aiplatform.v1beta1.PredictSchemata.parameters_schema_uri].
        """
        pass
    @property
    def output_config(self) -> global___BatchPredictionJob.OutputConfig:
        """Required. The Configuration specifying where output predictions should
        be written.
        The schema of any single prediction may be specified as a concatenation
        of [Model's][google.cloud.aiplatform.v1beta1.BatchPredictionJob.model]
        [PredictSchemata's][google.cloud.aiplatform.v1beta1.Model.predict_schemata]
        [instance_schema_uri][google.cloud.aiplatform.v1beta1.PredictSchemata.instance_schema_uri]
        and
        [prediction_schema_uri][google.cloud.aiplatform.v1beta1.PredictSchemata.prediction_schema_uri].
        """
        pass
    @property
    def dedicated_resources(self) -> google.cloud.aiplatform.v1beta1.machine_resources_pb2.BatchDedicatedResources:
        """The config of resources used by the Model during the batch prediction. If
        the Model [supports][google.cloud.aiplatform.v1beta1.Model.supported_deployment_resources_types]
        DEDICATED_RESOURCES this config may be provided (and the job will use these
        resources), if the Model doesn't support AUTOMATIC_RESOURCES, this config
        must be provided.
        """
        pass
    @property
    def manual_batch_tuning_parameters(self) -> google.cloud.aiplatform.v1beta1.manual_batch_tuning_parameters_pb2.ManualBatchTuningParameters:
        """Immutable. Parameters configuring the batch behavior. Currently only applicable when
        [dedicated_resources][google.cloud.aiplatform.v1beta1.BatchPredictionJob.dedicated_resources] are used (in other cases Vertex AI does
        the tuning itself).
        """
        pass
    generate_explanation: builtins.bool = ...
    """Generate explanation with the batch prediction results.

    When set to `true`, the batch prediction output changes based on the
    `predictions_format` field of the
    [BatchPredictionJob.output_config][google.cloud.aiplatform.v1beta1.BatchPredictionJob.output_config] object:

     * `bigquery`: output includes a column named `explanation`. The value
       is a struct that conforms to the [Explanation][google.cloud.aiplatform.v1beta1.Explanation] object.
     * `jsonl`: The JSON objects on each line include an additional entry
       keyed `explanation`. The value of the entry is a JSON object that
       conforms to the [Explanation][google.cloud.aiplatform.v1beta1.Explanation] object.
     * `csv`: Generating explanations for CSV format is not supported.

    If this field is set to true, either the [Model.explanation_spec][google.cloud.aiplatform.v1beta1.Model.explanation_spec] or
    [explanation_spec][google.cloud.aiplatform.v1beta1.BatchPredictionJob.explanation_spec] must be populated.
    """

    @property
    def explanation_spec(self) -> google.cloud.aiplatform.v1beta1.explanation_pb2.ExplanationSpec:
        """Explanation configuration for this BatchPredictionJob. Can be
        specified only if [generate_explanation][google.cloud.aiplatform.v1beta1.BatchPredictionJob.generate_explanation] is set to `true`.

        This value overrides the value of [Model.explanation_spec][google.cloud.aiplatform.v1beta1.Model.explanation_spec]. All fields of
        [explanation_spec][google.cloud.aiplatform.v1beta1.BatchPredictionJob.explanation_spec] are optional in the request. If a field of the
        [explanation_spec][google.cloud.aiplatform.v1beta1.BatchPredictionJob.explanation_spec] object is not populated, the corresponding field of
        the [Model.explanation_spec][google.cloud.aiplatform.v1beta1.Model.explanation_spec] object is inherited.
        """
        pass
    @property
    def output_info(self) -> global___BatchPredictionJob.OutputInfo:
        """Output only. Information further describing the output of this job."""
        pass
    state: google.cloud.aiplatform.v1beta1.job_state_pb2.JobState.ValueType = ...
    """Output only. The detailed state of the job."""

    @property
    def error(self) -> google.rpc.status_pb2.Status:
        """Output only. Only populated when the job's state is JOB_STATE_FAILED or
        JOB_STATE_CANCELLED.
        """
        pass
    @property
    def partial_failures(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[google.rpc.status_pb2.Status]:
        """Output only. Partial failures encountered.
        For example, single files that can't be read.
        This field never exceeds 20 entries.
        Status details fields contain standard GCP error details.
        """
        pass
    @property
    def resources_consumed(self) -> google.cloud.aiplatform.v1beta1.machine_resources_pb2.ResourcesConsumed:
        """Output only. Information about resources that had been consumed by this job.
        Provided in real time at best effort basis, as well as a final value
        once the job completes.

        Note: This field currently may be not populated for batch predictions that
        use AutoML Models.
        """
        pass
    @property
    def completion_stats(self) -> google.cloud.aiplatform.v1beta1.completion_stats_pb2.CompletionStats:
        """Output only. Statistics on completed and failed prediction instances."""
        pass
    @property
    def create_time(self) -> google.protobuf.timestamp_pb2.Timestamp:
        """Output only. Time when the BatchPredictionJob was created."""
        pass
    @property
    def start_time(self) -> google.protobuf.timestamp_pb2.Timestamp:
        """Output only. Time when the BatchPredictionJob for the first time entered the
        `JOB_STATE_RUNNING` state.
        """
        pass
    @property
    def end_time(self) -> google.protobuf.timestamp_pb2.Timestamp:
        """Output only. Time when the BatchPredictionJob entered any of the following states:
        `JOB_STATE_SUCCEEDED`, `JOB_STATE_FAILED`, `JOB_STATE_CANCELLED`.
        """
        pass
    @property
    def update_time(self) -> google.protobuf.timestamp_pb2.Timestamp:
        """Output only. Time when the BatchPredictionJob was most recently updated."""
        pass
    @property
    def labels(self) -> google.protobuf.internal.containers.ScalarMap[typing.Text, typing.Text]:
        """The labels with user-defined metadata to organize BatchPredictionJobs.

        Label keys and values can be no longer than 64 characters
        (Unicode codepoints), can only contain lowercase letters, numeric
        characters, underscores and dashes. International characters are allowed.

        See https://goo.gl/xmQnxf for more information and examples of labels.
        """
        pass
    @property
    def encryption_spec(self) -> google.cloud.aiplatform.v1beta1.encryption_spec_pb2.EncryptionSpec:
        """Customer-managed encryption key options for a BatchPredictionJob. If this
        is set, then all resources created by the BatchPredictionJob will be
        encrypted with the provided encryption key.
        """
        pass
    def __init__(self,
        *,
        name : typing.Text = ...,
        display_name : typing.Text = ...,
        model : typing.Text = ...,
        unmanaged_container_model : typing.Optional[google.cloud.aiplatform.v1beta1.unmanaged_container_model_pb2.UnmanagedContainerModel] = ...,
        input_config : typing.Optional[global___BatchPredictionJob.InputConfig] = ...,
        model_parameters : typing.Optional[google.protobuf.struct_pb2.Value] = ...,
        output_config : typing.Optional[global___BatchPredictionJob.OutputConfig] = ...,
        dedicated_resources : typing.Optional[google.cloud.aiplatform.v1beta1.machine_resources_pb2.BatchDedicatedResources] = ...,
        manual_batch_tuning_parameters : typing.Optional[google.cloud.aiplatform.v1beta1.manual_batch_tuning_parameters_pb2.ManualBatchTuningParameters] = ...,
        generate_explanation : builtins.bool = ...,
        explanation_spec : typing.Optional[google.cloud.aiplatform.v1beta1.explanation_pb2.ExplanationSpec] = ...,
        output_info : typing.Optional[global___BatchPredictionJob.OutputInfo] = ...,
        state : google.cloud.aiplatform.v1beta1.job_state_pb2.JobState.ValueType = ...,
        error : typing.Optional[google.rpc.status_pb2.Status] = ...,
        partial_failures : typing.Optional[typing.Iterable[google.rpc.status_pb2.Status]] = ...,
        resources_consumed : typing.Optional[google.cloud.aiplatform.v1beta1.machine_resources_pb2.ResourcesConsumed] = ...,
        completion_stats : typing.Optional[google.cloud.aiplatform.v1beta1.completion_stats_pb2.CompletionStats] = ...,
        create_time : typing.Optional[google.protobuf.timestamp_pb2.Timestamp] = ...,
        start_time : typing.Optional[google.protobuf.timestamp_pb2.Timestamp] = ...,
        end_time : typing.Optional[google.protobuf.timestamp_pb2.Timestamp] = ...,
        update_time : typing.Optional[google.protobuf.timestamp_pb2.Timestamp] = ...,
        labels : typing.Optional[typing.Mapping[typing.Text, typing.Text]] = ...,
        encryption_spec : typing.Optional[google.cloud.aiplatform.v1beta1.encryption_spec_pb2.EncryptionSpec] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["completion_stats",b"completion_stats","create_time",b"create_time","dedicated_resources",b"dedicated_resources","encryption_spec",b"encryption_spec","end_time",b"end_time","error",b"error","explanation_spec",b"explanation_spec","input_config",b"input_config","manual_batch_tuning_parameters",b"manual_batch_tuning_parameters","model_parameters",b"model_parameters","output_config",b"output_config","output_info",b"output_info","resources_consumed",b"resources_consumed","start_time",b"start_time","unmanaged_container_model",b"unmanaged_container_model","update_time",b"update_time"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["completion_stats",b"completion_stats","create_time",b"create_time","dedicated_resources",b"dedicated_resources","display_name",b"display_name","encryption_spec",b"encryption_spec","end_time",b"end_time","error",b"error","explanation_spec",b"explanation_spec","generate_explanation",b"generate_explanation","input_config",b"input_config","labels",b"labels","manual_batch_tuning_parameters",b"manual_batch_tuning_parameters","model",b"model","model_parameters",b"model_parameters","name",b"name","output_config",b"output_config","output_info",b"output_info","partial_failures",b"partial_failures","resources_consumed",b"resources_consumed","start_time",b"start_time","state",b"state","unmanaged_container_model",b"unmanaged_container_model","update_time",b"update_time"]) -> None: ...
global___BatchPredictionJob = BatchPredictionJob
