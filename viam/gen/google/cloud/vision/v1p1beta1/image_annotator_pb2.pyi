"""
@generated by mypy-protobuf.  Do not edit manually!
isort:skip_file
"""
import builtins
import google.cloud.vision.v1p1beta1.geometry_pb2
import google.cloud.vision.v1p1beta1.text_annotation_pb2
import google.cloud.vision.v1p1beta1.web_detection_pb2
import google.protobuf.descriptor
import google.protobuf.internal.containers
import google.protobuf.internal.enum_type_wrapper
import google.protobuf.message
import google.rpc.status_pb2
import google.type.color_pb2
import google.type.latlng_pb2
import typing
import typing_extensions

DESCRIPTOR: google.protobuf.descriptor.FileDescriptor = ...

class _Likelihood:
    ValueType = typing.NewType('ValueType', builtins.int)
    V: typing_extensions.TypeAlias = ValueType
class _LikelihoodEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_Likelihood.ValueType], builtins.type):
    DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor = ...
    UNKNOWN: Likelihood.ValueType = ...  # 0
    """Unknown likelihood."""

    VERY_UNLIKELY: Likelihood.ValueType = ...  # 1
    """It is very unlikely that the image belongs to the specified vertical."""

    UNLIKELY: Likelihood.ValueType = ...  # 2
    """It is unlikely that the image belongs to the specified vertical."""

    POSSIBLE: Likelihood.ValueType = ...  # 3
    """It is possible that the image belongs to the specified vertical."""

    LIKELY: Likelihood.ValueType = ...  # 4
    """It is likely that the image belongs to the specified vertical."""

    VERY_LIKELY: Likelihood.ValueType = ...  # 5
    """It is very likely that the image belongs to the specified vertical."""

class Likelihood(_Likelihood, metaclass=_LikelihoodEnumTypeWrapper):
    """A bucketized representation of likelihood, which is intended to give clients
    highly stable results across model upgrades.
    """
    pass

UNKNOWN: Likelihood.ValueType = ...  # 0
"""Unknown likelihood."""

VERY_UNLIKELY: Likelihood.ValueType = ...  # 1
"""It is very unlikely that the image belongs to the specified vertical."""

UNLIKELY: Likelihood.ValueType = ...  # 2
"""It is unlikely that the image belongs to the specified vertical."""

POSSIBLE: Likelihood.ValueType = ...  # 3
"""It is possible that the image belongs to the specified vertical."""

LIKELY: Likelihood.ValueType = ...  # 4
"""It is likely that the image belongs to the specified vertical."""

VERY_LIKELY: Likelihood.ValueType = ...  # 5
"""It is very likely that the image belongs to the specified vertical."""

global___Likelihood = Likelihood


class Feature(google.protobuf.message.Message):
    """Users describe the type of Google Cloud Vision API tasks to perform over
    images by using *Feature*s. Each Feature indicates a type of image
    detection task to perform. Features encode the Cloud Vision API
    vertical to operate on and the number of top-scoring results to return.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class _Type:
        ValueType = typing.NewType('ValueType', builtins.int)
        V: typing_extensions.TypeAlias = ValueType
    class _TypeEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_Type.ValueType], builtins.type):
        DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor = ...
        TYPE_UNSPECIFIED: Feature.Type.ValueType = ...  # 0
        """Unspecified feature type."""

        FACE_DETECTION: Feature.Type.ValueType = ...  # 1
        """Run face detection."""

        LANDMARK_DETECTION: Feature.Type.ValueType = ...  # 2
        """Run landmark detection."""

        LOGO_DETECTION: Feature.Type.ValueType = ...  # 3
        """Run logo detection."""

        LABEL_DETECTION: Feature.Type.ValueType = ...  # 4
        """Run label detection."""

        TEXT_DETECTION: Feature.Type.ValueType = ...  # 5
        """Run OCR."""

        DOCUMENT_TEXT_DETECTION: Feature.Type.ValueType = ...  # 11
        """Run dense text document OCR. Takes precedence when both
        DOCUMENT_TEXT_DETECTION and TEXT_DETECTION are present.
        """

        SAFE_SEARCH_DETECTION: Feature.Type.ValueType = ...  # 6
        """Run computer vision models to compute image safe-search properties."""

        IMAGE_PROPERTIES: Feature.Type.ValueType = ...  # 7
        """Compute a set of image properties, such as the image's dominant colors."""

        CROP_HINTS: Feature.Type.ValueType = ...  # 9
        """Run crop hints."""

        WEB_DETECTION: Feature.Type.ValueType = ...  # 10
        """Run web detection."""

    class Type(_Type, metaclass=_TypeEnumTypeWrapper):
        """Type of image feature."""
        pass

    TYPE_UNSPECIFIED: Feature.Type.ValueType = ...  # 0
    """Unspecified feature type."""

    FACE_DETECTION: Feature.Type.ValueType = ...  # 1
    """Run face detection."""

    LANDMARK_DETECTION: Feature.Type.ValueType = ...  # 2
    """Run landmark detection."""

    LOGO_DETECTION: Feature.Type.ValueType = ...  # 3
    """Run logo detection."""

    LABEL_DETECTION: Feature.Type.ValueType = ...  # 4
    """Run label detection."""

    TEXT_DETECTION: Feature.Type.ValueType = ...  # 5
    """Run OCR."""

    DOCUMENT_TEXT_DETECTION: Feature.Type.ValueType = ...  # 11
    """Run dense text document OCR. Takes precedence when both
    DOCUMENT_TEXT_DETECTION and TEXT_DETECTION are present.
    """

    SAFE_SEARCH_DETECTION: Feature.Type.ValueType = ...  # 6
    """Run computer vision models to compute image safe-search properties."""

    IMAGE_PROPERTIES: Feature.Type.ValueType = ...  # 7
    """Compute a set of image properties, such as the image's dominant colors."""

    CROP_HINTS: Feature.Type.ValueType = ...  # 9
    """Run crop hints."""

    WEB_DETECTION: Feature.Type.ValueType = ...  # 10
    """Run web detection."""


    TYPE_FIELD_NUMBER: builtins.int
    MAX_RESULTS_FIELD_NUMBER: builtins.int
    MODEL_FIELD_NUMBER: builtins.int
    type: global___Feature.Type.ValueType = ...
    """The feature type."""

    max_results: builtins.int = ...
    """Maximum number of results of this type."""

    model: typing.Text = ...
    """Model to use for the feature.
    Supported values: "builtin/stable" (the default if unset) and
    "builtin/latest".
    """

    def __init__(self,
        *,
        type : global___Feature.Type.ValueType = ...,
        max_results : builtins.int = ...,
        model : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["max_results",b"max_results","model",b"model","type",b"type"]) -> None: ...
global___Feature = Feature

class ImageSource(google.protobuf.message.Message):
    """External image source (Google Cloud Storage image location)."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    GCS_IMAGE_URI_FIELD_NUMBER: builtins.int
    IMAGE_URI_FIELD_NUMBER: builtins.int
    gcs_image_uri: typing.Text = ...
    """NOTE: For new code `image_uri` below is preferred.
    Google Cloud Storage image URI, which must be in the following form:
    `gs://bucket_name/object_name` (for details, see
    [Google Cloud Storage Request
    URIs](https://cloud.google.com/storage/docs/reference-uris)).
    NOTE: Cloud Storage object versioning is not supported.
    """

    image_uri: typing.Text = ...
    """Image URI which supports:
    1) Google Cloud Storage image URI, which must be in the following form:
    `gs://bucket_name/object_name` (for details, see
    [Google Cloud Storage Request
    URIs](https://cloud.google.com/storage/docs/reference-uris)).
    NOTE: Cloud Storage object versioning is not supported.
    2) Publicly accessible image HTTP/HTTPS URL.
    This is preferred over the legacy `gcs_image_uri` above. When both
    `gcs_image_uri` and `image_uri` are specified, `image_uri` takes
    precedence.
    """

    def __init__(self,
        *,
        gcs_image_uri : typing.Text = ...,
        image_uri : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["gcs_image_uri",b"gcs_image_uri","image_uri",b"image_uri"]) -> None: ...
global___ImageSource = ImageSource

class Image(google.protobuf.message.Message):
    """Client image to perform Google Cloud Vision API tasks over."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    CONTENT_FIELD_NUMBER: builtins.int
    SOURCE_FIELD_NUMBER: builtins.int
    content: builtins.bytes = ...
    """Image content, represented as a stream of bytes.
    Note: as with all `bytes` fields, protobuffers use a pure binary
    representation, whereas JSON representations use base64.
    """

    @property
    def source(self) -> global___ImageSource:
        """Google Cloud Storage image location. If both `content` and `source`
        are provided for an image, `content` takes precedence and is
        used to perform the image annotation request.
        """
        pass
    def __init__(self,
        *,
        content : builtins.bytes = ...,
        source : typing.Optional[global___ImageSource] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["source",b"source"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["content",b"content","source",b"source"]) -> None: ...
global___Image = Image

class FaceAnnotation(google.protobuf.message.Message):
    """A face annotation object contains the results of face detection."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class Landmark(google.protobuf.message.Message):
        """A face-specific landmark (for example, a face feature)."""
        DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
        class _Type:
            ValueType = typing.NewType('ValueType', builtins.int)
            V: typing_extensions.TypeAlias = ValueType
        class _TypeEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_Type.ValueType], builtins.type):
            DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor = ...
            UNKNOWN_LANDMARK: FaceAnnotation.Landmark.Type.ValueType = ...  # 0
            """Unknown face landmark detected. Should not be filled."""

            LEFT_EYE: FaceAnnotation.Landmark.Type.ValueType = ...  # 1
            """Left eye."""

            RIGHT_EYE: FaceAnnotation.Landmark.Type.ValueType = ...  # 2
            """Right eye."""

            LEFT_OF_LEFT_EYEBROW: FaceAnnotation.Landmark.Type.ValueType = ...  # 3
            """Left of left eyebrow."""

            RIGHT_OF_LEFT_EYEBROW: FaceAnnotation.Landmark.Type.ValueType = ...  # 4
            """Right of left eyebrow."""

            LEFT_OF_RIGHT_EYEBROW: FaceAnnotation.Landmark.Type.ValueType = ...  # 5
            """Left of right eyebrow."""

            RIGHT_OF_RIGHT_EYEBROW: FaceAnnotation.Landmark.Type.ValueType = ...  # 6
            """Right of right eyebrow."""

            MIDPOINT_BETWEEN_EYES: FaceAnnotation.Landmark.Type.ValueType = ...  # 7
            """Midpoint between eyes."""

            NOSE_TIP: FaceAnnotation.Landmark.Type.ValueType = ...  # 8
            """Nose tip."""

            UPPER_LIP: FaceAnnotation.Landmark.Type.ValueType = ...  # 9
            """Upper lip."""

            LOWER_LIP: FaceAnnotation.Landmark.Type.ValueType = ...  # 10
            """Lower lip."""

            MOUTH_LEFT: FaceAnnotation.Landmark.Type.ValueType = ...  # 11
            """Mouth left."""

            MOUTH_RIGHT: FaceAnnotation.Landmark.Type.ValueType = ...  # 12
            """Mouth right."""

            MOUTH_CENTER: FaceAnnotation.Landmark.Type.ValueType = ...  # 13
            """Mouth center."""

            NOSE_BOTTOM_RIGHT: FaceAnnotation.Landmark.Type.ValueType = ...  # 14
            """Nose, bottom right."""

            NOSE_BOTTOM_LEFT: FaceAnnotation.Landmark.Type.ValueType = ...  # 15
            """Nose, bottom left."""

            NOSE_BOTTOM_CENTER: FaceAnnotation.Landmark.Type.ValueType = ...  # 16
            """Nose, bottom center."""

            LEFT_EYE_TOP_BOUNDARY: FaceAnnotation.Landmark.Type.ValueType = ...  # 17
            """Left eye, top boundary."""

            LEFT_EYE_RIGHT_CORNER: FaceAnnotation.Landmark.Type.ValueType = ...  # 18
            """Left eye, right corner."""

            LEFT_EYE_BOTTOM_BOUNDARY: FaceAnnotation.Landmark.Type.ValueType = ...  # 19
            """Left eye, bottom boundary."""

            LEFT_EYE_LEFT_CORNER: FaceAnnotation.Landmark.Type.ValueType = ...  # 20
            """Left eye, left corner."""

            RIGHT_EYE_TOP_BOUNDARY: FaceAnnotation.Landmark.Type.ValueType = ...  # 21
            """Right eye, top boundary."""

            RIGHT_EYE_RIGHT_CORNER: FaceAnnotation.Landmark.Type.ValueType = ...  # 22
            """Right eye, right corner."""

            RIGHT_EYE_BOTTOM_BOUNDARY: FaceAnnotation.Landmark.Type.ValueType = ...  # 23
            """Right eye, bottom boundary."""

            RIGHT_EYE_LEFT_CORNER: FaceAnnotation.Landmark.Type.ValueType = ...  # 24
            """Right eye, left corner."""

            LEFT_EYEBROW_UPPER_MIDPOINT: FaceAnnotation.Landmark.Type.ValueType = ...  # 25
            """Left eyebrow, upper midpoint."""

            RIGHT_EYEBROW_UPPER_MIDPOINT: FaceAnnotation.Landmark.Type.ValueType = ...  # 26
            """Right eyebrow, upper midpoint."""

            LEFT_EAR_TRAGION: FaceAnnotation.Landmark.Type.ValueType = ...  # 27
            """Left ear tragion."""

            RIGHT_EAR_TRAGION: FaceAnnotation.Landmark.Type.ValueType = ...  # 28
            """Right ear tragion."""

            LEFT_EYE_PUPIL: FaceAnnotation.Landmark.Type.ValueType = ...  # 29
            """Left eye pupil."""

            RIGHT_EYE_PUPIL: FaceAnnotation.Landmark.Type.ValueType = ...  # 30
            """Right eye pupil."""

            FOREHEAD_GLABELLA: FaceAnnotation.Landmark.Type.ValueType = ...  # 31
            """Forehead glabella."""

            CHIN_GNATHION: FaceAnnotation.Landmark.Type.ValueType = ...  # 32
            """Chin gnathion."""

            CHIN_LEFT_GONION: FaceAnnotation.Landmark.Type.ValueType = ...  # 33
            """Chin left gonion."""

            CHIN_RIGHT_GONION: FaceAnnotation.Landmark.Type.ValueType = ...  # 34
            """Chin right gonion."""

        class Type(_Type, metaclass=_TypeEnumTypeWrapper):
            """Face landmark (feature) type.
            Left and right are defined from the vantage of the viewer of the image
            without considering mirror projections typical of photos. So, `LEFT_EYE`,
            typically, is the person's right eye.
            """
            pass

        UNKNOWN_LANDMARK: FaceAnnotation.Landmark.Type.ValueType = ...  # 0
        """Unknown face landmark detected. Should not be filled."""

        LEFT_EYE: FaceAnnotation.Landmark.Type.ValueType = ...  # 1
        """Left eye."""

        RIGHT_EYE: FaceAnnotation.Landmark.Type.ValueType = ...  # 2
        """Right eye."""

        LEFT_OF_LEFT_EYEBROW: FaceAnnotation.Landmark.Type.ValueType = ...  # 3
        """Left of left eyebrow."""

        RIGHT_OF_LEFT_EYEBROW: FaceAnnotation.Landmark.Type.ValueType = ...  # 4
        """Right of left eyebrow."""

        LEFT_OF_RIGHT_EYEBROW: FaceAnnotation.Landmark.Type.ValueType = ...  # 5
        """Left of right eyebrow."""

        RIGHT_OF_RIGHT_EYEBROW: FaceAnnotation.Landmark.Type.ValueType = ...  # 6
        """Right of right eyebrow."""

        MIDPOINT_BETWEEN_EYES: FaceAnnotation.Landmark.Type.ValueType = ...  # 7
        """Midpoint between eyes."""

        NOSE_TIP: FaceAnnotation.Landmark.Type.ValueType = ...  # 8
        """Nose tip."""

        UPPER_LIP: FaceAnnotation.Landmark.Type.ValueType = ...  # 9
        """Upper lip."""

        LOWER_LIP: FaceAnnotation.Landmark.Type.ValueType = ...  # 10
        """Lower lip."""

        MOUTH_LEFT: FaceAnnotation.Landmark.Type.ValueType = ...  # 11
        """Mouth left."""

        MOUTH_RIGHT: FaceAnnotation.Landmark.Type.ValueType = ...  # 12
        """Mouth right."""

        MOUTH_CENTER: FaceAnnotation.Landmark.Type.ValueType = ...  # 13
        """Mouth center."""

        NOSE_BOTTOM_RIGHT: FaceAnnotation.Landmark.Type.ValueType = ...  # 14
        """Nose, bottom right."""

        NOSE_BOTTOM_LEFT: FaceAnnotation.Landmark.Type.ValueType = ...  # 15
        """Nose, bottom left."""

        NOSE_BOTTOM_CENTER: FaceAnnotation.Landmark.Type.ValueType = ...  # 16
        """Nose, bottom center."""

        LEFT_EYE_TOP_BOUNDARY: FaceAnnotation.Landmark.Type.ValueType = ...  # 17
        """Left eye, top boundary."""

        LEFT_EYE_RIGHT_CORNER: FaceAnnotation.Landmark.Type.ValueType = ...  # 18
        """Left eye, right corner."""

        LEFT_EYE_BOTTOM_BOUNDARY: FaceAnnotation.Landmark.Type.ValueType = ...  # 19
        """Left eye, bottom boundary."""

        LEFT_EYE_LEFT_CORNER: FaceAnnotation.Landmark.Type.ValueType = ...  # 20
        """Left eye, left corner."""

        RIGHT_EYE_TOP_BOUNDARY: FaceAnnotation.Landmark.Type.ValueType = ...  # 21
        """Right eye, top boundary."""

        RIGHT_EYE_RIGHT_CORNER: FaceAnnotation.Landmark.Type.ValueType = ...  # 22
        """Right eye, right corner."""

        RIGHT_EYE_BOTTOM_BOUNDARY: FaceAnnotation.Landmark.Type.ValueType = ...  # 23
        """Right eye, bottom boundary."""

        RIGHT_EYE_LEFT_CORNER: FaceAnnotation.Landmark.Type.ValueType = ...  # 24
        """Right eye, left corner."""

        LEFT_EYEBROW_UPPER_MIDPOINT: FaceAnnotation.Landmark.Type.ValueType = ...  # 25
        """Left eyebrow, upper midpoint."""

        RIGHT_EYEBROW_UPPER_MIDPOINT: FaceAnnotation.Landmark.Type.ValueType = ...  # 26
        """Right eyebrow, upper midpoint."""

        LEFT_EAR_TRAGION: FaceAnnotation.Landmark.Type.ValueType = ...  # 27
        """Left ear tragion."""

        RIGHT_EAR_TRAGION: FaceAnnotation.Landmark.Type.ValueType = ...  # 28
        """Right ear tragion."""

        LEFT_EYE_PUPIL: FaceAnnotation.Landmark.Type.ValueType = ...  # 29
        """Left eye pupil."""

        RIGHT_EYE_PUPIL: FaceAnnotation.Landmark.Type.ValueType = ...  # 30
        """Right eye pupil."""

        FOREHEAD_GLABELLA: FaceAnnotation.Landmark.Type.ValueType = ...  # 31
        """Forehead glabella."""

        CHIN_GNATHION: FaceAnnotation.Landmark.Type.ValueType = ...  # 32
        """Chin gnathion."""

        CHIN_LEFT_GONION: FaceAnnotation.Landmark.Type.ValueType = ...  # 33
        """Chin left gonion."""

        CHIN_RIGHT_GONION: FaceAnnotation.Landmark.Type.ValueType = ...  # 34
        """Chin right gonion."""


        TYPE_FIELD_NUMBER: builtins.int
        POSITION_FIELD_NUMBER: builtins.int
        type: global___FaceAnnotation.Landmark.Type.ValueType = ...
        """Face landmark type."""

        @property
        def position(self) -> google.cloud.vision.v1p1beta1.geometry_pb2.Position:
            """Face landmark position."""
            pass
        def __init__(self,
            *,
            type : global___FaceAnnotation.Landmark.Type.ValueType = ...,
            position : typing.Optional[google.cloud.vision.v1p1beta1.geometry_pb2.Position] = ...,
            ) -> None: ...
        def HasField(self, field_name: typing_extensions.Literal["position",b"position"]) -> builtins.bool: ...
        def ClearField(self, field_name: typing_extensions.Literal["position",b"position","type",b"type"]) -> None: ...

    BOUNDING_POLY_FIELD_NUMBER: builtins.int
    FD_BOUNDING_POLY_FIELD_NUMBER: builtins.int
    LANDMARKS_FIELD_NUMBER: builtins.int
    ROLL_ANGLE_FIELD_NUMBER: builtins.int
    PAN_ANGLE_FIELD_NUMBER: builtins.int
    TILT_ANGLE_FIELD_NUMBER: builtins.int
    DETECTION_CONFIDENCE_FIELD_NUMBER: builtins.int
    LANDMARKING_CONFIDENCE_FIELD_NUMBER: builtins.int
    JOY_LIKELIHOOD_FIELD_NUMBER: builtins.int
    SORROW_LIKELIHOOD_FIELD_NUMBER: builtins.int
    ANGER_LIKELIHOOD_FIELD_NUMBER: builtins.int
    SURPRISE_LIKELIHOOD_FIELD_NUMBER: builtins.int
    UNDER_EXPOSED_LIKELIHOOD_FIELD_NUMBER: builtins.int
    BLURRED_LIKELIHOOD_FIELD_NUMBER: builtins.int
    HEADWEAR_LIKELIHOOD_FIELD_NUMBER: builtins.int
    @property
    def bounding_poly(self) -> google.cloud.vision.v1p1beta1.geometry_pb2.BoundingPoly:
        """The bounding polygon around the face. The coordinates of the bounding box
        are in the original image's scale, as returned in `ImageParams`.
        The bounding box is computed to "frame" the face in accordance with human
        expectations. It is based on the landmarker results.
        Note that one or more x and/or y coordinates may not be generated in the
        `BoundingPoly` (the polygon will be unbounded) if only a partial face
        appears in the image to be annotated.
        """
        pass
    @property
    def fd_bounding_poly(self) -> google.cloud.vision.v1p1beta1.geometry_pb2.BoundingPoly:
        """The `fd_bounding_poly` bounding polygon is tighter than the
        `boundingPoly`, and encloses only the skin part of the face. Typically, it
        is used to eliminate the face from any image analysis that detects the
        "amount of skin" visible in an image. It is not based on the
        landmarker results, only on the initial face detection, hence
        the <code>fd</code> (face detection) prefix.
        """
        pass
    @property
    def landmarks(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___FaceAnnotation.Landmark]:
        """Detected face landmarks."""
        pass
    roll_angle: builtins.float = ...
    """Roll angle, which indicates the amount of clockwise/anti-clockwise rotation
    of the face relative to the image vertical about the axis perpendicular to
    the face. Range [-180,180].
    """

    pan_angle: builtins.float = ...
    """Yaw angle, which indicates the leftward/rightward angle that the face is
    pointing relative to the vertical plane perpendicular to the image. Range
    [-180,180].
    """

    tilt_angle: builtins.float = ...
    """Pitch angle, which indicates the upwards/downwards angle that the face is
    pointing relative to the image's horizontal plane. Range [-180,180].
    """

    detection_confidence: builtins.float = ...
    """Detection confidence. Range [0, 1]."""

    landmarking_confidence: builtins.float = ...
    """Face landmarking confidence. Range [0, 1]."""

    joy_likelihood: global___Likelihood.ValueType = ...
    """Joy likelihood."""

    sorrow_likelihood: global___Likelihood.ValueType = ...
    """Sorrow likelihood."""

    anger_likelihood: global___Likelihood.ValueType = ...
    """Anger likelihood."""

    surprise_likelihood: global___Likelihood.ValueType = ...
    """Surprise likelihood."""

    under_exposed_likelihood: global___Likelihood.ValueType = ...
    """Under-exposed likelihood."""

    blurred_likelihood: global___Likelihood.ValueType = ...
    """Blurred likelihood."""

    headwear_likelihood: global___Likelihood.ValueType = ...
    """Headwear likelihood."""

    def __init__(self,
        *,
        bounding_poly : typing.Optional[google.cloud.vision.v1p1beta1.geometry_pb2.BoundingPoly] = ...,
        fd_bounding_poly : typing.Optional[google.cloud.vision.v1p1beta1.geometry_pb2.BoundingPoly] = ...,
        landmarks : typing.Optional[typing.Iterable[global___FaceAnnotation.Landmark]] = ...,
        roll_angle : builtins.float = ...,
        pan_angle : builtins.float = ...,
        tilt_angle : builtins.float = ...,
        detection_confidence : builtins.float = ...,
        landmarking_confidence : builtins.float = ...,
        joy_likelihood : global___Likelihood.ValueType = ...,
        sorrow_likelihood : global___Likelihood.ValueType = ...,
        anger_likelihood : global___Likelihood.ValueType = ...,
        surprise_likelihood : global___Likelihood.ValueType = ...,
        under_exposed_likelihood : global___Likelihood.ValueType = ...,
        blurred_likelihood : global___Likelihood.ValueType = ...,
        headwear_likelihood : global___Likelihood.ValueType = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["bounding_poly",b"bounding_poly","fd_bounding_poly",b"fd_bounding_poly"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["anger_likelihood",b"anger_likelihood","blurred_likelihood",b"blurred_likelihood","bounding_poly",b"bounding_poly","detection_confidence",b"detection_confidence","fd_bounding_poly",b"fd_bounding_poly","headwear_likelihood",b"headwear_likelihood","joy_likelihood",b"joy_likelihood","landmarking_confidence",b"landmarking_confidence","landmarks",b"landmarks","pan_angle",b"pan_angle","roll_angle",b"roll_angle","sorrow_likelihood",b"sorrow_likelihood","surprise_likelihood",b"surprise_likelihood","tilt_angle",b"tilt_angle","under_exposed_likelihood",b"under_exposed_likelihood"]) -> None: ...
global___FaceAnnotation = FaceAnnotation

class LocationInfo(google.protobuf.message.Message):
    """Detected entity location information."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    LAT_LNG_FIELD_NUMBER: builtins.int
    @property
    def lat_lng(self) -> google.type.latlng_pb2.LatLng:
        """lat/long location coordinates."""
        pass
    def __init__(self,
        *,
        lat_lng : typing.Optional[google.type.latlng_pb2.LatLng] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["lat_lng",b"lat_lng"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["lat_lng",b"lat_lng"]) -> None: ...
global___LocationInfo = LocationInfo

class Property(google.protobuf.message.Message):
    """A `Property` consists of a user-supplied name/value pair."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    NAME_FIELD_NUMBER: builtins.int
    VALUE_FIELD_NUMBER: builtins.int
    UINT64_VALUE_FIELD_NUMBER: builtins.int
    name: typing.Text = ...
    """Name of the property."""

    value: typing.Text = ...
    """Value of the property."""

    uint64_value: builtins.int = ...
    """Value of numeric properties."""

    def __init__(self,
        *,
        name : typing.Text = ...,
        value : typing.Text = ...,
        uint64_value : builtins.int = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["name",b"name","uint64_value",b"uint64_value","value",b"value"]) -> None: ...
global___Property = Property

class EntityAnnotation(google.protobuf.message.Message):
    """Set of detected entity features."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    MID_FIELD_NUMBER: builtins.int
    LOCALE_FIELD_NUMBER: builtins.int
    DESCRIPTION_FIELD_NUMBER: builtins.int
    SCORE_FIELD_NUMBER: builtins.int
    CONFIDENCE_FIELD_NUMBER: builtins.int
    TOPICALITY_FIELD_NUMBER: builtins.int
    BOUNDING_POLY_FIELD_NUMBER: builtins.int
    LOCATIONS_FIELD_NUMBER: builtins.int
    PROPERTIES_FIELD_NUMBER: builtins.int
    mid: typing.Text = ...
    """Opaque entity ID. Some IDs may be available in
    [Google Knowledge Graph Search
    API](https://developers.google.com/knowledge-graph/).
    """

    locale: typing.Text = ...
    """The language code for the locale in which the entity textual
    `description` is expressed.
    """

    description: typing.Text = ...
    """Entity textual description, expressed in its `locale` language."""

    score: builtins.float = ...
    """Overall score of the result. Range [0, 1]."""

    confidence: builtins.float = ...
    """The accuracy of the entity detection in an image.
    For example, for an image in which the "Eiffel Tower" entity is detected,
    this field represents the confidence that there is a tower in the query
    image. Range [0, 1].
    """

    topicality: builtins.float = ...
    """The relevancy of the ICA (Image Content Annotation) label to the
    image. For example, the relevancy of "tower" is likely higher to an image
    containing the detected "Eiffel Tower" than to an image containing a
    detected distant towering building, even though the confidence that
    there is a tower in each image may be the same. Range [0, 1].
    """

    @property
    def bounding_poly(self) -> google.cloud.vision.v1p1beta1.geometry_pb2.BoundingPoly:
        """Image region to which this entity belongs. Not produced
        for `LABEL_DETECTION` features.
        """
        pass
    @property
    def locations(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___LocationInfo]:
        """The location information for the detected entity. Multiple
        `LocationInfo` elements can be present because one location may
        indicate the location of the scene in the image, and another location
        may indicate the location of the place where the image was taken.
        Location information is usually present for landmarks.
        """
        pass
    @property
    def properties(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___Property]:
        """Some entities may have optional user-supplied `Property` (name/value)
        fields, such a score or string that qualifies the entity.
        """
        pass
    def __init__(self,
        *,
        mid : typing.Text = ...,
        locale : typing.Text = ...,
        description : typing.Text = ...,
        score : builtins.float = ...,
        confidence : builtins.float = ...,
        topicality : builtins.float = ...,
        bounding_poly : typing.Optional[google.cloud.vision.v1p1beta1.geometry_pb2.BoundingPoly] = ...,
        locations : typing.Optional[typing.Iterable[global___LocationInfo]] = ...,
        properties : typing.Optional[typing.Iterable[global___Property]] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["bounding_poly",b"bounding_poly"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["bounding_poly",b"bounding_poly","confidence",b"confidence","description",b"description","locale",b"locale","locations",b"locations","mid",b"mid","properties",b"properties","score",b"score","topicality",b"topicality"]) -> None: ...
global___EntityAnnotation = EntityAnnotation

class SafeSearchAnnotation(google.protobuf.message.Message):
    """Set of features pertaining to the image, computed by computer vision
    methods over safe-search verticals (for example, adult, spoof, medical,
    violence).
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    ADULT_FIELD_NUMBER: builtins.int
    SPOOF_FIELD_NUMBER: builtins.int
    MEDICAL_FIELD_NUMBER: builtins.int
    VIOLENCE_FIELD_NUMBER: builtins.int
    RACY_FIELD_NUMBER: builtins.int
    adult: global___Likelihood.ValueType = ...
    """Represents the adult content likelihood for the image. Adult content may
    contain elements such as nudity, pornographic images or cartoons, or
    sexual activities.
    """

    spoof: global___Likelihood.ValueType = ...
    """Spoof likelihood. The likelihood that an modification
    was made to the image's canonical version to make it appear
    funny or offensive.
    """

    medical: global___Likelihood.ValueType = ...
    """Likelihood that this is a medical image."""

    violence: global___Likelihood.ValueType = ...
    """Likelihood that this image contains violent content."""

    racy: global___Likelihood.ValueType = ...
    """Likelihood that the request image contains racy content. Racy content may
    include (but is not limited to) skimpy or sheer clothing, strategically
    covered nudity, lewd or provocative poses, or close-ups of sensitive
    body areas.
    """

    def __init__(self,
        *,
        adult : global___Likelihood.ValueType = ...,
        spoof : global___Likelihood.ValueType = ...,
        medical : global___Likelihood.ValueType = ...,
        violence : global___Likelihood.ValueType = ...,
        racy : global___Likelihood.ValueType = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["adult",b"adult","medical",b"medical","racy",b"racy","spoof",b"spoof","violence",b"violence"]) -> None: ...
global___SafeSearchAnnotation = SafeSearchAnnotation

class LatLongRect(google.protobuf.message.Message):
    """Rectangle determined by min and max `LatLng` pairs."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    MIN_LAT_LNG_FIELD_NUMBER: builtins.int
    MAX_LAT_LNG_FIELD_NUMBER: builtins.int
    @property
    def min_lat_lng(self) -> google.type.latlng_pb2.LatLng:
        """Min lat/long pair."""
        pass
    @property
    def max_lat_lng(self) -> google.type.latlng_pb2.LatLng:
        """Max lat/long pair."""
        pass
    def __init__(self,
        *,
        min_lat_lng : typing.Optional[google.type.latlng_pb2.LatLng] = ...,
        max_lat_lng : typing.Optional[google.type.latlng_pb2.LatLng] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["max_lat_lng",b"max_lat_lng","min_lat_lng",b"min_lat_lng"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["max_lat_lng",b"max_lat_lng","min_lat_lng",b"min_lat_lng"]) -> None: ...
global___LatLongRect = LatLongRect

class ColorInfo(google.protobuf.message.Message):
    """Color information consists of RGB channels, score, and the fraction of
    the image that the color occupies in the image.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    COLOR_FIELD_NUMBER: builtins.int
    SCORE_FIELD_NUMBER: builtins.int
    PIXEL_FRACTION_FIELD_NUMBER: builtins.int
    @property
    def color(self) -> google.type.color_pb2.Color:
        """RGB components of the color."""
        pass
    score: builtins.float = ...
    """Image-specific score for this color. Value in range [0, 1]."""

    pixel_fraction: builtins.float = ...
    """The fraction of pixels the color occupies in the image.
    Value in range [0, 1].
    """

    def __init__(self,
        *,
        color : typing.Optional[google.type.color_pb2.Color] = ...,
        score : builtins.float = ...,
        pixel_fraction : builtins.float = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["color",b"color"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["color",b"color","pixel_fraction",b"pixel_fraction","score",b"score"]) -> None: ...
global___ColorInfo = ColorInfo

class DominantColorsAnnotation(google.protobuf.message.Message):
    """Set of dominant colors and their corresponding scores."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    COLORS_FIELD_NUMBER: builtins.int
    @property
    def colors(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___ColorInfo]:
        """RGB color values with their score and pixel fraction."""
        pass
    def __init__(self,
        *,
        colors : typing.Optional[typing.Iterable[global___ColorInfo]] = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["colors",b"colors"]) -> None: ...
global___DominantColorsAnnotation = DominantColorsAnnotation

class ImageProperties(google.protobuf.message.Message):
    """Stores image properties, such as dominant colors."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    DOMINANT_COLORS_FIELD_NUMBER: builtins.int
    @property
    def dominant_colors(self) -> global___DominantColorsAnnotation:
        """If present, dominant colors completed successfully."""
        pass
    def __init__(self,
        *,
        dominant_colors : typing.Optional[global___DominantColorsAnnotation] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["dominant_colors",b"dominant_colors"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["dominant_colors",b"dominant_colors"]) -> None: ...
global___ImageProperties = ImageProperties

class CropHint(google.protobuf.message.Message):
    """Single crop hint that is used to generate a new crop when serving an image."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    BOUNDING_POLY_FIELD_NUMBER: builtins.int
    CONFIDENCE_FIELD_NUMBER: builtins.int
    IMPORTANCE_FRACTION_FIELD_NUMBER: builtins.int
    @property
    def bounding_poly(self) -> google.cloud.vision.v1p1beta1.geometry_pb2.BoundingPoly:
        """The bounding polygon for the crop region. The coordinates of the bounding
        box are in the original image's scale, as returned in `ImageParams`.
        """
        pass
    confidence: builtins.float = ...
    """Confidence of this being a salient region.  Range [0, 1]."""

    importance_fraction: builtins.float = ...
    """Fraction of importance of this salient region with respect to the original
    image.
    """

    def __init__(self,
        *,
        bounding_poly : typing.Optional[google.cloud.vision.v1p1beta1.geometry_pb2.BoundingPoly] = ...,
        confidence : builtins.float = ...,
        importance_fraction : builtins.float = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["bounding_poly",b"bounding_poly"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["bounding_poly",b"bounding_poly","confidence",b"confidence","importance_fraction",b"importance_fraction"]) -> None: ...
global___CropHint = CropHint

class CropHintsAnnotation(google.protobuf.message.Message):
    """Set of crop hints that are used to generate new crops when serving images."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    CROP_HINTS_FIELD_NUMBER: builtins.int
    @property
    def crop_hints(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___CropHint]:
        """Crop hint results."""
        pass
    def __init__(self,
        *,
        crop_hints : typing.Optional[typing.Iterable[global___CropHint]] = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["crop_hints",b"crop_hints"]) -> None: ...
global___CropHintsAnnotation = CropHintsAnnotation

class CropHintsParams(google.protobuf.message.Message):
    """Parameters for crop hints annotation request."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    ASPECT_RATIOS_FIELD_NUMBER: builtins.int
    @property
    def aspect_ratios(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[builtins.float]:
        """Aspect ratios in floats, representing the ratio of the width to the height
        of the image. For example, if the desired aspect ratio is 4/3, the
        corresponding float value should be 1.33333.  If not specified, the
        best possible crop is returned. The number of provided aspect ratios is
        limited to a maximum of 16; any aspect ratios provided after the 16th are
        ignored.
        """
        pass
    def __init__(self,
        *,
        aspect_ratios : typing.Optional[typing.Iterable[builtins.float]] = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["aspect_ratios",b"aspect_ratios"]) -> None: ...
global___CropHintsParams = CropHintsParams

class WebDetectionParams(google.protobuf.message.Message):
    """Parameters for web detection request."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    INCLUDE_GEO_RESULTS_FIELD_NUMBER: builtins.int
    include_geo_results: builtins.bool = ...
    """Whether to include results derived from the geo information in the image."""

    def __init__(self,
        *,
        include_geo_results : builtins.bool = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["include_geo_results",b"include_geo_results"]) -> None: ...
global___WebDetectionParams = WebDetectionParams

class TextDetectionParams(google.protobuf.message.Message):
    """Parameters for text detections. This is used to control TEXT_DETECTION and
    DOCUMENT_TEXT_DETECTION features.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    ENABLE_TEXT_DETECTION_CONFIDENCE_SCORE_FIELD_NUMBER: builtins.int
    enable_text_detection_confidence_score: builtins.bool = ...
    """By default, Cloud Vision API only includes confidence score for
    DOCUMENT_TEXT_DETECTION result. Set the flag to true to include confidence
    score for TEXT_DETECTION as well.
    """

    def __init__(self,
        *,
        enable_text_detection_confidence_score : builtins.bool = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["enable_text_detection_confidence_score",b"enable_text_detection_confidence_score"]) -> None: ...
global___TextDetectionParams = TextDetectionParams

class ImageContext(google.protobuf.message.Message):
    """Image context and/or feature-specific parameters."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    LAT_LONG_RECT_FIELD_NUMBER: builtins.int
    LANGUAGE_HINTS_FIELD_NUMBER: builtins.int
    CROP_HINTS_PARAMS_FIELD_NUMBER: builtins.int
    WEB_DETECTION_PARAMS_FIELD_NUMBER: builtins.int
    TEXT_DETECTION_PARAMS_FIELD_NUMBER: builtins.int
    @property
    def lat_long_rect(self) -> global___LatLongRect:
        """lat/long rectangle that specifies the location of the image."""
        pass
    @property
    def language_hints(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """List of languages to use for TEXT_DETECTION. In most cases, an empty value
        yields the best results since it enables automatic language detection. For
        languages based on the Latin alphabet, setting `language_hints` is not
        needed. In rare cases, when the language of the text in the image is known,
        setting a hint will help get better results (although it will be a
        significant hindrance if the hint is wrong). Text detection returns an
        error if one or more of the specified languages is not one of the
        [supported languages](https://cloud.google.com/vision/docs/languages).
        """
        pass
    @property
    def crop_hints_params(self) -> global___CropHintsParams:
        """Parameters for crop hints annotation request."""
        pass
    @property
    def web_detection_params(self) -> global___WebDetectionParams:
        """Parameters for web detection."""
        pass
    @property
    def text_detection_params(self) -> global___TextDetectionParams:
        """Parameters for text detection and document text detection."""
        pass
    def __init__(self,
        *,
        lat_long_rect : typing.Optional[global___LatLongRect] = ...,
        language_hints : typing.Optional[typing.Iterable[typing.Text]] = ...,
        crop_hints_params : typing.Optional[global___CropHintsParams] = ...,
        web_detection_params : typing.Optional[global___WebDetectionParams] = ...,
        text_detection_params : typing.Optional[global___TextDetectionParams] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["crop_hints_params",b"crop_hints_params","lat_long_rect",b"lat_long_rect","text_detection_params",b"text_detection_params","web_detection_params",b"web_detection_params"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["crop_hints_params",b"crop_hints_params","language_hints",b"language_hints","lat_long_rect",b"lat_long_rect","text_detection_params",b"text_detection_params","web_detection_params",b"web_detection_params"]) -> None: ...
global___ImageContext = ImageContext

class AnnotateImageRequest(google.protobuf.message.Message):
    """Request for performing Google Cloud Vision API tasks over a user-provided
    image, with user-requested features.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    IMAGE_FIELD_NUMBER: builtins.int
    FEATURES_FIELD_NUMBER: builtins.int
    IMAGE_CONTEXT_FIELD_NUMBER: builtins.int
    @property
    def image(self) -> global___Image:
        """The image to be processed."""
        pass
    @property
    def features(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___Feature]:
        """Requested features."""
        pass
    @property
    def image_context(self) -> global___ImageContext:
        """Additional context that may accompany the image."""
        pass
    def __init__(self,
        *,
        image : typing.Optional[global___Image] = ...,
        features : typing.Optional[typing.Iterable[global___Feature]] = ...,
        image_context : typing.Optional[global___ImageContext] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["image",b"image","image_context",b"image_context"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["features",b"features","image",b"image","image_context",b"image_context"]) -> None: ...
global___AnnotateImageRequest = AnnotateImageRequest

class AnnotateImageResponse(google.protobuf.message.Message):
    """Response to an image annotation request."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    FACE_ANNOTATIONS_FIELD_NUMBER: builtins.int
    LANDMARK_ANNOTATIONS_FIELD_NUMBER: builtins.int
    LOGO_ANNOTATIONS_FIELD_NUMBER: builtins.int
    LABEL_ANNOTATIONS_FIELD_NUMBER: builtins.int
    TEXT_ANNOTATIONS_FIELD_NUMBER: builtins.int
    FULL_TEXT_ANNOTATION_FIELD_NUMBER: builtins.int
    SAFE_SEARCH_ANNOTATION_FIELD_NUMBER: builtins.int
    IMAGE_PROPERTIES_ANNOTATION_FIELD_NUMBER: builtins.int
    CROP_HINTS_ANNOTATION_FIELD_NUMBER: builtins.int
    WEB_DETECTION_FIELD_NUMBER: builtins.int
    ERROR_FIELD_NUMBER: builtins.int
    @property
    def face_annotations(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___FaceAnnotation]:
        """If present, face detection has completed successfully."""
        pass
    @property
    def landmark_annotations(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___EntityAnnotation]:
        """If present, landmark detection has completed successfully."""
        pass
    @property
    def logo_annotations(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___EntityAnnotation]:
        """If present, logo detection has completed successfully."""
        pass
    @property
    def label_annotations(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___EntityAnnotation]:
        """If present, label detection has completed successfully."""
        pass
    @property
    def text_annotations(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___EntityAnnotation]:
        """If present, text (OCR) detection has completed successfully."""
        pass
    @property
    def full_text_annotation(self) -> google.cloud.vision.v1p1beta1.text_annotation_pb2.TextAnnotation:
        """If present, text (OCR) detection or document (OCR) text detection has
        completed successfully.
        This annotation provides the structural hierarchy for the OCR detected
        text.
        """
        pass
    @property
    def safe_search_annotation(self) -> global___SafeSearchAnnotation:
        """If present, safe-search annotation has completed successfully."""
        pass
    @property
    def image_properties_annotation(self) -> global___ImageProperties:
        """If present, image properties were extracted successfully."""
        pass
    @property
    def crop_hints_annotation(self) -> global___CropHintsAnnotation:
        """If present, crop hints have completed successfully."""
        pass
    @property
    def web_detection(self) -> google.cloud.vision.v1p1beta1.web_detection_pb2.WebDetection:
        """If present, web detection has completed successfully."""
        pass
    @property
    def error(self) -> google.rpc.status_pb2.Status:
        """If set, represents the error message for the operation.
        Note that filled-in image annotations are guaranteed to be
        correct, even when `error` is set.
        """
        pass
    def __init__(self,
        *,
        face_annotations : typing.Optional[typing.Iterable[global___FaceAnnotation]] = ...,
        landmark_annotations : typing.Optional[typing.Iterable[global___EntityAnnotation]] = ...,
        logo_annotations : typing.Optional[typing.Iterable[global___EntityAnnotation]] = ...,
        label_annotations : typing.Optional[typing.Iterable[global___EntityAnnotation]] = ...,
        text_annotations : typing.Optional[typing.Iterable[global___EntityAnnotation]] = ...,
        full_text_annotation : typing.Optional[google.cloud.vision.v1p1beta1.text_annotation_pb2.TextAnnotation] = ...,
        safe_search_annotation : typing.Optional[global___SafeSearchAnnotation] = ...,
        image_properties_annotation : typing.Optional[global___ImageProperties] = ...,
        crop_hints_annotation : typing.Optional[global___CropHintsAnnotation] = ...,
        web_detection : typing.Optional[google.cloud.vision.v1p1beta1.web_detection_pb2.WebDetection] = ...,
        error : typing.Optional[google.rpc.status_pb2.Status] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["crop_hints_annotation",b"crop_hints_annotation","error",b"error","full_text_annotation",b"full_text_annotation","image_properties_annotation",b"image_properties_annotation","safe_search_annotation",b"safe_search_annotation","web_detection",b"web_detection"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["crop_hints_annotation",b"crop_hints_annotation","error",b"error","face_annotations",b"face_annotations","full_text_annotation",b"full_text_annotation","image_properties_annotation",b"image_properties_annotation","label_annotations",b"label_annotations","landmark_annotations",b"landmark_annotations","logo_annotations",b"logo_annotations","safe_search_annotation",b"safe_search_annotation","text_annotations",b"text_annotations","web_detection",b"web_detection"]) -> None: ...
global___AnnotateImageResponse = AnnotateImageResponse

class BatchAnnotateImagesRequest(google.protobuf.message.Message):
    """Multiple image annotation requests are batched into a single service call."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    REQUESTS_FIELD_NUMBER: builtins.int
    @property
    def requests(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___AnnotateImageRequest]:
        """Required. Individual image annotation requests for this batch."""
        pass
    def __init__(self,
        *,
        requests : typing.Optional[typing.Iterable[global___AnnotateImageRequest]] = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["requests",b"requests"]) -> None: ...
global___BatchAnnotateImagesRequest = BatchAnnotateImagesRequest

class BatchAnnotateImagesResponse(google.protobuf.message.Message):
    """Response to a batch image annotation request."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    RESPONSES_FIELD_NUMBER: builtins.int
    @property
    def responses(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___AnnotateImageResponse]:
        """Individual responses to image annotation requests within the batch."""
        pass
    def __init__(self,
        *,
        responses : typing.Optional[typing.Iterable[global___AnnotateImageResponse]] = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["responses",b"responses"]) -> None: ...
global___BatchAnnotateImagesResponse = BatchAnnotateImagesResponse
