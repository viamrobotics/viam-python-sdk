"""
@generated by mypy-protobuf.  Do not edit manually!
isort:skip_file
"""
import builtins
import google.cloud.datalabeling.v1beta1.dataset_pb2
import google.cloud.datalabeling.v1beta1.evaluation_pb2
import google.cloud.datalabeling.v1beta1.human_annotation_config_pb2
import google.protobuf.descriptor
import google.protobuf.internal.containers
import google.protobuf.internal.enum_type_wrapper
import google.protobuf.message
import google.protobuf.timestamp_pb2
import google.rpc.status_pb2
import typing
import typing_extensions

DESCRIPTOR: google.protobuf.descriptor.FileDescriptor = ...

class EvaluationJob(google.protobuf.message.Message):
    """Defines an evaluation job that runs periodically to generate
    [Evaluations][google.cloud.datalabeling.v1beta1.Evaluation]. [Creating an evaluation
    job](/ml-engine/docs/continuous-evaluation/create-job) is the starting point
    for using continuous evaluation.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class _State:
        ValueType = typing.NewType('ValueType', builtins.int)
        V: typing_extensions.TypeAlias = ValueType
    class _StateEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_State.ValueType], builtins.type):
        DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor = ...
        STATE_UNSPECIFIED: EvaluationJob.State.ValueType = ...  # 0
        SCHEDULED: EvaluationJob.State.ValueType = ...  # 1
        """The job is scheduled to run at the [configured interval][google.cloud.datalabeling.v1beta1.EvaluationJob.schedule]. You
        can [pause][google.cloud.datalabeling.v1beta1.DataLabelingService.PauseEvaluationJob] or
        [delete][google.cloud.datalabeling.v1beta1.DataLabelingService.DeleteEvaluationJob] the job.

        When the job is in this state, it samples prediction input and output
        from your model version into your BigQuery table as predictions occur.
        """

        RUNNING: EvaluationJob.State.ValueType = ...  # 2
        """The job is currently running. When the job runs, Data Labeling Service
        does several things:

        1. If you have configured your job to use Data Labeling Service for
           ground truth labeling, the service creates a
           [Dataset][google.cloud.datalabeling.v1beta1.Dataset] and a labeling task for all data sampled
           since the last time the job ran. Human labelers provide ground truth
           labels for your data. Human labeling may take hours, or even days,
           depending on how much data has been sampled. The job remains in the
           `RUNNING` state during this time, and it can even be running multiple
           times in parallel if it gets triggered again (for example 24 hours
           later) before the earlier run has completed. When human labelers have
           finished labeling the data, the next step occurs.
           <br><br>
           If you have configured your job to provide your own ground truth
           labels, Data Labeling Service still creates a [Dataset][google.cloud.datalabeling.v1beta1.Dataset] for newly
           sampled data, but it expects that you have already added ground truth
           labels to the BigQuery table by this time. The next step occurs
           immediately.

        2. Data Labeling Service creates an [Evaluation][google.cloud.datalabeling.v1beta1.Evaluation] by comparing your
           model version's predictions with the ground truth labels.

        If the job remains in this state for a long time, it continues to sample
        prediction data into your BigQuery table and will run again at the next
        interval, even if it causes the job to run multiple times in parallel.
        """

        PAUSED: EvaluationJob.State.ValueType = ...  # 3
        """The job is not sampling prediction input and output into your BigQuery
        table and it will not run according to its schedule. You can
        [resume][google.cloud.datalabeling.v1beta1.DataLabelingService.ResumeEvaluationJob] the job.
        """

        STOPPED: EvaluationJob.State.ValueType = ...  # 4
        """The job has this state right before it is deleted."""

    class State(_State, metaclass=_StateEnumTypeWrapper):
        """State of the job."""
        pass

    STATE_UNSPECIFIED: EvaluationJob.State.ValueType = ...  # 0
    SCHEDULED: EvaluationJob.State.ValueType = ...  # 1
    """The job is scheduled to run at the [configured interval][google.cloud.datalabeling.v1beta1.EvaluationJob.schedule]. You
    can [pause][google.cloud.datalabeling.v1beta1.DataLabelingService.PauseEvaluationJob] or
    [delete][google.cloud.datalabeling.v1beta1.DataLabelingService.DeleteEvaluationJob] the job.

    When the job is in this state, it samples prediction input and output
    from your model version into your BigQuery table as predictions occur.
    """

    RUNNING: EvaluationJob.State.ValueType = ...  # 2
    """The job is currently running. When the job runs, Data Labeling Service
    does several things:

    1. If you have configured your job to use Data Labeling Service for
       ground truth labeling, the service creates a
       [Dataset][google.cloud.datalabeling.v1beta1.Dataset] and a labeling task for all data sampled
       since the last time the job ran. Human labelers provide ground truth
       labels for your data. Human labeling may take hours, or even days,
       depending on how much data has been sampled. The job remains in the
       `RUNNING` state during this time, and it can even be running multiple
       times in parallel if it gets triggered again (for example 24 hours
       later) before the earlier run has completed. When human labelers have
       finished labeling the data, the next step occurs.
       <br><br>
       If you have configured your job to provide your own ground truth
       labels, Data Labeling Service still creates a [Dataset][google.cloud.datalabeling.v1beta1.Dataset] for newly
       sampled data, but it expects that you have already added ground truth
       labels to the BigQuery table by this time. The next step occurs
       immediately.

    2. Data Labeling Service creates an [Evaluation][google.cloud.datalabeling.v1beta1.Evaluation] by comparing your
       model version's predictions with the ground truth labels.

    If the job remains in this state for a long time, it continues to sample
    prediction data into your BigQuery table and will run again at the next
    interval, even if it causes the job to run multiple times in parallel.
    """

    PAUSED: EvaluationJob.State.ValueType = ...  # 3
    """The job is not sampling prediction input and output into your BigQuery
    table and it will not run according to its schedule. You can
    [resume][google.cloud.datalabeling.v1beta1.DataLabelingService.ResumeEvaluationJob] the job.
    """

    STOPPED: EvaluationJob.State.ValueType = ...  # 4
    """The job has this state right before it is deleted."""


    NAME_FIELD_NUMBER: builtins.int
    DESCRIPTION_FIELD_NUMBER: builtins.int
    STATE_FIELD_NUMBER: builtins.int
    SCHEDULE_FIELD_NUMBER: builtins.int
    MODEL_VERSION_FIELD_NUMBER: builtins.int
    EVALUATION_JOB_CONFIG_FIELD_NUMBER: builtins.int
    ANNOTATION_SPEC_SET_FIELD_NUMBER: builtins.int
    LABEL_MISSING_GROUND_TRUTH_FIELD_NUMBER: builtins.int
    ATTEMPTS_FIELD_NUMBER: builtins.int
    CREATE_TIME_FIELD_NUMBER: builtins.int
    name: typing.Text = ...
    """Output only. After you create a job, Data Labeling Service assigns a name
    to the job with the following format:

    "projects/<var>{project_id}</var>/evaluationJobs/<var>{evaluation_job_id}</var>"
    """

    description: typing.Text = ...
    """Required. Description of the job. The description can be up to 25,000
    characters long.
    """

    state: global___EvaluationJob.State.ValueType = ...
    """Output only. Describes the current state of the job."""

    schedule: typing.Text = ...
    """Required. Describes the interval at which the job runs. This interval must
    be at least 1 day, and it is rounded to the nearest day. For example, if
    you specify a 50-hour interval, the job runs every 2 days.

    You can provide the schedule in
    [crontab format](/scheduler/docs/configuring/cron-job-schedules) or in an
    [English-like
    format](/appengine/docs/standard/python/config/cronref#schedule_format).

    Regardless of what you specify, the job will run at 10:00 AM UTC. Only the
    interval from this schedule is used, not the specific time of day.
    """

    model_version: typing.Text = ...
    """Required. The [AI Platform Prediction model
    version](/ml-engine/docs/prediction-overview) to be evaluated. Prediction
    input and output is sampled from this model version. When creating an
    evaluation job, specify the model version in the following format:

    "projects/<var>{project_id}</var>/models/<var>{model_name}</var>/versions/<var>{version_name}</var>"

    There can only be one evaluation job per model version.
    """

    @property
    def evaluation_job_config(self) -> global___EvaluationJobConfig:
        """Required. Configuration details for the evaluation job."""
        pass
    annotation_spec_set: typing.Text = ...
    """Required. Name of the [AnnotationSpecSet][google.cloud.datalabeling.v1beta1.AnnotationSpecSet] describing all the
    labels that your machine learning model outputs. You must create this
    resource before you create an evaluation job and provide its name in the
    following format:

    "projects/<var>{project_id}</var>/annotationSpecSets/<var>{annotation_spec_set_id}</var>"
    """

    label_missing_ground_truth: builtins.bool = ...
    """Required. Whether you want Data Labeling Service to provide ground truth
    labels for prediction input. If you want the service to assign human
    labelers to annotate your data, set this to `true`. If you want to provide
    your own ground truth labels in the evaluation job's BigQuery table, set
    this to `false`.
    """

    @property
    def attempts(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___Attempt]:
        """Output only. Every time the evaluation job runs and an error occurs, the
        failed attempt is appended to this array.
        """
        pass
    @property
    def create_time(self) -> google.protobuf.timestamp_pb2.Timestamp:
        """Output only. Timestamp of when this evaluation job was created."""
        pass
    def __init__(self,
        *,
        name : typing.Text = ...,
        description : typing.Text = ...,
        state : global___EvaluationJob.State.ValueType = ...,
        schedule : typing.Text = ...,
        model_version : typing.Text = ...,
        evaluation_job_config : typing.Optional[global___EvaluationJobConfig] = ...,
        annotation_spec_set : typing.Text = ...,
        label_missing_ground_truth : builtins.bool = ...,
        attempts : typing.Optional[typing.Iterable[global___Attempt]] = ...,
        create_time : typing.Optional[google.protobuf.timestamp_pb2.Timestamp] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["create_time",b"create_time","evaluation_job_config",b"evaluation_job_config"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["annotation_spec_set",b"annotation_spec_set","attempts",b"attempts","create_time",b"create_time","description",b"description","evaluation_job_config",b"evaluation_job_config","label_missing_ground_truth",b"label_missing_ground_truth","model_version",b"model_version","name",b"name","schedule",b"schedule","state",b"state"]) -> None: ...
global___EvaluationJob = EvaluationJob

class EvaluationJobConfig(google.protobuf.message.Message):
    """Configures specific details of how a continuous evaluation job works. Provide
    this configuration when you create an EvaluationJob.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class BigqueryImportKeysEntry(google.protobuf.message.Message):
        DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
        KEY_FIELD_NUMBER: builtins.int
        VALUE_FIELD_NUMBER: builtins.int
        key: typing.Text = ...
        value: typing.Text = ...
        def __init__(self,
            *,
            key : typing.Text = ...,
            value : typing.Text = ...,
            ) -> None: ...
        def ClearField(self, field_name: typing_extensions.Literal["key",b"key","value",b"value"]) -> None: ...

    IMAGE_CLASSIFICATION_CONFIG_FIELD_NUMBER: builtins.int
    BOUNDING_POLY_CONFIG_FIELD_NUMBER: builtins.int
    TEXT_CLASSIFICATION_CONFIG_FIELD_NUMBER: builtins.int
    INPUT_CONFIG_FIELD_NUMBER: builtins.int
    EVALUATION_CONFIG_FIELD_NUMBER: builtins.int
    HUMAN_ANNOTATION_CONFIG_FIELD_NUMBER: builtins.int
    BIGQUERY_IMPORT_KEYS_FIELD_NUMBER: builtins.int
    EXAMPLE_COUNT_FIELD_NUMBER: builtins.int
    EXAMPLE_SAMPLE_PERCENTAGE_FIELD_NUMBER: builtins.int
    EVALUATION_JOB_ALERT_CONFIG_FIELD_NUMBER: builtins.int
    @property
    def image_classification_config(self) -> google.cloud.datalabeling.v1beta1.human_annotation_config_pb2.ImageClassificationConfig:
        """Specify this field if your model version performs image classification or
        general classification.

        `annotationSpecSet` in this configuration must match
        [EvaluationJob.annotationSpecSet][google.cloud.datalabeling.v1beta1.EvaluationJob.annotation_spec_set].
        `allowMultiLabel` in this configuration must match
        `classificationMetadata.isMultiLabel` in [input_config][google.cloud.datalabeling.v1beta1.EvaluationJobConfig.input_config].
        """
        pass
    @property
    def bounding_poly_config(self) -> google.cloud.datalabeling.v1beta1.human_annotation_config_pb2.BoundingPolyConfig:
        """Specify this field if your model version performs image object detection
        (bounding box detection).

        `annotationSpecSet` in this configuration must match
        [EvaluationJob.annotationSpecSet][google.cloud.datalabeling.v1beta1.EvaluationJob.annotation_spec_set].
        """
        pass
    @property
    def text_classification_config(self) -> google.cloud.datalabeling.v1beta1.human_annotation_config_pb2.TextClassificationConfig:
        """Specify this field if your model version performs text classification.

        `annotationSpecSet` in this configuration must match
        [EvaluationJob.annotationSpecSet][google.cloud.datalabeling.v1beta1.EvaluationJob.annotation_spec_set].
        `allowMultiLabel` in this configuration must match
        `classificationMetadata.isMultiLabel` in [input_config][google.cloud.datalabeling.v1beta1.EvaluationJobConfig.input_config].
        """
        pass
    @property
    def input_config(self) -> google.cloud.datalabeling.v1beta1.dataset_pb2.InputConfig:
        """Rquired. Details for the sampled prediction input. Within this
        configuration, there are requirements for several fields:

        * `dataType` must be one of `IMAGE`, `TEXT`, or `GENERAL_DATA`.
        * `annotationType` must be one of `IMAGE_CLASSIFICATION_ANNOTATION`,
          `TEXT_CLASSIFICATION_ANNOTATION`, `GENERAL_CLASSIFICATION_ANNOTATION`,
          or `IMAGE_BOUNDING_BOX_ANNOTATION` (image object detection).
        * If your machine learning model performs classification, you must specify
          `classificationMetadata.isMultiLabel`.
        * You must specify `bigquerySource` (not `gcsSource`).
        """
        pass
    @property
    def evaluation_config(self) -> google.cloud.datalabeling.v1beta1.evaluation_pb2.EvaluationConfig:
        """Required. Details for calculating evaluation metrics and creating
        [Evaulations][google.cloud.datalabeling.v1beta1.Evaluation]. If your model version performs image object
        detection, you must specify the `boundingBoxEvaluationOptions` field within
        this configuration. Otherwise, provide an empty object for this
        configuration.
        """
        pass
    @property
    def human_annotation_config(self) -> google.cloud.datalabeling.v1beta1.human_annotation_config_pb2.HumanAnnotationConfig:
        """Optional. Details for human annotation of your data. If you set
        [labelMissingGroundTruth][google.cloud.datalabeling.v1beta1.EvaluationJob.label_missing_ground_truth] to
        `true` for this evaluation job, then you must specify this field. If you
        plan to provide your own ground truth labels, then omit this field.

        Note that you must create an [Instruction][google.cloud.datalabeling.v1beta1.Instruction] resource before you can
        specify this field. Provide the name of the instruction resource in the
        `instruction` field within this configuration.
        """
        pass
    @property
    def bigquery_import_keys(self) -> google.protobuf.internal.containers.ScalarMap[typing.Text, typing.Text]:
        """Required. Prediction keys that tell Data Labeling Service where to find the
        data for evaluation in your BigQuery table. When the service samples
        prediction input and output from your model version and saves it to
        BigQuery, the data gets stored as JSON strings in the BigQuery table. These
        keys tell Data Labeling Service how to parse the JSON.

        You can provide the following entries in this field:

        * `data_json_key`: the data key for prediction input. You must provide
          either this key or `reference_json_key`.
        * `reference_json_key`: the data reference key for prediction input. You
          must provide either this key or `data_json_key`.
        * `label_json_key`: the label key for prediction output. Required.
        * `label_score_json_key`: the score key for prediction output. Required.
        * `bounding_box_json_key`: the bounding box key for prediction output.
          Required if your model version perform image object detection.

        Learn [how to configure prediction
        keys](/ml-engine/docs/continuous-evaluation/create-job#prediction-keys).
        """
        pass
    example_count: builtins.int = ...
    """Required. The maximum number of predictions to sample and save to BigQuery
    during each [evaluation interval][google.cloud.datalabeling.v1beta1.EvaluationJob.schedule]. This limit
    overrides `example_sample_percentage`: even if the service has not sampled
    enough predictions to fulfill `example_sample_perecentage` during an
    interval, it stops sampling predictions when it meets this limit.
    """

    example_sample_percentage: builtins.float = ...
    """Required. Fraction of predictions to sample and save to BigQuery during
    each [evaluation interval][google.cloud.datalabeling.v1beta1.EvaluationJob.schedule]. For example, 0.1 means
    10% of predictions served by your model version get saved to BigQuery.
    """

    @property
    def evaluation_job_alert_config(self) -> global___EvaluationJobAlertConfig:
        """Optional. Configuration details for evaluation job alerts. Specify this
        field if you want to receive email alerts if the evaluation job finds that
        your predictions have low mean average precision during a run.
        """
        pass
    def __init__(self,
        *,
        image_classification_config : typing.Optional[google.cloud.datalabeling.v1beta1.human_annotation_config_pb2.ImageClassificationConfig] = ...,
        bounding_poly_config : typing.Optional[google.cloud.datalabeling.v1beta1.human_annotation_config_pb2.BoundingPolyConfig] = ...,
        text_classification_config : typing.Optional[google.cloud.datalabeling.v1beta1.human_annotation_config_pb2.TextClassificationConfig] = ...,
        input_config : typing.Optional[google.cloud.datalabeling.v1beta1.dataset_pb2.InputConfig] = ...,
        evaluation_config : typing.Optional[google.cloud.datalabeling.v1beta1.evaluation_pb2.EvaluationConfig] = ...,
        human_annotation_config : typing.Optional[google.cloud.datalabeling.v1beta1.human_annotation_config_pb2.HumanAnnotationConfig] = ...,
        bigquery_import_keys : typing.Optional[typing.Mapping[typing.Text, typing.Text]] = ...,
        example_count : builtins.int = ...,
        example_sample_percentage : builtins.float = ...,
        evaluation_job_alert_config : typing.Optional[global___EvaluationJobAlertConfig] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["bounding_poly_config",b"bounding_poly_config","evaluation_config",b"evaluation_config","evaluation_job_alert_config",b"evaluation_job_alert_config","human_annotation_config",b"human_annotation_config","human_annotation_request_config",b"human_annotation_request_config","image_classification_config",b"image_classification_config","input_config",b"input_config","text_classification_config",b"text_classification_config"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["bigquery_import_keys",b"bigquery_import_keys","bounding_poly_config",b"bounding_poly_config","evaluation_config",b"evaluation_config","evaluation_job_alert_config",b"evaluation_job_alert_config","example_count",b"example_count","example_sample_percentage",b"example_sample_percentage","human_annotation_config",b"human_annotation_config","human_annotation_request_config",b"human_annotation_request_config","image_classification_config",b"image_classification_config","input_config",b"input_config","text_classification_config",b"text_classification_config"]) -> None: ...
    def WhichOneof(self, oneof_group: typing_extensions.Literal["human_annotation_request_config",b"human_annotation_request_config"]) -> typing.Optional[typing_extensions.Literal["image_classification_config","bounding_poly_config","text_classification_config"]]: ...
global___EvaluationJobConfig = EvaluationJobConfig

class EvaluationJobAlertConfig(google.protobuf.message.Message):
    """Provides details for how an evaluation job sends email alerts based on the
    results of a run.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    EMAIL_FIELD_NUMBER: builtins.int
    MIN_ACCEPTABLE_MEAN_AVERAGE_PRECISION_FIELD_NUMBER: builtins.int
    email: typing.Text = ...
    """Required. An email address to send alerts to."""

    min_acceptable_mean_average_precision: builtins.float = ...
    """Required. A number between 0 and 1 that describes a minimum mean average
    precision threshold. When the evaluation job runs, if it calculates that
    your model version's predictions from the recent interval have
    [meanAveragePrecision][google.cloud.datalabeling.v1beta1.PrCurve.mean_average_precision] below this
    threshold, then it sends an alert to your specified email.
    """

    def __init__(self,
        *,
        email : typing.Text = ...,
        min_acceptable_mean_average_precision : builtins.float = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["email",b"email","min_acceptable_mean_average_precision",b"min_acceptable_mean_average_precision"]) -> None: ...
global___EvaluationJobAlertConfig = EvaluationJobAlertConfig

class Attempt(google.protobuf.message.Message):
    """Records a failed evaluation job run."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    ATTEMPT_TIME_FIELD_NUMBER: builtins.int
    PARTIAL_FAILURES_FIELD_NUMBER: builtins.int
    @property
    def attempt_time(self) -> google.protobuf.timestamp_pb2.Timestamp: ...
    @property
    def partial_failures(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[google.rpc.status_pb2.Status]:
        """Details of errors that occurred."""
        pass
    def __init__(self,
        *,
        attempt_time : typing.Optional[google.protobuf.timestamp_pb2.Timestamp] = ...,
        partial_failures : typing.Optional[typing.Iterable[google.rpc.status_pb2.Status]] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["attempt_time",b"attempt_time"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["attempt_time",b"attempt_time","partial_failures",b"partial_failures"]) -> None: ...
global___Attempt = Attempt
