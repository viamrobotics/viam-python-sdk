"""
@generated by mypy-protobuf.  Do not edit manually!
isort:skip_file
"""
import builtins
import google.protobuf.descriptor
import google.protobuf.internal.containers
import google.protobuf.internal.enum_type_wrapper
import google.protobuf.message
import google.rpc.status_pb2
import typing
import typing_extensions

DESCRIPTOR: google.protobuf.descriptor.FileDescriptor = ...

class TranslateSpeechConfig(google.protobuf.message.Message):
    """Provides information to the speech translation that specifies how to process
    the request.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    AUDIO_ENCODING_FIELD_NUMBER: builtins.int
    SOURCE_LANGUAGE_CODE_FIELD_NUMBER: builtins.int
    TARGET_LANGUAGE_CODE_FIELD_NUMBER: builtins.int
    ALTERNATIVE_SOURCE_LANGUAGE_CODES_FIELD_NUMBER: builtins.int
    SAMPLE_RATE_HERTZ_FIELD_NUMBER: builtins.int
    MODEL_FIELD_NUMBER: builtins.int
    audio_encoding: typing.Text = ...
    """Required. Encoding of audio data.
    Supported formats:

    - `linear16`

      Uncompressed 16-bit signed little-endian samples (Linear PCM).

    - `flac`

      `flac` (Free Lossless Audio Codec) is the recommended encoding
      because it is lossless--therefore recognition is not compromised--and
      requires only about half the bandwidth of `linear16`.

    - `mulaw`

      8-bit samples that compand 14-bit audio samples using G.711 PCMU/mu-law.

    - `amr`

      Adaptive Multi-Rate Narrowband codec. `sample_rate_hertz` must be 8000.

    - `amr-wb`

      Adaptive Multi-Rate Wideband codec. `sample_rate_hertz` must be 16000.

    - `ogg-opus`

      Opus encoded audio frames in Ogg container
      ([OggOpus](https://wiki.xiph.org/OggOpus)).
      `sample_rate_hertz` must be one of 8000, 12000, 16000, 24000, or 48000.

    - `mp3`

      MP3 audio. Support all standard MP3 bitrates (which range from 32-320
      kbps). When using this encoding, `sample_rate_hertz` has to match the
      sample rate of the file being used.
    """

    source_language_code: typing.Text = ...
    """Required. Source language code (BCP-47) of the input audio."""

    target_language_code: typing.Text = ...
    """Required. Target language code (BCP-47) of the output."""

    @property
    def alternative_source_language_codes(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """Optional. A list of up to 3 additional language codes (BCP-47), listing possible
        alternative languages of the supplied audio. If alternative source
        languages are listed, speech translation result will translate in the most
        likely language detected including the main source_language_code. The
        translated result will include the language code of the language detected
        in the audio.
        Note:
        1. If the provided alternative_source_language_code is not supported
        by current API version, we will skip that language code.
        2. If user only provided one eligible alternative_source_language_codes,
        the translation will happen between source_language_code and
        alternative_source_language_codes. The target_language_code will be
        ignored. It will be useful in conversation mode.
        """
        pass
    sample_rate_hertz: builtins.int = ...
    """Optional. Sample rate in Hertz of the audio data. Valid values are:
    8000-48000. 16000 is optimal. For best results, set the sampling rate of
    the audio source to 16000 Hz. If that's not possible, use the native sample
    rate of the audio source (instead of re-sampling).
    """

    model: typing.Text = ...
    """Optional."""

    def __init__(self,
        *,
        audio_encoding : typing.Text = ...,
        source_language_code : typing.Text = ...,
        target_language_code : typing.Text = ...,
        alternative_source_language_codes : typing.Optional[typing.Iterable[typing.Text]] = ...,
        sample_rate_hertz : builtins.int = ...,
        model : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["alternative_source_language_codes",b"alternative_source_language_codes","audio_encoding",b"audio_encoding","model",b"model","sample_rate_hertz",b"sample_rate_hertz","source_language_code",b"source_language_code","target_language_code",b"target_language_code"]) -> None: ...
global___TranslateSpeechConfig = TranslateSpeechConfig

class StreamingTranslateSpeechConfig(google.protobuf.message.Message):
    """Config used for streaming translation."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    AUDIO_CONFIG_FIELD_NUMBER: builtins.int
    SINGLE_UTTERANCE_FIELD_NUMBER: builtins.int
    STABILITY_FIELD_NUMBER: builtins.int
    TRANSLATION_MODE_FIELD_NUMBER: builtins.int
    DISABLE_INTERIM_RESULTS_FIELD_NUMBER: builtins.int
    @property
    def audio_config(self) -> global___TranslateSpeechConfig:
        """Required. The common config for all the following audio contents."""
        pass
    single_utterance: builtins.bool = ...
    """Optional. If `false` or omitted, the system performs
    continuous translation (continuing to wait for and process audio even if
    the user pauses speaking) until the client closes the input stream (gRPC
    API) or until the maximum time limit has been reached. May return multiple
    `StreamingTranslateSpeechResult`s with the `is_final` flag set to `true`.

    If `true`, the speech translator will detect a single spoken utterance.
    When it detects that the user has paused or stopped speaking, it will
    return an `END_OF_SINGLE_UTTERANCE` event and cease translation.
    When the client receives `END_OF_SINGLE_UTTERANCE` event, the client should
    stop sending the requests. However, clients should keep receiving remaining
    responses until the stream is terminated. To construct the complete
    sentence in a streaming way, one should override (if `is_final` of previous
    response is false), or append (if 'is_final' of previous response is true).
    """

    stability: typing.Text = ...
    """Optional. Stability control for the media translation text. The value should be
    "LOW", "MEDIUM", "HIGH". It applies to text/text_and_audio translation
    only.
    For audio translation mode, we only support HIGH stability mode,
    low/medium stability mode will throw argument error.
    Default empty string will be treated as "HIGH" in audio translation mode;
    will be treated as "LOW" in other translation mode.
    Note that stability and speed would be trade off.
    1. "LOW": In low mode, translation service will start to do translation
    right after getting recognition response. The speed will be faster.
    2. "MEDIUM": In medium mode, translation service will
    check if the recognition response is stable enough or not, and only
    translate recognition response which is not likely to be changed later.
    3. "HIGH": In high mode, translation service will wait for more stable
    recognition responses, and then start to do translation. Also, the
    following recognition responses cannot modify previous recognition
    responses. Thus it may impact quality in some situation. "HIGH" stability
    will generate "final" responses more frequently.
    """

    translation_mode: typing.Text = ...
    """Optional. Translation mode, the value should be "text", "audio", "text_and_audio".
    Default empty string will be treated as "text".
    1. "text": The response will be text translation. Text translation has a
    field "is_final". Detailed definition can be found in
    `TextTranslationResult`.
    2. "audio": The response will be audio translation. Audio translation does
    not have "is_final" field, which means each audio translation response is
    stable and will not be changed by later response.
    Translation mode "audio" can only be used with "high" stability mode,
    3. "text_and_audio": The response will have a text translation, when
    "is_final" is true, we will also output its corresponding audio
    translation. When "is_final" is false, audio_translation field will be
    empty.
    """

    disable_interim_results: builtins.bool = ...
    """Optional. If disable_interim_results is true, we will only return "final" responses.
    Otherwise, we will return all the responses. Default value will be false.
    User can only set disable_interim_results to be true with "high" stability
    mode.
    """

    def __init__(self,
        *,
        audio_config : typing.Optional[global___TranslateSpeechConfig] = ...,
        single_utterance : builtins.bool = ...,
        stability : typing.Text = ...,
        translation_mode : typing.Text = ...,
        disable_interim_results : builtins.bool = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["audio_config",b"audio_config"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["audio_config",b"audio_config","disable_interim_results",b"disable_interim_results","single_utterance",b"single_utterance","stability",b"stability","translation_mode",b"translation_mode"]) -> None: ...
global___StreamingTranslateSpeechConfig = StreamingTranslateSpeechConfig

class StreamingTranslateSpeechRequest(google.protobuf.message.Message):
    """The top-level message sent by the client for the `StreamingTranslateSpeech`
    method. Multiple `StreamingTranslateSpeechRequest` messages are sent. The
    first message must contain a `streaming_config` message and must not contain
    `audio_content` data. All subsequent messages must contain `audio_content`
    data and must not contain a `streaming_config` message.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    STREAMING_CONFIG_FIELD_NUMBER: builtins.int
    AUDIO_CONTENT_FIELD_NUMBER: builtins.int
    @property
    def streaming_config(self) -> global___StreamingTranslateSpeechConfig:
        """Provides information to the recognizer that specifies how to process the
        request. The first `StreamingTranslateSpeechRequest` message must contain
        a `streaming_config` message.
        """
        pass
    audio_content: builtins.bytes = ...
    """The audio data to be translated. Sequential chunks of audio data are sent
    in sequential `StreamingTranslateSpeechRequest` messages. The first
    `StreamingTranslateSpeechRequest` message must not contain
    `audio_content` data and all subsequent `StreamingTranslateSpeechRequest`
    messages must contain `audio_content` data. The audio bytes must be
    encoded as specified in `StreamingTranslateSpeechConfig`. Note: as with
    all bytes fields, protobuffers use a pure binary representation (not
    base64).
    """

    def __init__(self,
        *,
        streaming_config : typing.Optional[global___StreamingTranslateSpeechConfig] = ...,
        audio_content : builtins.bytes = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["audio_content",b"audio_content","streaming_config",b"streaming_config","streaming_request",b"streaming_request"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["audio_content",b"audio_content","streaming_config",b"streaming_config","streaming_request",b"streaming_request"]) -> None: ...
    def WhichOneof(self, oneof_group: typing_extensions.Literal["streaming_request",b"streaming_request"]) -> typing.Optional[typing_extensions.Literal["streaming_config","audio_content"]]: ...
global___StreamingTranslateSpeechRequest = StreamingTranslateSpeechRequest

class StreamingTranslateSpeechResult(google.protobuf.message.Message):
    """A streaming speech translation result corresponding to a portion of the audio
    that is currently being processed.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class TextTranslationResult(google.protobuf.message.Message):
        """Text translation result."""
        DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
        TRANSLATION_FIELD_NUMBER: builtins.int
        IS_FINAL_FIELD_NUMBER: builtins.int
        translation: typing.Text = ...
        """Output only. The translated sentence."""

        is_final: builtins.bool = ...
        """Output only. If `false`, this `StreamingTranslateSpeechResult` represents
        an interim result that may change. If `true`, this is the final time the
        translation service will return this particular
        `StreamingTranslateSpeechResult`, the streaming translator will not
        return any further hypotheses for this portion of the transcript and
        corresponding audio.
        """

        def __init__(self,
            *,
            translation : typing.Text = ...,
            is_final : builtins.bool = ...,
            ) -> None: ...
        def ClearField(self, field_name: typing_extensions.Literal["is_final",b"is_final","translation",b"translation"]) -> None: ...

    class AudioTranslationResult(google.protobuf.message.Message):
        """Audio translation result."""
        DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
        AUDIO_TRANSLATION_FIELD_NUMBER: builtins.int
        audio_translation: builtins.bytes = ...
        """Output only. The translated audio."""

        def __init__(self,
            *,
            audio_translation : builtins.bytes = ...,
            ) -> None: ...
        def ClearField(self, field_name: typing_extensions.Literal["audio_translation",b"audio_translation"]) -> None: ...

    TEXT_TRANSLATION_RESULT_FIELD_NUMBER: builtins.int
    AUDIO_TRANSLATION_RESULT_FIELD_NUMBER: builtins.int
    RECOGNITION_RESULT_FIELD_NUMBER: builtins.int
    DETECTED_SOURCE_LANGUAGE_CODE_FIELD_NUMBER: builtins.int
    @property
    def text_translation_result(self) -> global___StreamingTranslateSpeechResult.TextTranslationResult:
        """Text translation result."""
        pass
    @property
    def audio_translation_result(self) -> global___StreamingTranslateSpeechResult.AudioTranslationResult:
        """Audio translation result."""
        pass
    recognition_result: typing.Text = ...
    """Output only. The debug only recognition result in original language. This field is debug
    only and will be set to empty string if not available.
    This is implementation detail and will not be backward compatible.
    """

    detected_source_language_code: typing.Text = ...
    """Output only."""

    def __init__(self,
        *,
        text_translation_result : typing.Optional[global___StreamingTranslateSpeechResult.TextTranslationResult] = ...,
        audio_translation_result : typing.Optional[global___StreamingTranslateSpeechResult.AudioTranslationResult] = ...,
        recognition_result : typing.Text = ...,
        detected_source_language_code : typing.Text = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["audio_translation_result",b"audio_translation_result","text_translation_result",b"text_translation_result"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["audio_translation_result",b"audio_translation_result","detected_source_language_code",b"detected_source_language_code","recognition_result",b"recognition_result","text_translation_result",b"text_translation_result"]) -> None: ...
global___StreamingTranslateSpeechResult = StreamingTranslateSpeechResult

class StreamingTranslateSpeechResponse(google.protobuf.message.Message):
    """A streaming speech translation response corresponding to a portion of
    the audio currently processed.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class _SpeechEventType:
        ValueType = typing.NewType('ValueType', builtins.int)
        V: typing_extensions.TypeAlias = ValueType
    class _SpeechEventTypeEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_SpeechEventType.ValueType], builtins.type):
        DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor = ...
        SPEECH_EVENT_TYPE_UNSPECIFIED: StreamingTranslateSpeechResponse.SpeechEventType.ValueType = ...  # 0
        """No speech event specified."""

        END_OF_SINGLE_UTTERANCE: StreamingTranslateSpeechResponse.SpeechEventType.ValueType = ...  # 1
        """This event indicates that the server has detected the end of the user's
        speech utterance and expects no additional speech. Therefore, the server
        will not process additional audio (although it may subsequently return
        additional results). When the client receives `END_OF_SINGLE_UTTERANCE`
        event, the client should stop sending the requests. However, clients
        should keep receiving remaining responses until the stream is terminated.
        To construct the complete sentence in a streaming way, one should
        override (if `is_final` of previous response is `false`), or append (if
        `is_final` of previous response is `true`). This event is only sent if
        `single_utterance` was set to `true`, and is not used otherwise.
        """

    class SpeechEventType(_SpeechEventType, metaclass=_SpeechEventTypeEnumTypeWrapper):
        """Indicates the type of speech event."""
        pass

    SPEECH_EVENT_TYPE_UNSPECIFIED: StreamingTranslateSpeechResponse.SpeechEventType.ValueType = ...  # 0
    """No speech event specified."""

    END_OF_SINGLE_UTTERANCE: StreamingTranslateSpeechResponse.SpeechEventType.ValueType = ...  # 1
    """This event indicates that the server has detected the end of the user's
    speech utterance and expects no additional speech. Therefore, the server
    will not process additional audio (although it may subsequently return
    additional results). When the client receives `END_OF_SINGLE_UTTERANCE`
    event, the client should stop sending the requests. However, clients
    should keep receiving remaining responses until the stream is terminated.
    To construct the complete sentence in a streaming way, one should
    override (if `is_final` of previous response is `false`), or append (if
    `is_final` of previous response is `true`). This event is only sent if
    `single_utterance` was set to `true`, and is not used otherwise.
    """


    ERROR_FIELD_NUMBER: builtins.int
    RESULT_FIELD_NUMBER: builtins.int
    SPEECH_EVENT_TYPE_FIELD_NUMBER: builtins.int
    @property
    def error(self) -> google.rpc.status_pb2.Status:
        """Output only. If set, returns a [google.rpc.Status][google.rpc.Status] message that
        specifies the error for the operation.
        """
        pass
    @property
    def result(self) -> global___StreamingTranslateSpeechResult:
        """Output only. The translation result that is currently being processed (For text
        translation, `is_final` could be `true` or `false`.
        For audio translation, we do not have is_final field, which means each
        audio response is stable and will not get changed later. For
        text_and_audio, we still have `is_final` field in text translation, but we
        only output corresponsding audio when `is_final` is true.).
        """
        pass
    speech_event_type: global___StreamingTranslateSpeechResponse.SpeechEventType.ValueType = ...
    """Output only. Indicates the type of speech event."""

    def __init__(self,
        *,
        error : typing.Optional[google.rpc.status_pb2.Status] = ...,
        result : typing.Optional[global___StreamingTranslateSpeechResult] = ...,
        speech_event_type : global___StreamingTranslateSpeechResponse.SpeechEventType.ValueType = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["error",b"error","result",b"result"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["error",b"error","result",b"result","speech_event_type",b"speech_event_type"]) -> None: ...
global___StreamingTranslateSpeechResponse = StreamingTranslateSpeechResponse
