"""
@generated by mypy-protobuf.  Do not edit manually!
isort:skip_file
"""
import builtins
import google.protobuf.descriptor
import google.protobuf.duration_pb2
import google.protobuf.internal.containers
import google.protobuf.internal.enum_type_wrapper
import google.protobuf.message
import typing
import typing_extensions

DESCRIPTOR: google.protobuf.descriptor.FileDescriptor = ...

class _AudioEncoding:
    ValueType = typing.NewType('ValueType', builtins.int)
    V: typing_extensions.TypeAlias = ValueType
class _AudioEncodingEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_AudioEncoding.ValueType], builtins.type):
    DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor = ...
    AUDIO_ENCODING_UNSPECIFIED: AudioEncoding.ValueType = ...  # 0
    """Not specified."""

    AUDIO_ENCODING_LINEAR_16: AudioEncoding.ValueType = ...  # 1
    """Uncompressed 16-bit signed little-endian samples (Linear PCM)."""

    AUDIO_ENCODING_FLAC: AudioEncoding.ValueType = ...  # 2
    """[`FLAC`](https://xiph.org/flac/documentation.html) (Free Lossless Audio
    Codec) is the recommended encoding because it is lossless (therefore
    recognition is not compromised) and requires only about half the
    bandwidth of `LINEAR16`. `FLAC` stream encoding supports 16-bit and
    24-bit samples, however, not all fields in `STREAMINFO` are supported.
    """

    AUDIO_ENCODING_MULAW: AudioEncoding.ValueType = ...  # 3
    """8-bit samples that compand 14-bit audio samples using G.711 PCMU/mu-law."""

    AUDIO_ENCODING_AMR: AudioEncoding.ValueType = ...  # 4
    """Adaptive Multi-Rate Narrowband codec. `sample_rate_hertz` must be 8000."""

    AUDIO_ENCODING_AMR_WB: AudioEncoding.ValueType = ...  # 5
    """Adaptive Multi-Rate Wideband codec. `sample_rate_hertz` must be 16000."""

    AUDIO_ENCODING_OGG_OPUS: AudioEncoding.ValueType = ...  # 6
    """Opus encoded audio frames in Ogg container
    ([OggOpus](https://wiki.xiph.org/OggOpus)).
    `sample_rate_hertz` must be 16000.
    """

    AUDIO_ENCODING_SPEEX_WITH_HEADER_BYTE: AudioEncoding.ValueType = ...  # 7
    """Although the use of lossy encodings is not recommended, if a very low
    bitrate encoding is required, `OGG_OPUS` is highly preferred over
    Speex encoding. The [Speex](https://speex.org/) encoding supported by
    Dialogflow API has a header byte in each block, as in MIME type
    `audio/x-speex-with-header-byte`.
    It is a variant of the RTP Speex encoding defined in
    [RFC 5574](https://tools.ietf.org/html/rfc5574).
    The stream is a sequence of blocks, one block per RTP packet. Each block
    starts with a byte containing the length of the block, in bytes, followed
    by one or more frames of Speex data, padded to an integral number of
    bytes (octets) as specified in RFC 5574. In other words, each RTP header
    is replaced with a single byte containing the block length. Only Speex
    wideband is supported. `sample_rate_hertz` must be 16000.
    """

class AudioEncoding(_AudioEncoding, metaclass=_AudioEncodingEnumTypeWrapper):
    """Audio encoding of the audio content sent in the conversational query request.
    Refer to the
    [Cloud Speech API
    documentation](https://cloud.google.com/speech-to-text/docs/basics) for more
    details.
    """
    pass

AUDIO_ENCODING_UNSPECIFIED: AudioEncoding.ValueType = ...  # 0
"""Not specified."""

AUDIO_ENCODING_LINEAR_16: AudioEncoding.ValueType = ...  # 1
"""Uncompressed 16-bit signed little-endian samples (Linear PCM)."""

AUDIO_ENCODING_FLAC: AudioEncoding.ValueType = ...  # 2
"""[`FLAC`](https://xiph.org/flac/documentation.html) (Free Lossless Audio
Codec) is the recommended encoding because it is lossless (therefore
recognition is not compromised) and requires only about half the
bandwidth of `LINEAR16`. `FLAC` stream encoding supports 16-bit and
24-bit samples, however, not all fields in `STREAMINFO` are supported.
"""

AUDIO_ENCODING_MULAW: AudioEncoding.ValueType = ...  # 3
"""8-bit samples that compand 14-bit audio samples using G.711 PCMU/mu-law."""

AUDIO_ENCODING_AMR: AudioEncoding.ValueType = ...  # 4
"""Adaptive Multi-Rate Narrowband codec. `sample_rate_hertz` must be 8000."""

AUDIO_ENCODING_AMR_WB: AudioEncoding.ValueType = ...  # 5
"""Adaptive Multi-Rate Wideband codec. `sample_rate_hertz` must be 16000."""

AUDIO_ENCODING_OGG_OPUS: AudioEncoding.ValueType = ...  # 6
"""Opus encoded audio frames in Ogg container
([OggOpus](https://wiki.xiph.org/OggOpus)).
`sample_rate_hertz` must be 16000.
"""

AUDIO_ENCODING_SPEEX_WITH_HEADER_BYTE: AudioEncoding.ValueType = ...  # 7
"""Although the use of lossy encodings is not recommended, if a very low
bitrate encoding is required, `OGG_OPUS` is highly preferred over
Speex encoding. The [Speex](https://speex.org/) encoding supported by
Dialogflow API has a header byte in each block, as in MIME type
`audio/x-speex-with-header-byte`.
It is a variant of the RTP Speex encoding defined in
[RFC 5574](https://tools.ietf.org/html/rfc5574).
The stream is a sequence of blocks, one block per RTP packet. Each block
starts with a byte containing the length of the block, in bytes, followed
by one or more frames of Speex data, padded to an integral number of
bytes (octets) as specified in RFC 5574. In other words, each RTP header
is replaced with a single byte containing the block length. Only Speex
wideband is supported. `sample_rate_hertz` must be 16000.
"""

global___AudioEncoding = AudioEncoding


class _SpeechModelVariant:
    ValueType = typing.NewType('ValueType', builtins.int)
    V: typing_extensions.TypeAlias = ValueType
class _SpeechModelVariantEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_SpeechModelVariant.ValueType], builtins.type):
    DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor = ...
    SPEECH_MODEL_VARIANT_UNSPECIFIED: SpeechModelVariant.ValueType = ...  # 0
    """No model variant specified. In this case Dialogflow defaults to
    USE_BEST_AVAILABLE.
    """

    USE_BEST_AVAILABLE: SpeechModelVariant.ValueType = ...  # 1
    """Use the best available variant of the [Speech
    model][InputAudioConfig.model] that the caller is eligible for.

    Please see the [Dialogflow
    docs](https://cloud.google.com/dialogflow/docs/data-logging) for
    how to make your project eligible for enhanced models.
    """

    USE_STANDARD: SpeechModelVariant.ValueType = ...  # 2
    """Use standard model variant even if an enhanced model is available.  See the
    [Cloud Speech
    documentation](https://cloud.google.com/speech-to-text/docs/enhanced-models)
    for details about enhanced models.
    """

    USE_ENHANCED: SpeechModelVariant.ValueType = ...  # 3
    """Use an enhanced model variant:

    * If an enhanced variant does not exist for the given
      [model][google.cloud.dialogflow.v2beta1.InputAudioConfig.model] and request language, Dialogflow falls
      back to the standard variant.

      The [Cloud Speech
      documentation](https://cloud.google.com/speech-to-text/docs/enhanced-models)
      describes which models have enhanced variants.

    * If the API caller isn't eligible for enhanced models, Dialogflow returns
      an error.  Please see the [Dialogflow
      docs](https://cloud.google.com/dialogflow/docs/data-logging)
      for how to make your project eligible.
    """

class SpeechModelVariant(_SpeechModelVariant, metaclass=_SpeechModelVariantEnumTypeWrapper):
    """Variant of the specified [Speech model][google.cloud.dialogflow.v2beta1.InputAudioConfig.model] to use.

    See the [Cloud Speech
    documentation](https://cloud.google.com/speech-to-text/docs/enhanced-models)
    for which models have different variants. For example, the "phone_call" model
    has both a standard and an enhanced variant. When you use an enhanced model,
    you will generally receive higher quality results than for a standard model.
    """
    pass

SPEECH_MODEL_VARIANT_UNSPECIFIED: SpeechModelVariant.ValueType = ...  # 0
"""No model variant specified. In this case Dialogflow defaults to
USE_BEST_AVAILABLE.
"""

USE_BEST_AVAILABLE: SpeechModelVariant.ValueType = ...  # 1
"""Use the best available variant of the [Speech
model][InputAudioConfig.model] that the caller is eligible for.

Please see the [Dialogflow
docs](https://cloud.google.com/dialogflow/docs/data-logging) for
how to make your project eligible for enhanced models.
"""

USE_STANDARD: SpeechModelVariant.ValueType = ...  # 2
"""Use standard model variant even if an enhanced model is available.  See the
[Cloud Speech
documentation](https://cloud.google.com/speech-to-text/docs/enhanced-models)
for details about enhanced models.
"""

USE_ENHANCED: SpeechModelVariant.ValueType = ...  # 3
"""Use an enhanced model variant:

* If an enhanced variant does not exist for the given
  [model][google.cloud.dialogflow.v2beta1.InputAudioConfig.model] and request language, Dialogflow falls
  back to the standard variant.

  The [Cloud Speech
  documentation](https://cloud.google.com/speech-to-text/docs/enhanced-models)
  describes which models have enhanced variants.

* If the API caller isn't eligible for enhanced models, Dialogflow returns
  an error.  Please see the [Dialogflow
  docs](https://cloud.google.com/dialogflow/docs/data-logging)
  for how to make your project eligible.
"""

global___SpeechModelVariant = SpeechModelVariant


class _SsmlVoiceGender:
    ValueType = typing.NewType('ValueType', builtins.int)
    V: typing_extensions.TypeAlias = ValueType
class _SsmlVoiceGenderEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_SsmlVoiceGender.ValueType], builtins.type):
    DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor = ...
    SSML_VOICE_GENDER_UNSPECIFIED: SsmlVoiceGender.ValueType = ...  # 0
    """An unspecified gender, which means that the client doesn't care which
    gender the selected voice will have.
    """

    SSML_VOICE_GENDER_MALE: SsmlVoiceGender.ValueType = ...  # 1
    """A male voice."""

    SSML_VOICE_GENDER_FEMALE: SsmlVoiceGender.ValueType = ...  # 2
    """A female voice."""

    SSML_VOICE_GENDER_NEUTRAL: SsmlVoiceGender.ValueType = ...  # 3
    """A gender-neutral voice."""

class SsmlVoiceGender(_SsmlVoiceGender, metaclass=_SsmlVoiceGenderEnumTypeWrapper):
    """Gender of the voice as described in
    [SSML voice element](https://www.w3.org/TR/speech-synthesis11/#edef_voice).
    """
    pass

SSML_VOICE_GENDER_UNSPECIFIED: SsmlVoiceGender.ValueType = ...  # 0
"""An unspecified gender, which means that the client doesn't care which
gender the selected voice will have.
"""

SSML_VOICE_GENDER_MALE: SsmlVoiceGender.ValueType = ...  # 1
"""A male voice."""

SSML_VOICE_GENDER_FEMALE: SsmlVoiceGender.ValueType = ...  # 2
"""A female voice."""

SSML_VOICE_GENDER_NEUTRAL: SsmlVoiceGender.ValueType = ...  # 3
"""A gender-neutral voice."""

global___SsmlVoiceGender = SsmlVoiceGender


class _OutputAudioEncoding:
    ValueType = typing.NewType('ValueType', builtins.int)
    V: typing_extensions.TypeAlias = ValueType
class _OutputAudioEncodingEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_OutputAudioEncoding.ValueType], builtins.type):
    DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor = ...
    OUTPUT_AUDIO_ENCODING_UNSPECIFIED: OutputAudioEncoding.ValueType = ...  # 0
    """Not specified."""

    OUTPUT_AUDIO_ENCODING_LINEAR_16: OutputAudioEncoding.ValueType = ...  # 1
    """Uncompressed 16-bit signed little-endian samples (Linear PCM).
    Audio content returned as LINEAR16 also contains a WAV header.
    """

    OUTPUT_AUDIO_ENCODING_MP3: OutputAudioEncoding.ValueType = ...  # 2
    """MP3 audio at 32kbps."""

    OUTPUT_AUDIO_ENCODING_MP3_64_KBPS: OutputAudioEncoding.ValueType = ...  # 4
    """MP3 audio at 64kbps."""

    OUTPUT_AUDIO_ENCODING_OGG_OPUS: OutputAudioEncoding.ValueType = ...  # 3
    """Opus encoded audio wrapped in an ogg container. The result will be a
    file which can be played natively on Android, and in browsers (at least
    Chrome and Firefox). The quality of the encoding is considerably higher
    than MP3 while using approximately the same bitrate.
    """

    OUTPUT_AUDIO_ENCODING_MULAW: OutputAudioEncoding.ValueType = ...  # 5
    """8-bit samples that compand 14-bit audio samples using G.711 PCMU/mu-law."""

class OutputAudioEncoding(_OutputAudioEncoding, metaclass=_OutputAudioEncodingEnumTypeWrapper):
    """Audio encoding of the output audio format in Text-To-Speech."""
    pass

OUTPUT_AUDIO_ENCODING_UNSPECIFIED: OutputAudioEncoding.ValueType = ...  # 0
"""Not specified."""

OUTPUT_AUDIO_ENCODING_LINEAR_16: OutputAudioEncoding.ValueType = ...  # 1
"""Uncompressed 16-bit signed little-endian samples (Linear PCM).
Audio content returned as LINEAR16 also contains a WAV header.
"""

OUTPUT_AUDIO_ENCODING_MP3: OutputAudioEncoding.ValueType = ...  # 2
"""MP3 audio at 32kbps."""

OUTPUT_AUDIO_ENCODING_MP3_64_KBPS: OutputAudioEncoding.ValueType = ...  # 4
"""MP3 audio at 64kbps."""

OUTPUT_AUDIO_ENCODING_OGG_OPUS: OutputAudioEncoding.ValueType = ...  # 3
"""Opus encoded audio wrapped in an ogg container. The result will be a
file which can be played natively on Android, and in browsers (at least
Chrome and Firefox). The quality of the encoding is considerably higher
than MP3 while using approximately the same bitrate.
"""

OUTPUT_AUDIO_ENCODING_MULAW: OutputAudioEncoding.ValueType = ...  # 5
"""8-bit samples that compand 14-bit audio samples using G.711 PCMU/mu-law."""

global___OutputAudioEncoding = OutputAudioEncoding


class _TelephonyDtmf:
    ValueType = typing.NewType('ValueType', builtins.int)
    V: typing_extensions.TypeAlias = ValueType
class _TelephonyDtmfEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_TelephonyDtmf.ValueType], builtins.type):
    DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor = ...
    TELEPHONY_DTMF_UNSPECIFIED: TelephonyDtmf.ValueType = ...  # 0
    """Not specified. This value may be used to indicate an absent digit."""

    DTMF_ONE: TelephonyDtmf.ValueType = ...  # 1
    """Number: '1'."""

    DTMF_TWO: TelephonyDtmf.ValueType = ...  # 2
    """Number: '2'."""

    DTMF_THREE: TelephonyDtmf.ValueType = ...  # 3
    """Number: '3'."""

    DTMF_FOUR: TelephonyDtmf.ValueType = ...  # 4
    """Number: '4'."""

    DTMF_FIVE: TelephonyDtmf.ValueType = ...  # 5
    """Number: '5'."""

    DTMF_SIX: TelephonyDtmf.ValueType = ...  # 6
    """Number: '6'."""

    DTMF_SEVEN: TelephonyDtmf.ValueType = ...  # 7
    """Number: '7'."""

    DTMF_EIGHT: TelephonyDtmf.ValueType = ...  # 8
    """Number: '8'."""

    DTMF_NINE: TelephonyDtmf.ValueType = ...  # 9
    """Number: '9'."""

    DTMF_ZERO: TelephonyDtmf.ValueType = ...  # 10
    """Number: '0'."""

    DTMF_A: TelephonyDtmf.ValueType = ...  # 11
    """Letter: 'A'."""

    DTMF_B: TelephonyDtmf.ValueType = ...  # 12
    """Letter: 'B'."""

    DTMF_C: TelephonyDtmf.ValueType = ...  # 13
    """Letter: 'C'."""

    DTMF_D: TelephonyDtmf.ValueType = ...  # 14
    """Letter: 'D'."""

    DTMF_STAR: TelephonyDtmf.ValueType = ...  # 15
    """Asterisk/star: '*'."""

    DTMF_POUND: TelephonyDtmf.ValueType = ...  # 16
    """Pound/diamond/hash/square/gate/octothorpe: '#'."""

class TelephonyDtmf(_TelephonyDtmf, metaclass=_TelephonyDtmfEnumTypeWrapper):
    """[DTMF](https://en.wikipedia.org/wiki/Dual-tone_multi-frequency_signaling)
    digit in Telephony Gateway.
    """
    pass

TELEPHONY_DTMF_UNSPECIFIED: TelephonyDtmf.ValueType = ...  # 0
"""Not specified. This value may be used to indicate an absent digit."""

DTMF_ONE: TelephonyDtmf.ValueType = ...  # 1
"""Number: '1'."""

DTMF_TWO: TelephonyDtmf.ValueType = ...  # 2
"""Number: '2'."""

DTMF_THREE: TelephonyDtmf.ValueType = ...  # 3
"""Number: '3'."""

DTMF_FOUR: TelephonyDtmf.ValueType = ...  # 4
"""Number: '4'."""

DTMF_FIVE: TelephonyDtmf.ValueType = ...  # 5
"""Number: '5'."""

DTMF_SIX: TelephonyDtmf.ValueType = ...  # 6
"""Number: '6'."""

DTMF_SEVEN: TelephonyDtmf.ValueType = ...  # 7
"""Number: '7'."""

DTMF_EIGHT: TelephonyDtmf.ValueType = ...  # 8
"""Number: '8'."""

DTMF_NINE: TelephonyDtmf.ValueType = ...  # 9
"""Number: '9'."""

DTMF_ZERO: TelephonyDtmf.ValueType = ...  # 10
"""Number: '0'."""

DTMF_A: TelephonyDtmf.ValueType = ...  # 11
"""Letter: 'A'."""

DTMF_B: TelephonyDtmf.ValueType = ...  # 12
"""Letter: 'B'."""

DTMF_C: TelephonyDtmf.ValueType = ...  # 13
"""Letter: 'C'."""

DTMF_D: TelephonyDtmf.ValueType = ...  # 14
"""Letter: 'D'."""

DTMF_STAR: TelephonyDtmf.ValueType = ...  # 15
"""Asterisk/star: '*'."""

DTMF_POUND: TelephonyDtmf.ValueType = ...  # 16
"""Pound/diamond/hash/square/gate/octothorpe: '#'."""

global___TelephonyDtmf = TelephonyDtmf


class SpeechContext(google.protobuf.message.Message):
    """Hints for the speech recognizer to help with recognition in a specific
    conversation state.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    PHRASES_FIELD_NUMBER: builtins.int
    BOOST_FIELD_NUMBER: builtins.int
    @property
    def phrases(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """Optional. A list of strings containing words and phrases that the speech
        recognizer should recognize with higher likelihood.

        This list can be used to:

        * improve accuracy for words and phrases you expect the user to say,
          e.g. typical commands for your Dialogflow agent
        * add additional words to the speech recognizer vocabulary
        * ...

        See the [Cloud Speech
        documentation](https://cloud.google.com/speech-to-text/quotas) for usage
        limits.
        """
        pass
    boost: builtins.float = ...
    """Optional. Boost for this context compared to other contexts:

    * If the boost is positive, Dialogflow will increase the probability that
      the phrases in this context are recognized over similar sounding phrases.
    * If the boost is unspecified or non-positive, Dialogflow will not apply
      any boost.

    Dialogflow recommends that you use boosts in the range (0, 20] and that you
    find a value that fits your use case with binary search.
    """

    def __init__(self,
        *,
        phrases : typing.Optional[typing.Iterable[typing.Text]] = ...,
        boost : builtins.float = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["boost",b"boost","phrases",b"phrases"]) -> None: ...
global___SpeechContext = SpeechContext

class SpeechWordInfo(google.protobuf.message.Message):
    """Information for a word recognized by the speech recognizer."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    WORD_FIELD_NUMBER: builtins.int
    START_OFFSET_FIELD_NUMBER: builtins.int
    END_OFFSET_FIELD_NUMBER: builtins.int
    CONFIDENCE_FIELD_NUMBER: builtins.int
    word: typing.Text = ...
    """The word this info is for."""

    @property
    def start_offset(self) -> google.protobuf.duration_pb2.Duration:
        """Time offset relative to the beginning of the audio that corresponds to the
        start of the spoken word. This is an experimental feature and the accuracy
        of the time offset can vary.
        """
        pass
    @property
    def end_offset(self) -> google.protobuf.duration_pb2.Duration:
        """Time offset relative to the beginning of the audio that corresponds to the
        end of the spoken word. This is an experimental feature and the accuracy of
        the time offset can vary.
        """
        pass
    confidence: builtins.float = ...
    """The Speech confidence between 0.0 and 1.0 for this word. A higher number
    indicates an estimated greater likelihood that the recognized word is
    correct. The default of 0.0 is a sentinel value indicating that confidence
    was not set.

    This field is not guaranteed to be fully stable over time for the same
    audio input. Users should also not rely on it to always be provided.
    """

    def __init__(self,
        *,
        word : typing.Text = ...,
        start_offset : typing.Optional[google.protobuf.duration_pb2.Duration] = ...,
        end_offset : typing.Optional[google.protobuf.duration_pb2.Duration] = ...,
        confidence : builtins.float = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["end_offset",b"end_offset","start_offset",b"start_offset"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["confidence",b"confidence","end_offset",b"end_offset","start_offset",b"start_offset","word",b"word"]) -> None: ...
global___SpeechWordInfo = SpeechWordInfo

class InputAudioConfig(google.protobuf.message.Message):
    """Instructs the speech recognizer on how to process the audio content."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    AUDIO_ENCODING_FIELD_NUMBER: builtins.int
    SAMPLE_RATE_HERTZ_FIELD_NUMBER: builtins.int
    LANGUAGE_CODE_FIELD_NUMBER: builtins.int
    ENABLE_WORD_INFO_FIELD_NUMBER: builtins.int
    PHRASE_HINTS_FIELD_NUMBER: builtins.int
    SPEECH_CONTEXTS_FIELD_NUMBER: builtins.int
    MODEL_FIELD_NUMBER: builtins.int
    MODEL_VARIANT_FIELD_NUMBER: builtins.int
    SINGLE_UTTERANCE_FIELD_NUMBER: builtins.int
    DISABLE_NO_SPEECH_RECOGNIZED_EVENT_FIELD_NUMBER: builtins.int
    audio_encoding: global___AudioEncoding.ValueType = ...
    """Required. Audio encoding of the audio content to process."""

    sample_rate_hertz: builtins.int = ...
    """Required. Sample rate (in Hertz) of the audio content sent in the query.
    Refer to
    [Cloud Speech API
    documentation](https://cloud.google.com/speech-to-text/docs/basics) for
    more details.
    """

    language_code: typing.Text = ...
    """Required. The language of the supplied audio. Dialogflow does not do
    translations. See [Language
    Support](https://cloud.google.com/dialogflow/docs/reference/language)
    for a list of the currently supported language codes. Note that queries in
    the same session do not necessarily need to specify the same language.
    """

    enable_word_info: builtins.bool = ...
    """If `true`, Dialogflow returns [SpeechWordInfo][google.cloud.dialogflow.v2beta1.SpeechWordInfo] in
    [StreamingRecognitionResult][google.cloud.dialogflow.v2beta1.StreamingRecognitionResult] with information about the recognized speech
    words, e.g. start and end time offsets. If false or unspecified, Speech
    doesn't return any word-level information.
    """

    @property
    def phrase_hints(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """A list of strings containing words and phrases that the speech
        recognizer should recognize with higher likelihood.

        See [the Cloud Speech
        documentation](https://cloud.google.com/speech-to-text/docs/basics#phrase-hints)
        for more details.

        This field is deprecated. Please use [speech_contexts]() instead. If you
        specify both [phrase_hints]() and [speech_contexts](), Dialogflow will
        treat the [phrase_hints]() as a single additional [SpeechContext]().
        """
        pass
    @property
    def speech_contexts(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___SpeechContext]:
        """Context information to assist speech recognition.

        See [the Cloud Speech
        documentation](https://cloud.google.com/speech-to-text/docs/basics#phrase-hints)
        for more details.
        """
        pass
    model: typing.Text = ...
    """Which Speech model to select for the given request. Select the
    model best suited to your domain to get best results. If a model is not
    explicitly specified, then we auto-select a model based on the parameters
    in the InputAudioConfig.
    If enhanced speech model is enabled for the agent and an enhanced
    version of the specified model for the language does not exist, then the
    speech is recognized using the standard version of the specified model.
    Refer to
    [Cloud Speech API
    documentation](https://cloud.google.com/speech-to-text/docs/basics#select-model)
    for more details.
    """

    model_variant: global___SpeechModelVariant.ValueType = ...
    """Which variant of the [Speech model][google.cloud.dialogflow.v2beta1.InputAudioConfig.model] to use."""

    single_utterance: builtins.bool = ...
    """If `false` (default), recognition does not cease until the
    client closes the stream.
    If `true`, the recognizer will detect a single spoken utterance in input
    audio. Recognition ceases when it detects the audio's voice has
    stopped or paused. In this case, once a detected intent is received, the
    client should close the stream and start a new request with a new stream as
    needed.
    Note: This setting is relevant only for streaming methods.
    Note: When specified, InputAudioConfig.single_utterance takes precedence
    over StreamingDetectIntentRequest.single_utterance.
    """

    disable_no_speech_recognized_event: builtins.bool = ...
    """Only used in [Participants.AnalyzeContent][google.cloud.dialogflow.v2beta1.Participants.AnalyzeContent] and
    [Participants.StreamingAnalyzeContent][google.cloud.dialogflow.v2beta1.Participants.StreamingAnalyzeContent].
    If `false` and recognition doesn't return any result, trigger
    `NO_SPEECH_RECOGNIZED` event to Dialogflow agent.
    """

    def __init__(self,
        *,
        audio_encoding : global___AudioEncoding.ValueType = ...,
        sample_rate_hertz : builtins.int = ...,
        language_code : typing.Text = ...,
        enable_word_info : builtins.bool = ...,
        phrase_hints : typing.Optional[typing.Iterable[typing.Text]] = ...,
        speech_contexts : typing.Optional[typing.Iterable[global___SpeechContext]] = ...,
        model : typing.Text = ...,
        model_variant : global___SpeechModelVariant.ValueType = ...,
        single_utterance : builtins.bool = ...,
        disable_no_speech_recognized_event : builtins.bool = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["audio_encoding",b"audio_encoding","disable_no_speech_recognized_event",b"disable_no_speech_recognized_event","enable_word_info",b"enable_word_info","language_code",b"language_code","model",b"model","model_variant",b"model_variant","phrase_hints",b"phrase_hints","sample_rate_hertz",b"sample_rate_hertz","single_utterance",b"single_utterance","speech_contexts",b"speech_contexts"]) -> None: ...
global___InputAudioConfig = InputAudioConfig

class VoiceSelectionParams(google.protobuf.message.Message):
    """Description of which voice to use for speech synthesis."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    NAME_FIELD_NUMBER: builtins.int
    SSML_GENDER_FIELD_NUMBER: builtins.int
    name: typing.Text = ...
    """Optional. The name of the voice. If not set, the service will choose a
    voice based on the other parameters such as language_code and
    [ssml_gender][google.cloud.dialogflow.v2beta1.VoiceSelectionParams.ssml_gender].

    For the list of available voices, please refer to [Supported voices and
    languages](https://cloud.google.com/text-to-speech/docs/voices).
    """

    ssml_gender: global___SsmlVoiceGender.ValueType = ...
    """Optional. The preferred gender of the voice. If not set, the service will
    choose a voice based on the other parameters such as language_code and
    [name][google.cloud.dialogflow.v2beta1.VoiceSelectionParams.name]. Note that this is only a preference, not requirement. If a
    voice of the appropriate gender is not available, the synthesizer should
    substitute a voice with a different gender rather than failing the request.
    """

    def __init__(self,
        *,
        name : typing.Text = ...,
        ssml_gender : global___SsmlVoiceGender.ValueType = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["name",b"name","ssml_gender",b"ssml_gender"]) -> None: ...
global___VoiceSelectionParams = VoiceSelectionParams

class SynthesizeSpeechConfig(google.protobuf.message.Message):
    """Configuration of how speech should be synthesized."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    SPEAKING_RATE_FIELD_NUMBER: builtins.int
    PITCH_FIELD_NUMBER: builtins.int
    VOLUME_GAIN_DB_FIELD_NUMBER: builtins.int
    EFFECTS_PROFILE_ID_FIELD_NUMBER: builtins.int
    VOICE_FIELD_NUMBER: builtins.int
    speaking_rate: builtins.float = ...
    """Optional. Speaking rate/speed, in the range [0.25, 4.0]. 1.0 is the normal
    native speed supported by the specific voice. 2.0 is twice as fast, and
    0.5 is half as fast. If unset(0.0), defaults to the native 1.0 speed. Any
    other values < 0.25 or > 4.0 will return an error.
    """

    pitch: builtins.float = ...
    """Optional. Speaking pitch, in the range [-20.0, 20.0]. 20 means increase 20
    semitones from the original pitch. -20 means decrease 20 semitones from the
    original pitch.
    """

    volume_gain_db: builtins.float = ...
    """Optional. Volume gain (in dB) of the normal native volume supported by the
    specific voice, in the range [-96.0, 16.0]. If unset, or set to a value of
    0.0 (dB), will play at normal native signal amplitude. A value of -6.0 (dB)
    will play at approximately half the amplitude of the normal native signal
    amplitude. A value of +6.0 (dB) will play at approximately twice the
    amplitude of the normal native signal amplitude. We strongly recommend not
    to exceed +10 (dB) as there's usually no effective increase in loudness for
    any value greater than that.
    """

    @property
    def effects_profile_id(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """Optional. An identifier which selects 'audio effects' profiles that are
        applied on (post synthesized) text to speech. Effects are applied on top of
        each other in the order they are given.
        """
        pass
    @property
    def voice(self) -> global___VoiceSelectionParams:
        """Optional. The desired voice of the synthesized audio."""
        pass
    def __init__(self,
        *,
        speaking_rate : builtins.float = ...,
        pitch : builtins.float = ...,
        volume_gain_db : builtins.float = ...,
        effects_profile_id : typing.Optional[typing.Iterable[typing.Text]] = ...,
        voice : typing.Optional[global___VoiceSelectionParams] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["voice",b"voice"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["effects_profile_id",b"effects_profile_id","pitch",b"pitch","speaking_rate",b"speaking_rate","voice",b"voice","volume_gain_db",b"volume_gain_db"]) -> None: ...
global___SynthesizeSpeechConfig = SynthesizeSpeechConfig

class OutputAudioConfig(google.protobuf.message.Message):
    """Instructs the speech synthesizer how to generate the output audio content.
    If this audio config is supplied in a request, it overrides all existing
    text-to-speech settings applied to the agent.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    AUDIO_ENCODING_FIELD_NUMBER: builtins.int
    SAMPLE_RATE_HERTZ_FIELD_NUMBER: builtins.int
    SYNTHESIZE_SPEECH_CONFIG_FIELD_NUMBER: builtins.int
    audio_encoding: global___OutputAudioEncoding.ValueType = ...
    """Required. Audio encoding of the synthesized audio content."""

    sample_rate_hertz: builtins.int = ...
    """The synthesis sample rate (in hertz) for this audio. If not
    provided, then the synthesizer will use the default sample rate based on
    the audio encoding. If this is different from the voice's natural sample
    rate, then the synthesizer will honor this request by converting to the
    desired sample rate (which might result in worse audio quality).
    """

    @property
    def synthesize_speech_config(self) -> global___SynthesizeSpeechConfig:
        """Configuration of how speech should be synthesized."""
        pass
    def __init__(self,
        *,
        audio_encoding : global___OutputAudioEncoding.ValueType = ...,
        sample_rate_hertz : builtins.int = ...,
        synthesize_speech_config : typing.Optional[global___SynthesizeSpeechConfig] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["synthesize_speech_config",b"synthesize_speech_config"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["audio_encoding",b"audio_encoding","sample_rate_hertz",b"sample_rate_hertz","synthesize_speech_config",b"synthesize_speech_config"]) -> None: ...
global___OutputAudioConfig = OutputAudioConfig

class TelephonyDtmfEvents(google.protobuf.message.Message):
    """A wrapper of repeated TelephonyDtmf digits."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    DTMF_EVENTS_FIELD_NUMBER: builtins.int
    @property
    def dtmf_events(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[global___TelephonyDtmf.ValueType]:
        """A sequence of TelephonyDtmf digits."""
        pass
    def __init__(self,
        *,
        dtmf_events : typing.Optional[typing.Iterable[global___TelephonyDtmf.ValueType]] = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["dtmf_events",b"dtmf_events"]) -> None: ...
global___TelephonyDtmfEvents = TelephonyDtmfEvents

class SpeechToTextConfig(google.protobuf.message.Message):
    """Configures speech transcription for [ConversationProfile][google.cloud.dialogflow.v2beta1.ConversationProfile]."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    SPEECH_MODEL_VARIANT_FIELD_NUMBER: builtins.int
    speech_model_variant: global___SpeechModelVariant.ValueType = ...
    """The speech model used in speech to text.
    `SPEECH_MODEL_VARIANT_UNSPECIFIED`, `USE_BEST_AVAILABLE` will be treated as
    `USE_ENHANCED`. It can be overridden in [AnalyzeContentRequest][google.cloud.dialogflow.v2beta1.AnalyzeContentRequest] and
    [StreamingAnalyzeContentRequest][google.cloud.dialogflow.v2beta1.StreamingAnalyzeContentRequest] request.
    If enhanced model variant is specified and an enhanced
    version of the specified model for the language does not exist, then it
    would emit an error.
    """

    def __init__(self,
        *,
        speech_model_variant : global___SpeechModelVariant.ValueType = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["speech_model_variant",b"speech_model_variant"]) -> None: ...
global___SpeechToTextConfig = SpeechToTextConfig
