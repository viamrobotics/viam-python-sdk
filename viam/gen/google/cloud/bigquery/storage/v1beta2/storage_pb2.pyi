"""
@generated by mypy-protobuf.  Do not edit manually!
isort:skip_file
"""
import builtins
import google.cloud.bigquery.storage.v1beta2.arrow_pb2
import google.cloud.bigquery.storage.v1beta2.avro_pb2
import google.cloud.bigquery.storage.v1beta2.protobuf_pb2
import google.cloud.bigquery.storage.v1beta2.stream_pb2
import google.cloud.bigquery.storage.v1beta2.table_pb2
import google.protobuf.descriptor
import google.protobuf.internal.containers
import google.protobuf.internal.enum_type_wrapper
import google.protobuf.message
import google.protobuf.timestamp_pb2
import google.protobuf.wrappers_pb2
import google.rpc.status_pb2
import typing
import typing_extensions

DESCRIPTOR: google.protobuf.descriptor.FileDescriptor = ...

class CreateReadSessionRequest(google.protobuf.message.Message):
    """Request message for `CreateReadSession`."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    PARENT_FIELD_NUMBER: builtins.int
    READ_SESSION_FIELD_NUMBER: builtins.int
    MAX_STREAM_COUNT_FIELD_NUMBER: builtins.int
    parent: typing.Text = ...
    """Required. The request project that owns the session, in the form of
    `projects/{project_id}`.
    """

    @property
    def read_session(self) -> google.cloud.bigquery.storage.v1beta2.stream_pb2.ReadSession:
        """Required. Session to be created."""
        pass
    max_stream_count: builtins.int = ...
    """Max initial number of streams. If unset or zero, the server will
    provide a value of streams so as to produce reasonable throughput. Must be
    non-negative. The number of streams may be lower than the requested number,
    depending on the amount parallelism that is reasonable for the table. Error
    will be returned if the max count is greater than the current system
    max limit of 1,000.

    Streams must be read starting from offset 0.
    """

    def __init__(self,
        *,
        parent : typing.Text = ...,
        read_session : typing.Optional[google.cloud.bigquery.storage.v1beta2.stream_pb2.ReadSession] = ...,
        max_stream_count : builtins.int = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["read_session",b"read_session"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["max_stream_count",b"max_stream_count","parent",b"parent","read_session",b"read_session"]) -> None: ...
global___CreateReadSessionRequest = CreateReadSessionRequest

class ReadRowsRequest(google.protobuf.message.Message):
    """Request message for `ReadRows`."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    READ_STREAM_FIELD_NUMBER: builtins.int
    OFFSET_FIELD_NUMBER: builtins.int
    read_stream: typing.Text = ...
    """Required. Stream to read rows from."""

    offset: builtins.int = ...
    """The offset requested must be less than the last row read from Read.
    Requesting a larger offset is undefined. If not specified, start reading
    from offset zero.
    """

    def __init__(self,
        *,
        read_stream : typing.Text = ...,
        offset : builtins.int = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["offset",b"offset","read_stream",b"read_stream"]) -> None: ...
global___ReadRowsRequest = ReadRowsRequest

class ThrottleState(google.protobuf.message.Message):
    """Information on if the current connection is being throttled."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    THROTTLE_PERCENT_FIELD_NUMBER: builtins.int
    throttle_percent: builtins.int = ...
    """How much this connection is being throttled. Zero means no throttling,
    100 means fully throttled.
    """

    def __init__(self,
        *,
        throttle_percent : builtins.int = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["throttle_percent",b"throttle_percent"]) -> None: ...
global___ThrottleState = ThrottleState

class StreamStats(google.protobuf.message.Message):
    """Estimated stream statistics for a given Stream."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class Progress(google.protobuf.message.Message):
        DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
        AT_RESPONSE_START_FIELD_NUMBER: builtins.int
        AT_RESPONSE_END_FIELD_NUMBER: builtins.int
        at_response_start: builtins.float = ...
        """The fraction of rows assigned to the stream that have been processed by
        the server so far, not including the rows in the current response
        message.

        This value, along with `at_response_end`, can be used to interpolate
        the progress made as the rows in the message are being processed using
        the following formula: `at_response_start + (at_response_end -
        at_response_start) * rows_processed_from_response / rows_in_response`.

        Note that if a filter is provided, the `at_response_end` value of the
        previous response may not necessarily be equal to the
        `at_response_start` value of the current response.
        """

        at_response_end: builtins.float = ...
        """Similar to `at_response_start`, except that this value includes the
        rows in the current response.
        """

        def __init__(self,
            *,
            at_response_start : builtins.float = ...,
            at_response_end : builtins.float = ...,
            ) -> None: ...
        def ClearField(self, field_name: typing_extensions.Literal["at_response_end",b"at_response_end","at_response_start",b"at_response_start"]) -> None: ...

    PROGRESS_FIELD_NUMBER: builtins.int
    @property
    def progress(self) -> global___StreamStats.Progress:
        """Represents the progress of the current stream."""
        pass
    def __init__(self,
        *,
        progress : typing.Optional[global___StreamStats.Progress] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["progress",b"progress"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["progress",b"progress"]) -> None: ...
global___StreamStats = StreamStats

class ReadRowsResponse(google.protobuf.message.Message):
    """Response from calling `ReadRows` may include row data, progress and
    throttling information.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    AVRO_ROWS_FIELD_NUMBER: builtins.int
    ARROW_RECORD_BATCH_FIELD_NUMBER: builtins.int
    ROW_COUNT_FIELD_NUMBER: builtins.int
    STATS_FIELD_NUMBER: builtins.int
    THROTTLE_STATE_FIELD_NUMBER: builtins.int
    AVRO_SCHEMA_FIELD_NUMBER: builtins.int
    ARROW_SCHEMA_FIELD_NUMBER: builtins.int
    @property
    def avro_rows(self) -> google.cloud.bigquery.storage.v1beta2.avro_pb2.AvroRows:
        """Serialized row data in AVRO format."""
        pass
    @property
    def arrow_record_batch(self) -> google.cloud.bigquery.storage.v1beta2.arrow_pb2.ArrowRecordBatch:
        """Serialized row data in Arrow RecordBatch format."""
        pass
    row_count: builtins.int = ...
    """Number of serialized rows in the rows block."""

    @property
    def stats(self) -> global___StreamStats:
        """Statistics for the stream."""
        pass
    @property
    def throttle_state(self) -> global___ThrottleState:
        """Throttling state. If unset, the latest response still describes
        the current throttling status.
        """
        pass
    @property
    def avro_schema(self) -> google.cloud.bigquery.storage.v1beta2.avro_pb2.AvroSchema:
        """Output only. Avro schema."""
        pass
    @property
    def arrow_schema(self) -> google.cloud.bigquery.storage.v1beta2.arrow_pb2.ArrowSchema:
        """Output only. Arrow schema."""
        pass
    def __init__(self,
        *,
        avro_rows : typing.Optional[google.cloud.bigquery.storage.v1beta2.avro_pb2.AvroRows] = ...,
        arrow_record_batch : typing.Optional[google.cloud.bigquery.storage.v1beta2.arrow_pb2.ArrowRecordBatch] = ...,
        row_count : builtins.int = ...,
        stats : typing.Optional[global___StreamStats] = ...,
        throttle_state : typing.Optional[global___ThrottleState] = ...,
        avro_schema : typing.Optional[google.cloud.bigquery.storage.v1beta2.avro_pb2.AvroSchema] = ...,
        arrow_schema : typing.Optional[google.cloud.bigquery.storage.v1beta2.arrow_pb2.ArrowSchema] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["arrow_record_batch",b"arrow_record_batch","arrow_schema",b"arrow_schema","avro_rows",b"avro_rows","avro_schema",b"avro_schema","rows",b"rows","schema",b"schema","stats",b"stats","throttle_state",b"throttle_state"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["arrow_record_batch",b"arrow_record_batch","arrow_schema",b"arrow_schema","avro_rows",b"avro_rows","avro_schema",b"avro_schema","row_count",b"row_count","rows",b"rows","schema",b"schema","stats",b"stats","throttle_state",b"throttle_state"]) -> None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing_extensions.Literal["rows",b"rows"]) -> typing.Optional[typing_extensions.Literal["avro_rows","arrow_record_batch"]]: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing_extensions.Literal["schema",b"schema"]) -> typing.Optional[typing_extensions.Literal["avro_schema","arrow_schema"]]: ...
global___ReadRowsResponse = ReadRowsResponse

class SplitReadStreamRequest(google.protobuf.message.Message):
    """Request message for `SplitReadStream`."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    NAME_FIELD_NUMBER: builtins.int
    FRACTION_FIELD_NUMBER: builtins.int
    name: typing.Text = ...
    """Required. Name of the stream to split."""

    fraction: builtins.float = ...
    """A value in the range (0.0, 1.0) that specifies the fractional point at
    which the original stream should be split. The actual split point is
    evaluated on pre-filtered rows, so if a filter is provided, then there is
    no guarantee that the division of the rows between the new child streams
    will be proportional to this fractional value. Additionally, because the
    server-side unit for assigning data is collections of rows, this fraction
    will always map to a data storage boundary on the server side.
    """

    def __init__(self,
        *,
        name : typing.Text = ...,
        fraction : builtins.float = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["fraction",b"fraction","name",b"name"]) -> None: ...
global___SplitReadStreamRequest = SplitReadStreamRequest

class SplitReadStreamResponse(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    PRIMARY_STREAM_FIELD_NUMBER: builtins.int
    REMAINDER_STREAM_FIELD_NUMBER: builtins.int
    @property
    def primary_stream(self) -> google.cloud.bigquery.storage.v1beta2.stream_pb2.ReadStream:
        """Primary stream, which contains the beginning portion of
        |original_stream|. An empty value indicates that the original stream can no
        longer be split.
        """
        pass
    @property
    def remainder_stream(self) -> google.cloud.bigquery.storage.v1beta2.stream_pb2.ReadStream:
        """Remainder stream, which contains the tail of |original_stream|. An empty
        value indicates that the original stream can no longer be split.
        """
        pass
    def __init__(self,
        *,
        primary_stream : typing.Optional[google.cloud.bigquery.storage.v1beta2.stream_pb2.ReadStream] = ...,
        remainder_stream : typing.Optional[google.cloud.bigquery.storage.v1beta2.stream_pb2.ReadStream] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["primary_stream",b"primary_stream","remainder_stream",b"remainder_stream"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["primary_stream",b"primary_stream","remainder_stream",b"remainder_stream"]) -> None: ...
global___SplitReadStreamResponse = SplitReadStreamResponse

class CreateWriteStreamRequest(google.protobuf.message.Message):
    """Request message for `CreateWriteStream`."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    PARENT_FIELD_NUMBER: builtins.int
    WRITE_STREAM_FIELD_NUMBER: builtins.int
    parent: typing.Text = ...
    """Required. Reference to the table to which the stream belongs, in the format
    of `projects/{project}/datasets/{dataset}/tables/{table}`.
    """

    @property
    def write_stream(self) -> google.cloud.bigquery.storage.v1beta2.stream_pb2.WriteStream:
        """Required. Stream to be created."""
        pass
    def __init__(self,
        *,
        parent : typing.Text = ...,
        write_stream : typing.Optional[google.cloud.bigquery.storage.v1beta2.stream_pb2.WriteStream] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["write_stream",b"write_stream"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["parent",b"parent","write_stream",b"write_stream"]) -> None: ...
global___CreateWriteStreamRequest = CreateWriteStreamRequest

class AppendRowsRequest(google.protobuf.message.Message):
    """Request message for `AppendRows`."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class ProtoData(google.protobuf.message.Message):
        """Proto schema and data."""
        DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
        WRITER_SCHEMA_FIELD_NUMBER: builtins.int
        ROWS_FIELD_NUMBER: builtins.int
        @property
        def writer_schema(self) -> google.cloud.bigquery.storage.v1beta2.protobuf_pb2.ProtoSchema:
            """Proto schema used to serialize the data."""
            pass
        @property
        def rows(self) -> google.cloud.bigquery.storage.v1beta2.protobuf_pb2.ProtoRows:
            """Serialized row data in protobuf message format."""
            pass
        def __init__(self,
            *,
            writer_schema : typing.Optional[google.cloud.bigquery.storage.v1beta2.protobuf_pb2.ProtoSchema] = ...,
            rows : typing.Optional[google.cloud.bigquery.storage.v1beta2.protobuf_pb2.ProtoRows] = ...,
            ) -> None: ...
        def HasField(self, field_name: typing_extensions.Literal["rows",b"rows","writer_schema",b"writer_schema"]) -> builtins.bool: ...
        def ClearField(self, field_name: typing_extensions.Literal["rows",b"rows","writer_schema",b"writer_schema"]) -> None: ...

    WRITE_STREAM_FIELD_NUMBER: builtins.int
    OFFSET_FIELD_NUMBER: builtins.int
    PROTO_ROWS_FIELD_NUMBER: builtins.int
    TRACE_ID_FIELD_NUMBER: builtins.int
    write_stream: typing.Text = ...
    """Required. The stream that is the target of the append operation. This value must be
    specified for the initial request. If subsequent requests specify the
    stream name, it must equal to the value provided in the first request.
    To write to the _default stream, populate this field with a string in the
    format `projects/{project}/datasets/{dataset}/tables/{table}/_default`.
    """

    @property
    def offset(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """If present, the write is only performed if the next append offset is same
        as the provided value. If not present, the write is performed at the
        current end of stream. Specifying a value for this field is not allowed
        when calling AppendRows for the '_default' stream.
        """
        pass
    @property
    def proto_rows(self) -> global___AppendRowsRequest.ProtoData:
        """Rows in proto format."""
        pass
    trace_id: typing.Text = ...
    """Id set by client to annotate its identity. Only initial request setting is
    respected.
    """

    def __init__(self,
        *,
        write_stream : typing.Text = ...,
        offset : typing.Optional[google.protobuf.wrappers_pb2.Int64Value] = ...,
        proto_rows : typing.Optional[global___AppendRowsRequest.ProtoData] = ...,
        trace_id : typing.Text = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["offset",b"offset","proto_rows",b"proto_rows","rows",b"rows"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["offset",b"offset","proto_rows",b"proto_rows","rows",b"rows","trace_id",b"trace_id","write_stream",b"write_stream"]) -> None: ...
    def WhichOneof(self, oneof_group: typing_extensions.Literal["rows",b"rows"]) -> typing.Optional[typing_extensions.Literal["proto_rows"]]: ...
global___AppendRowsRequest = AppendRowsRequest

class AppendRowsResponse(google.protobuf.message.Message):
    """Response message for `AppendRows`."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class AppendResult(google.protobuf.message.Message):
        """AppendResult is returned for successful append requests."""
        DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
        OFFSET_FIELD_NUMBER: builtins.int
        @property
        def offset(self) -> google.protobuf.wrappers_pb2.Int64Value:
            """The row offset at which the last append occurred. The offset will not be
            set if appending using default streams.
            """
            pass
        def __init__(self,
            *,
            offset : typing.Optional[google.protobuf.wrappers_pb2.Int64Value] = ...,
            ) -> None: ...
        def HasField(self, field_name: typing_extensions.Literal["offset",b"offset"]) -> builtins.bool: ...
        def ClearField(self, field_name: typing_extensions.Literal["offset",b"offset"]) -> None: ...

    APPEND_RESULT_FIELD_NUMBER: builtins.int
    ERROR_FIELD_NUMBER: builtins.int
    UPDATED_SCHEMA_FIELD_NUMBER: builtins.int
    @property
    def append_result(self) -> global___AppendRowsResponse.AppendResult:
        """Result if the append is successful."""
        pass
    @property
    def error(self) -> google.rpc.status_pb2.Status:
        """Error returned when problems were encountered.  If present,
        it indicates rows were not accepted into the system.
        Users can retry or continue with other append requests within the
        same connection.

        Additional information about error signalling:

        ALREADY_EXISTS: Happens when an append specified an offset, and the
        backend already has received data at this offset.  Typically encountered
        in retry scenarios, and can be ignored.

        OUT_OF_RANGE: Returned when the specified offset in the stream is beyond
        the current end of the stream.

        INVALID_ARGUMENT: Indicates a malformed request or data.

        ABORTED: Request processing is aborted because of prior failures.  The
        request can be retried if previous failure is addressed.

        INTERNAL: Indicates server side error(s) that can be retried.
        """
        pass
    @property
    def updated_schema(self) -> google.cloud.bigquery.storage.v1beta2.table_pb2.TableSchema:
        """If backend detects a schema update, pass it to user so that user can
        use it to input new type of message. It will be empty when no schema
        updates have occurred.
        """
        pass
    def __init__(self,
        *,
        append_result : typing.Optional[global___AppendRowsResponse.AppendResult] = ...,
        error : typing.Optional[google.rpc.status_pb2.Status] = ...,
        updated_schema : typing.Optional[google.cloud.bigquery.storage.v1beta2.table_pb2.TableSchema] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["append_result",b"append_result","error",b"error","response",b"response","updated_schema",b"updated_schema"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["append_result",b"append_result","error",b"error","response",b"response","updated_schema",b"updated_schema"]) -> None: ...
    def WhichOneof(self, oneof_group: typing_extensions.Literal["response",b"response"]) -> typing.Optional[typing_extensions.Literal["append_result","error"]]: ...
global___AppendRowsResponse = AppendRowsResponse

class GetWriteStreamRequest(google.protobuf.message.Message):
    """Request message for `GetWriteStreamRequest`."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    NAME_FIELD_NUMBER: builtins.int
    name: typing.Text = ...
    """Required. Name of the stream to get, in the form of
    `projects/{project}/datasets/{dataset}/tables/{table}/streams/{stream}`.
    """

    def __init__(self,
        *,
        name : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["name",b"name"]) -> None: ...
global___GetWriteStreamRequest = GetWriteStreamRequest

class BatchCommitWriteStreamsRequest(google.protobuf.message.Message):
    """Request message for `BatchCommitWriteStreams`."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    PARENT_FIELD_NUMBER: builtins.int
    WRITE_STREAMS_FIELD_NUMBER: builtins.int
    parent: typing.Text = ...
    """Required. Parent table that all the streams should belong to, in the form of
    `projects/{project}/datasets/{dataset}/tables/{table}`.
    """

    @property
    def write_streams(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """Required. The group of streams that will be committed atomically."""
        pass
    def __init__(self,
        *,
        parent : typing.Text = ...,
        write_streams : typing.Optional[typing.Iterable[typing.Text]] = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["parent",b"parent","write_streams",b"write_streams"]) -> None: ...
global___BatchCommitWriteStreamsRequest = BatchCommitWriteStreamsRequest

class BatchCommitWriteStreamsResponse(google.protobuf.message.Message):
    """Response message for `BatchCommitWriteStreams`."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    COMMIT_TIME_FIELD_NUMBER: builtins.int
    STREAM_ERRORS_FIELD_NUMBER: builtins.int
    @property
    def commit_time(self) -> google.protobuf.timestamp_pb2.Timestamp:
        """The time at which streams were committed in microseconds granularity.
        This field will only exist when there are no stream errors.
        **Note** if this field is not set, it means the commit was not successful.
        """
        pass
    @property
    def stream_errors(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___StorageError]:
        """Stream level error if commit failed. Only streams with error will be in
        the list.
        If empty, there is no error and all streams are committed successfully.
        If non empty, certain streams have errors and ZERO stream is committed due
        to atomicity guarantee.
        """
        pass
    def __init__(self,
        *,
        commit_time : typing.Optional[google.protobuf.timestamp_pb2.Timestamp] = ...,
        stream_errors : typing.Optional[typing.Iterable[global___StorageError]] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["commit_time",b"commit_time"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["commit_time",b"commit_time","stream_errors",b"stream_errors"]) -> None: ...
global___BatchCommitWriteStreamsResponse = BatchCommitWriteStreamsResponse

class FinalizeWriteStreamRequest(google.protobuf.message.Message):
    """Request message for invoking `FinalizeWriteStream`."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    NAME_FIELD_NUMBER: builtins.int
    name: typing.Text = ...
    """Required. Name of the stream to finalize, in the form of
    `projects/{project}/datasets/{dataset}/tables/{table}/streams/{stream}`.
    """

    def __init__(self,
        *,
        name : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["name",b"name"]) -> None: ...
global___FinalizeWriteStreamRequest = FinalizeWriteStreamRequest

class FinalizeWriteStreamResponse(google.protobuf.message.Message):
    """Response message for `FinalizeWriteStream`."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    ROW_COUNT_FIELD_NUMBER: builtins.int
    row_count: builtins.int = ...
    """Number of rows in the finalized stream."""

    def __init__(self,
        *,
        row_count : builtins.int = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["row_count",b"row_count"]) -> None: ...
global___FinalizeWriteStreamResponse = FinalizeWriteStreamResponse

class FlushRowsRequest(google.protobuf.message.Message):
    """Request message for `FlushRows`."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    WRITE_STREAM_FIELD_NUMBER: builtins.int
    OFFSET_FIELD_NUMBER: builtins.int
    write_stream: typing.Text = ...
    """Required. The stream that is the target of the flush operation."""

    @property
    def offset(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Ending offset of the flush operation. Rows before this offset(including
        this offset) will be flushed.
        """
        pass
    def __init__(self,
        *,
        write_stream : typing.Text = ...,
        offset : typing.Optional[google.protobuf.wrappers_pb2.Int64Value] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["offset",b"offset"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["offset",b"offset","write_stream",b"write_stream"]) -> None: ...
global___FlushRowsRequest = FlushRowsRequest

class FlushRowsResponse(google.protobuf.message.Message):
    """Respond message for `FlushRows`."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    OFFSET_FIELD_NUMBER: builtins.int
    offset: builtins.int = ...
    """The rows before this offset (including this offset) are flushed."""

    def __init__(self,
        *,
        offset : builtins.int = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["offset",b"offset"]) -> None: ...
global___FlushRowsResponse = FlushRowsResponse

class StorageError(google.protobuf.message.Message):
    """Structured custom BigQuery Storage error message. The error can be attached
    as error details in the returned rpc Status. In particular, the use of error
    codes allows more structured error handling, and reduces the need to evaluate
    unstructured error text strings.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class _StorageErrorCode:
        ValueType = typing.NewType('ValueType', builtins.int)
        V: typing_extensions.TypeAlias = ValueType
    class _StorageErrorCodeEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_StorageErrorCode.ValueType], builtins.type):
        DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor = ...
        STORAGE_ERROR_CODE_UNSPECIFIED: StorageError.StorageErrorCode.ValueType = ...  # 0
        """Default error."""

        TABLE_NOT_FOUND: StorageError.StorageErrorCode.ValueType = ...  # 1
        """Table is not found in the system."""

        STREAM_ALREADY_COMMITTED: StorageError.StorageErrorCode.ValueType = ...  # 2
        """Stream is already committed."""

        STREAM_NOT_FOUND: StorageError.StorageErrorCode.ValueType = ...  # 3
        """Stream is not found."""

        INVALID_STREAM_TYPE: StorageError.StorageErrorCode.ValueType = ...  # 4
        """Invalid Stream type.
        For example, you try to commit a stream that is not pending.
        """

        INVALID_STREAM_STATE: StorageError.StorageErrorCode.ValueType = ...  # 5
        """Invalid Stream state.
        For example, you try to commit a stream that is not finalized or is
        garbaged.
        """

        STREAM_FINALIZED: StorageError.StorageErrorCode.ValueType = ...  # 6
        """Stream is finalized."""

    class StorageErrorCode(_StorageErrorCode, metaclass=_StorageErrorCodeEnumTypeWrapper):
        """Error code for `StorageError`."""
        pass

    STORAGE_ERROR_CODE_UNSPECIFIED: StorageError.StorageErrorCode.ValueType = ...  # 0
    """Default error."""

    TABLE_NOT_FOUND: StorageError.StorageErrorCode.ValueType = ...  # 1
    """Table is not found in the system."""

    STREAM_ALREADY_COMMITTED: StorageError.StorageErrorCode.ValueType = ...  # 2
    """Stream is already committed."""

    STREAM_NOT_FOUND: StorageError.StorageErrorCode.ValueType = ...  # 3
    """Stream is not found."""

    INVALID_STREAM_TYPE: StorageError.StorageErrorCode.ValueType = ...  # 4
    """Invalid Stream type.
    For example, you try to commit a stream that is not pending.
    """

    INVALID_STREAM_STATE: StorageError.StorageErrorCode.ValueType = ...  # 5
    """Invalid Stream state.
    For example, you try to commit a stream that is not finalized or is
    garbaged.
    """

    STREAM_FINALIZED: StorageError.StorageErrorCode.ValueType = ...  # 6
    """Stream is finalized."""


    CODE_FIELD_NUMBER: builtins.int
    ENTITY_FIELD_NUMBER: builtins.int
    ERROR_MESSAGE_FIELD_NUMBER: builtins.int
    code: global___StorageError.StorageErrorCode.ValueType = ...
    """BigQuery Storage specific error code."""

    entity: typing.Text = ...
    """Name of the failed entity."""

    error_message: typing.Text = ...
    """Message that describes the error."""

    def __init__(self,
        *,
        code : global___StorageError.StorageErrorCode.ValueType = ...,
        entity : typing.Text = ...,
        error_message : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["code",b"code","entity",b"entity","error_message",b"error_message"]) -> None: ...
global___StorageError = StorageError
