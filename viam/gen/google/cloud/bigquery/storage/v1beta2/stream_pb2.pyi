"""
@generated by mypy-protobuf.  Do not edit manually!
isort:skip_file
"""
import builtins
import google.cloud.bigquery.storage.v1beta2.arrow_pb2
import google.cloud.bigquery.storage.v1beta2.avro_pb2
import google.cloud.bigquery.storage.v1beta2.table_pb2
import google.protobuf.descriptor
import google.protobuf.internal.containers
import google.protobuf.internal.enum_type_wrapper
import google.protobuf.message
import google.protobuf.timestamp_pb2
import typing
import typing_extensions

DESCRIPTOR: google.protobuf.descriptor.FileDescriptor = ...

class _DataFormat:
    ValueType = typing.NewType('ValueType', builtins.int)
    V: typing_extensions.TypeAlias = ValueType
class _DataFormatEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_DataFormat.ValueType], builtins.type):
    DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor = ...
    DATA_FORMAT_UNSPECIFIED: DataFormat.ValueType = ...  # 0
    AVRO: DataFormat.ValueType = ...  # 1
    """Avro is a standard open source row based file format.
    See https://avro.apache.org/ for more details.
    """

    ARROW: DataFormat.ValueType = ...  # 2
    """Arrow is a standard open source column-based message format.
    See https://arrow.apache.org/ for more details.
    """

class DataFormat(_DataFormat, metaclass=_DataFormatEnumTypeWrapper):
    """Data format for input or output data."""
    pass

DATA_FORMAT_UNSPECIFIED: DataFormat.ValueType = ...  # 0
AVRO: DataFormat.ValueType = ...  # 1
"""Avro is a standard open source row based file format.
See https://avro.apache.org/ for more details.
"""

ARROW: DataFormat.ValueType = ...  # 2
"""Arrow is a standard open source column-based message format.
See https://arrow.apache.org/ for more details.
"""

global___DataFormat = DataFormat


class ReadSession(google.protobuf.message.Message):
    """Information about the ReadSession."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class TableModifiers(google.protobuf.message.Message):
        """Additional attributes when reading a table."""
        DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
        SNAPSHOT_TIME_FIELD_NUMBER: builtins.int
        @property
        def snapshot_time(self) -> google.protobuf.timestamp_pb2.Timestamp:
            """The snapshot time of the table. If not set, interpreted as now."""
            pass
        def __init__(self,
            *,
            snapshot_time : typing.Optional[google.protobuf.timestamp_pb2.Timestamp] = ...,
            ) -> None: ...
        def HasField(self, field_name: typing_extensions.Literal["snapshot_time",b"snapshot_time"]) -> builtins.bool: ...
        def ClearField(self, field_name: typing_extensions.Literal["snapshot_time",b"snapshot_time"]) -> None: ...

    class TableReadOptions(google.protobuf.message.Message):
        """Options dictating how we read a table."""
        DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
        SELECTED_FIELDS_FIELD_NUMBER: builtins.int
        ROW_RESTRICTION_FIELD_NUMBER: builtins.int
        ARROW_SERIALIZATION_OPTIONS_FIELD_NUMBER: builtins.int
        @property
        def selected_fields(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
            """Names of the fields in the table that should be read. If empty, all
            fields will be read. If the specified field is a nested field, all
            the sub-fields in the field will be selected. The output field order is
            unrelated to the order of fields in selected_fields.
            """
            pass
        row_restriction: typing.Text = ...
        """SQL text filtering statement, similar to a WHERE clause in a query.
        Aggregates are not supported.

        Examples: "int_field > 5"
                  "date_field = CAST('2014-9-27' as DATE)"
                  "nullable_field is not NULL"
                  "st_equals(geo_field, st_geofromtext("POINT(2, 2)"))"
                  "numeric_field BETWEEN 1.0 AND 5.0"

        Restricted to a maximum length for 1 MB.
        """

        @property
        def arrow_serialization_options(self) -> google.cloud.bigquery.storage.v1beta2.arrow_pb2.ArrowSerializationOptions:
            """Optional. Options specific to the Apache Arrow output format."""
            pass
        def __init__(self,
            *,
            selected_fields : typing.Optional[typing.Iterable[typing.Text]] = ...,
            row_restriction : typing.Text = ...,
            arrow_serialization_options : typing.Optional[google.cloud.bigquery.storage.v1beta2.arrow_pb2.ArrowSerializationOptions] = ...,
            ) -> None: ...
        def HasField(self, field_name: typing_extensions.Literal["arrow_serialization_options",b"arrow_serialization_options"]) -> builtins.bool: ...
        def ClearField(self, field_name: typing_extensions.Literal["arrow_serialization_options",b"arrow_serialization_options","row_restriction",b"row_restriction","selected_fields",b"selected_fields"]) -> None: ...

    NAME_FIELD_NUMBER: builtins.int
    EXPIRE_TIME_FIELD_NUMBER: builtins.int
    DATA_FORMAT_FIELD_NUMBER: builtins.int
    AVRO_SCHEMA_FIELD_NUMBER: builtins.int
    ARROW_SCHEMA_FIELD_NUMBER: builtins.int
    TABLE_FIELD_NUMBER: builtins.int
    TABLE_MODIFIERS_FIELD_NUMBER: builtins.int
    READ_OPTIONS_FIELD_NUMBER: builtins.int
    STREAMS_FIELD_NUMBER: builtins.int
    name: typing.Text = ...
    """Output only. Unique identifier for the session, in the form
    `projects/{project_id}/locations/{location}/sessions/{session_id}`.
    """

    @property
    def expire_time(self) -> google.protobuf.timestamp_pb2.Timestamp:
        """Output only. Time at which the session becomes invalid. After this time, subsequent
        requests to read this Session will return errors. The expire_time is
        automatically assigned and currently cannot be specified or updated.
        """
        pass
    data_format: global___DataFormat.ValueType = ...
    """Immutable. Data format of the output data."""

    @property
    def avro_schema(self) -> google.cloud.bigquery.storage.v1beta2.avro_pb2.AvroSchema:
        """Output only. Avro schema."""
        pass
    @property
    def arrow_schema(self) -> google.cloud.bigquery.storage.v1beta2.arrow_pb2.ArrowSchema:
        """Output only. Arrow schema."""
        pass
    table: typing.Text = ...
    """Immutable. Table that this ReadSession is reading from, in the form
    `projects/{project_id}/datasets/{dataset_id}/tables/{table_id}
    """

    @property
    def table_modifiers(self) -> global___ReadSession.TableModifiers:
        """Optional. Any modifiers which are applied when reading from the specified table."""
        pass
    @property
    def read_options(self) -> global___ReadSession.TableReadOptions:
        """Optional. Read options for this session (e.g. column selection, filters)."""
        pass
    @property
    def streams(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___ReadStream]:
        """Output only. A list of streams created with the session.

        At least one stream is created with the session. In the future, larger
        request_stream_count values *may* result in this list being unpopulated,
        in that case, the user will need to use a List method to get the streams
        instead, which is not yet available.
        """
        pass
    def __init__(self,
        *,
        name : typing.Text = ...,
        expire_time : typing.Optional[google.protobuf.timestamp_pb2.Timestamp] = ...,
        data_format : global___DataFormat.ValueType = ...,
        avro_schema : typing.Optional[google.cloud.bigquery.storage.v1beta2.avro_pb2.AvroSchema] = ...,
        arrow_schema : typing.Optional[google.cloud.bigquery.storage.v1beta2.arrow_pb2.ArrowSchema] = ...,
        table : typing.Text = ...,
        table_modifiers : typing.Optional[global___ReadSession.TableModifiers] = ...,
        read_options : typing.Optional[global___ReadSession.TableReadOptions] = ...,
        streams : typing.Optional[typing.Iterable[global___ReadStream]] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["arrow_schema",b"arrow_schema","avro_schema",b"avro_schema","expire_time",b"expire_time","read_options",b"read_options","schema",b"schema","table_modifiers",b"table_modifiers"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["arrow_schema",b"arrow_schema","avro_schema",b"avro_schema","data_format",b"data_format","expire_time",b"expire_time","name",b"name","read_options",b"read_options","schema",b"schema","streams",b"streams","table",b"table","table_modifiers",b"table_modifiers"]) -> None: ...
    def WhichOneof(self, oneof_group: typing_extensions.Literal["schema",b"schema"]) -> typing.Optional[typing_extensions.Literal["avro_schema","arrow_schema"]]: ...
global___ReadSession = ReadSession

class ReadStream(google.protobuf.message.Message):
    """Information about a single stream that gets data out of the storage system.
    Most of the information about `ReadStream` instances is aggregated, making
    `ReadStream` lightweight.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    NAME_FIELD_NUMBER: builtins.int
    name: typing.Text = ...
    """Output only. Name of the stream, in the form
    `projects/{project_id}/locations/{location}/sessions/{session_id}/streams/{stream_id}`.
    """

    def __init__(self,
        *,
        name : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["name",b"name"]) -> None: ...
global___ReadStream = ReadStream

class WriteStream(google.protobuf.message.Message):
    """Information about a single stream that gets data inside the storage system."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class _Type:
        ValueType = typing.NewType('ValueType', builtins.int)
        V: typing_extensions.TypeAlias = ValueType
    class _TypeEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_Type.ValueType], builtins.type):
        DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor = ...
        TYPE_UNSPECIFIED: WriteStream.Type.ValueType = ...  # 0
        """Unknown type."""

        COMMITTED: WriteStream.Type.ValueType = ...  # 1
        """Data will commit automatically and appear as soon as the write is
        acknowledged.
        """

        PENDING: WriteStream.Type.ValueType = ...  # 2
        """Data is invisible until the stream is committed."""

        BUFFERED: WriteStream.Type.ValueType = ...  # 3
        """Data is only visible up to the offset to which it was flushed."""

    class Type(_Type, metaclass=_TypeEnumTypeWrapper):
        """Type enum of the stream."""
        pass

    TYPE_UNSPECIFIED: WriteStream.Type.ValueType = ...  # 0
    """Unknown type."""

    COMMITTED: WriteStream.Type.ValueType = ...  # 1
    """Data will commit automatically and appear as soon as the write is
    acknowledged.
    """

    PENDING: WriteStream.Type.ValueType = ...  # 2
    """Data is invisible until the stream is committed."""

    BUFFERED: WriteStream.Type.ValueType = ...  # 3
    """Data is only visible up to the offset to which it was flushed."""


    NAME_FIELD_NUMBER: builtins.int
    TYPE_FIELD_NUMBER: builtins.int
    CREATE_TIME_FIELD_NUMBER: builtins.int
    COMMIT_TIME_FIELD_NUMBER: builtins.int
    TABLE_SCHEMA_FIELD_NUMBER: builtins.int
    name: typing.Text = ...
    """Output only. Name of the stream, in the form
    `projects/{project}/datasets/{dataset}/tables/{table}/streams/{stream}`.
    """

    type: global___WriteStream.Type.ValueType = ...
    """Immutable. Type of the stream."""

    @property
    def create_time(self) -> google.protobuf.timestamp_pb2.Timestamp:
        """Output only. Create time of the stream. For the _default stream, this is the
        creation_time of the table.
        """
        pass
    @property
    def commit_time(self) -> google.protobuf.timestamp_pb2.Timestamp:
        """Output only. Commit time of the stream.
        If a stream is of `COMMITTED` type, then it will have a commit_time same as
        `create_time`. If the stream is of `PENDING` type, commit_time being empty
        means it is not committed.
        """
        pass
    @property
    def table_schema(self) -> google.cloud.bigquery.storage.v1beta2.table_pb2.TableSchema:
        """Output only. The schema of the destination table. It is only returned in
        `CreateWriteStream` response. Caller should generate data that's
        compatible with this schema to send in initial `AppendRowsRequest`.
        The table schema could go out of date during the life time of the stream.
        """
        pass
    def __init__(self,
        *,
        name : typing.Text = ...,
        type : global___WriteStream.Type.ValueType = ...,
        create_time : typing.Optional[google.protobuf.timestamp_pb2.Timestamp] = ...,
        commit_time : typing.Optional[google.protobuf.timestamp_pb2.Timestamp] = ...,
        table_schema : typing.Optional[google.cloud.bigquery.storage.v1beta2.table_pb2.TableSchema] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["commit_time",b"commit_time","create_time",b"create_time","table_schema",b"table_schema"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["commit_time",b"commit_time","create_time",b"create_time","name",b"name","table_schema",b"table_schema","type",b"type"]) -> None: ...
global___WriteStream = WriteStream
