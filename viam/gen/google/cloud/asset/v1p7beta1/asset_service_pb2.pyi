"""
@generated by mypy-protobuf.  Do not edit manually!
isort:skip_file
"""
import builtins
import google.protobuf.descriptor
import google.protobuf.internal.containers
import google.protobuf.internal.enum_type_wrapper
import google.protobuf.message
import google.protobuf.timestamp_pb2
import typing
import typing_extensions

DESCRIPTOR: google.protobuf.descriptor.FileDescriptor = ...

class _ContentType:
    ValueType = typing.NewType('ValueType', builtins.int)
    V: typing_extensions.TypeAlias = ValueType
class _ContentTypeEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_ContentType.ValueType], builtins.type):
    DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor = ...
    CONTENT_TYPE_UNSPECIFIED: ContentType.ValueType = ...  # 0
    """Unspecified content type."""

    RESOURCE: ContentType.ValueType = ...  # 1
    """Resource metadata."""

    IAM_POLICY: ContentType.ValueType = ...  # 2
    """The actual IAM policy set on a resource."""

    ORG_POLICY: ContentType.ValueType = ...  # 4
    """The Cloud Organization Policy set on an asset."""

    ACCESS_POLICY: ContentType.ValueType = ...  # 5
    """The Cloud Access context manager Policy set on an asset."""

    RELATIONSHIP: ContentType.ValueType = ...  # 7
    """The related resources."""

class ContentType(_ContentType, metaclass=_ContentTypeEnumTypeWrapper):
    """Asset content type."""
    pass

CONTENT_TYPE_UNSPECIFIED: ContentType.ValueType = ...  # 0
"""Unspecified content type."""

RESOURCE: ContentType.ValueType = ...  # 1
"""Resource metadata."""

IAM_POLICY: ContentType.ValueType = ...  # 2
"""The actual IAM policy set on a resource."""

ORG_POLICY: ContentType.ValueType = ...  # 4
"""The Cloud Organization Policy set on an asset."""

ACCESS_POLICY: ContentType.ValueType = ...  # 5
"""The Cloud Access context manager Policy set on an asset."""

RELATIONSHIP: ContentType.ValueType = ...  # 7
"""The related resources."""

global___ContentType = ContentType


class ExportAssetsRequest(google.protobuf.message.Message):
    """Export asset request."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    PARENT_FIELD_NUMBER: builtins.int
    READ_TIME_FIELD_NUMBER: builtins.int
    ASSET_TYPES_FIELD_NUMBER: builtins.int
    CONTENT_TYPE_FIELD_NUMBER: builtins.int
    OUTPUT_CONFIG_FIELD_NUMBER: builtins.int
    RELATIONSHIP_TYPES_FIELD_NUMBER: builtins.int
    parent: typing.Text = ...
    """Required. The relative name of the root asset. This can only be an
    organization number (such as "organizations/123"), a project ID (such as
    "projects/my-project-id"), or a project number (such as "projects/12345"),
    or a folder number (such as "folders/123").
    """

    @property
    def read_time(self) -> google.protobuf.timestamp_pb2.Timestamp:
        """Timestamp to take an asset snapshot. This can only be set to a timestamp
        between the current time and the current time minus 35 days (inclusive).
        If not specified, the current time will be used. Due to delays in resource
        data collection and indexing, there is a volatile window during which
        running the same query may get different results.
        """
        pass
    @property
    def asset_types(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """A list of asset types to take a snapshot for. For example:
        "compute.googleapis.com/Disk".

        Regular expressions are also supported. For example:

        * "compute.googleapis.com.*" snapshots resources whose asset type starts
        with "compute.googleapis.com".
        * ".*Instance" snapshots resources whose asset type ends with "Instance".
        * ".*Instance.*" snapshots resources whose asset type contains "Instance".

        See [RE2](https://github.com/google/re2/wiki/Syntax) for all supported
        regular expression syntax. If the regular expression does not match any
        supported asset type, an INVALID_ARGUMENT error will be returned.

        If specified, only matching assets will be returned, otherwise, it will
        snapshot all asset types. See [Introduction to Cloud Asset
        Inventory](https://cloud.google.com/asset-inventory/docs/overview)
        for all supported asset types.
        """
        pass
    content_type: global___ContentType.ValueType = ...
    """Asset content type. If not specified, no content but the asset name will be
    returned.
    """

    @property
    def output_config(self) -> global___OutputConfig:
        """Required. Output configuration indicating where the results will be output
        to.
        """
        pass
    @property
    def relationship_types(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """A list of relationship types to export, for example:
        `INSTANCE_TO_INSTANCEGROUP`. This field should only be specified if
        content_type=RELATIONSHIP. If specified, it will snapshot [asset_types]'
        specified relationships, or give errors if any relationship_types'
        supported types are not in [asset_types]. If not specified, it will
        snapshot all [asset_types]' supported relationships. An unspecified
        [asset_types] field means all supported asset_types. See [Introduction to
        Cloud Asset
        Inventory](https://cloud.google.com/asset-inventory/docs/overview) for all
        supported asset types and relationship types.
        """
        pass
    def __init__(self,
        *,
        parent : typing.Text = ...,
        read_time : typing.Optional[google.protobuf.timestamp_pb2.Timestamp] = ...,
        asset_types : typing.Optional[typing.Iterable[typing.Text]] = ...,
        content_type : global___ContentType.ValueType = ...,
        output_config : typing.Optional[global___OutputConfig] = ...,
        relationship_types : typing.Optional[typing.Iterable[typing.Text]] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["output_config",b"output_config","read_time",b"read_time"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["asset_types",b"asset_types","content_type",b"content_type","output_config",b"output_config","parent",b"parent","read_time",b"read_time","relationship_types",b"relationship_types"]) -> None: ...
global___ExportAssetsRequest = ExportAssetsRequest

class ExportAssetsResponse(google.protobuf.message.Message):
    """The export asset response. This message is returned by the
    [google.longrunning.Operations.GetOperation][google.longrunning.Operations.GetOperation]
    method in the returned
    [google.longrunning.Operation.response][google.longrunning.Operation.response]
    field.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    READ_TIME_FIELD_NUMBER: builtins.int
    OUTPUT_CONFIG_FIELD_NUMBER: builtins.int
    OUTPUT_RESULT_FIELD_NUMBER: builtins.int
    @property
    def read_time(self) -> google.protobuf.timestamp_pb2.Timestamp:
        """Time the snapshot was taken."""
        pass
    @property
    def output_config(self) -> global___OutputConfig:
        """Output configuration indicating where the results were output to."""
        pass
    @property
    def output_result(self) -> global___OutputResult:
        """Output result indicating where the assets were exported to. For example, a
        set of actual Google Cloud Storage object uris where the assets are
        exported to. The uris can be different from what [output_config] has
        specified, as the service will split the output object into multiple ones
        once it exceeds a single Google Cloud Storage object limit.
        """
        pass
    def __init__(self,
        *,
        read_time : typing.Optional[google.protobuf.timestamp_pb2.Timestamp] = ...,
        output_config : typing.Optional[global___OutputConfig] = ...,
        output_result : typing.Optional[global___OutputResult] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["output_config",b"output_config","output_result",b"output_result","read_time",b"read_time"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["output_config",b"output_config","output_result",b"output_result","read_time",b"read_time"]) -> None: ...
global___ExportAssetsResponse = ExportAssetsResponse

class OutputConfig(google.protobuf.message.Message):
    """Output configuration for export assets destination."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    GCS_DESTINATION_FIELD_NUMBER: builtins.int
    BIGQUERY_DESTINATION_FIELD_NUMBER: builtins.int
    @property
    def gcs_destination(self) -> global___GcsDestination:
        """Destination on Cloud Storage."""
        pass
    @property
    def bigquery_destination(self) -> global___BigQueryDestination:
        """Destination on BigQuery. The output table stores the fields in asset
        proto as columns in BigQuery.
        """
        pass
    def __init__(self,
        *,
        gcs_destination : typing.Optional[global___GcsDestination] = ...,
        bigquery_destination : typing.Optional[global___BigQueryDestination] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["bigquery_destination",b"bigquery_destination","destination",b"destination","gcs_destination",b"gcs_destination"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["bigquery_destination",b"bigquery_destination","destination",b"destination","gcs_destination",b"gcs_destination"]) -> None: ...
    def WhichOneof(self, oneof_group: typing_extensions.Literal["destination",b"destination"]) -> typing.Optional[typing_extensions.Literal["gcs_destination","bigquery_destination"]]: ...
global___OutputConfig = OutputConfig

class OutputResult(google.protobuf.message.Message):
    """Output result of export assets."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    GCS_RESULT_FIELD_NUMBER: builtins.int
    @property
    def gcs_result(self) -> global___GcsOutputResult:
        """Export result on Cloud Storage."""
        pass
    def __init__(self,
        *,
        gcs_result : typing.Optional[global___GcsOutputResult] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["gcs_result",b"gcs_result","result",b"result"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["gcs_result",b"gcs_result","result",b"result"]) -> None: ...
    def WhichOneof(self, oneof_group: typing_extensions.Literal["result",b"result"]) -> typing.Optional[typing_extensions.Literal["gcs_result"]]: ...
global___OutputResult = OutputResult

class GcsOutputResult(google.protobuf.message.Message):
    """A Cloud Storage output result."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    URIS_FIELD_NUMBER: builtins.int
    @property
    def uris(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """List of uris of the Cloud Storage objects. Example:
        "gs://bucket_name/object_name".
        """
        pass
    def __init__(self,
        *,
        uris : typing.Optional[typing.Iterable[typing.Text]] = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["uris",b"uris"]) -> None: ...
global___GcsOutputResult = GcsOutputResult

class GcsDestination(google.protobuf.message.Message):
    """A Cloud Storage location."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    URI_FIELD_NUMBER: builtins.int
    URI_PREFIX_FIELD_NUMBER: builtins.int
    uri: typing.Text = ...
    """The uri of the Cloud Storage object. It's the same uri that is used by
    gsutil. Example: "gs://bucket_name/object_name". See [Viewing and
    Editing Object
    Metadata](https://cloud.google.com/storage/docs/viewing-editing-metadata)
    for more information.
    """

    uri_prefix: typing.Text = ...
    """The uri prefix of all generated Cloud Storage objects. Example:
    "gs://bucket_name/object_name_prefix". Each object uri is in format:
    "gs://bucket_name/object_name_prefix/{ASSET_TYPE}/{SHARD_NUMBER} and only
    contains assets for that type. <shard number> starts from 0. Example:
    "gs://bucket_name/object_name_prefix/compute.googleapis.com/Disk/0" is
    the first shard of output objects containing all
    compute.googleapis.com/Disk assets. An INVALID_ARGUMENT error will be
    returned if file with the same name "gs://bucket_name/object_name_prefix"
    already exists.
    """

    def __init__(self,
        *,
        uri : typing.Text = ...,
        uri_prefix : typing.Text = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["object_uri",b"object_uri","uri",b"uri","uri_prefix",b"uri_prefix"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["object_uri",b"object_uri","uri",b"uri","uri_prefix",b"uri_prefix"]) -> None: ...
    def WhichOneof(self, oneof_group: typing_extensions.Literal["object_uri",b"object_uri"]) -> typing.Optional[typing_extensions.Literal["uri","uri_prefix"]]: ...
global___GcsDestination = GcsDestination

class BigQueryDestination(google.protobuf.message.Message):
    """A BigQuery destination for exporting assets to."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    DATASET_FIELD_NUMBER: builtins.int
    TABLE_FIELD_NUMBER: builtins.int
    FORCE_FIELD_NUMBER: builtins.int
    PARTITION_SPEC_FIELD_NUMBER: builtins.int
    SEPARATE_TABLES_PER_ASSET_TYPE_FIELD_NUMBER: builtins.int
    dataset: typing.Text = ...
    """Required. The BigQuery dataset in format
    "projects/projectId/datasets/datasetId", to which the snapshot result
    should be exported. If this dataset does not exist, the export call returns
    an INVALID_ARGUMENT error.
    """

    table: typing.Text = ...
    """Required. The BigQuery table to which the snapshot result should be
    written. If this table does not exist, a new table with the given name
    will be created.
    """

    force: builtins.bool = ...
    """If the destination table already exists and this flag is `TRUE`, the
    table will be overwritten by the contents of assets snapshot. If the flag
    is `FALSE` or unset and the destination table already exists, the export
    call returns an INVALID_ARGUMEMT error.
    """

    @property
    def partition_spec(self) -> global___PartitionSpec:
        """[partition_spec] determines whether to export to partitioned table(s) and
        how to partition the data.

        If [partition_spec] is unset or [partition_spec.partition_key] is unset or
        `PARTITION_KEY_UNSPECIFIED`, the snapshot results will be exported to
        non-partitioned table(s). [force] will decide whether to overwrite existing
        table(s).

        If [partition_spec] is specified. First, the snapshot results will be
        written to partitioned table(s) with two additional timestamp columns,
        readTime and requestTime, one of which will be the partition key. Secondly,
        in the case when any destination table already exists, it will first try to
        update existing table's schema as necessary by appending additional
        columns. Then, if [force] is `TRUE`, the corresponding partition will be
        overwritten by the snapshot results (data in different partitions will
        remain intact); if [force] is unset or `FALSE`, it will append the data. An
        error will be returned if the schema update or data appension fails.
        """
        pass
    separate_tables_per_asset_type: builtins.bool = ...
    """If this flag is `TRUE`, the snapshot results will be written to one or
    multiple tables, each of which contains results of one asset type. The
    [force] and [partition_spec] fields will apply to each of them.

    Field [table] will be concatenated with "_" and the asset type names (see
    https://cloud.google.com/asset-inventory/docs/supported-asset-types for
    supported asset types) to construct per-asset-type table names, in which
    all non-alphanumeric characters like "." and "/" will be substituted by
    "_". Example: if field [table] is "mytable" and snapshot results
    contain "storage.googleapis.com/Bucket" assets, the corresponding table
    name will be "mytable_storage_googleapis_com_Bucket". If any of these
    tables does not exist, a new table with the concatenated name will be
    created.

    When [content_type] in the ExportAssetsRequest is `RESOURCE`, the schema of
    each table will include RECORD-type columns mapped to the nested fields in
    the Asset.resource.data field of that asset type (up to the 15 nested level
    BigQuery supports
    (https://cloud.google.com/bigquery/docs/nested-repeated#limitations)). The
    fields in >15 nested levels will be stored in JSON format string as a child
    column of its parent RECORD column.

    If error occurs when exporting to any table, the whole export call will
    return an error but the export results that already succeed will persist.
    Example: if exporting to table_type_A succeeds when exporting to
    table_type_B fails during one export call, the results in table_type_A will
    persist and there will not be partial results persisting in a table.
    """

    def __init__(self,
        *,
        dataset : typing.Text = ...,
        table : typing.Text = ...,
        force : builtins.bool = ...,
        partition_spec : typing.Optional[global___PartitionSpec] = ...,
        separate_tables_per_asset_type : builtins.bool = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["partition_spec",b"partition_spec"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["dataset",b"dataset","force",b"force","partition_spec",b"partition_spec","separate_tables_per_asset_type",b"separate_tables_per_asset_type","table",b"table"]) -> None: ...
global___BigQueryDestination = BigQueryDestination

class PartitionSpec(google.protobuf.message.Message):
    """Specifications of BigQuery partitioned table as export destination."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class _PartitionKey:
        ValueType = typing.NewType('ValueType', builtins.int)
        V: typing_extensions.TypeAlias = ValueType
    class _PartitionKeyEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_PartitionKey.ValueType], builtins.type):
        DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor = ...
        PARTITION_KEY_UNSPECIFIED: PartitionSpec.PartitionKey.ValueType = ...  # 0
        """Unspecified partition key. If used, it means using non-partitioned table."""

        READ_TIME: PartitionSpec.PartitionKey.ValueType = ...  # 1
        """The time when the snapshot is taken. If specified as partition key, the
        result table(s) is partitoned by the additional timestamp column,
        readTime. If [read_time] in ExportAssetsRequest is specified, the
        readTime column's value will be the same as it. Otherwise, its value will
        be the current time that is used to take the snapshot.
        """

        REQUEST_TIME: PartitionSpec.PartitionKey.ValueType = ...  # 2
        """The time when the request is received and started to be processed. If
        specified as partition key, the result table(s) is partitoned by the
        requestTime column, an additional timestamp column representing when the
        request was received.
        """

    class PartitionKey(_PartitionKey, metaclass=_PartitionKeyEnumTypeWrapper):
        """This enum is used to determine the partition key column when exporting
        assets to BigQuery partitioned table(s). Note that, if the partition key is
        a timestamp column, the actual partition is based on its date value
        (expressed in UTC. see details in
        https://cloud.google.com/bigquery/docs/partitioned-tables#date_timestamp_partitioned_tables).
        """
        pass

    PARTITION_KEY_UNSPECIFIED: PartitionSpec.PartitionKey.ValueType = ...  # 0
    """Unspecified partition key. If used, it means using non-partitioned table."""

    READ_TIME: PartitionSpec.PartitionKey.ValueType = ...  # 1
    """The time when the snapshot is taken. If specified as partition key, the
    result table(s) is partitoned by the additional timestamp column,
    readTime. If [read_time] in ExportAssetsRequest is specified, the
    readTime column's value will be the same as it. Otherwise, its value will
    be the current time that is used to take the snapshot.
    """

    REQUEST_TIME: PartitionSpec.PartitionKey.ValueType = ...  # 2
    """The time when the request is received and started to be processed. If
    specified as partition key, the result table(s) is partitoned by the
    requestTime column, an additional timestamp column representing when the
    request was received.
    """


    PARTITION_KEY_FIELD_NUMBER: builtins.int
    partition_key: global___PartitionSpec.PartitionKey.ValueType = ...
    """The partition key for BigQuery partitioned table."""

    def __init__(self,
        *,
        partition_key : global___PartitionSpec.PartitionKey.ValueType = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["partition_key",b"partition_key"]) -> None: ...
global___PartitionSpec = PartitionSpec
