"""
@generated by mypy-protobuf.  Do not edit manually!
isort:skip_file
"""
import builtins
import google.cloud.retail.v2.product_pb2
import google.cloud.retail.v2.user_event_pb2
import google.protobuf.descriptor
import google.protobuf.field_mask_pb2
import google.protobuf.internal.containers
import google.protobuf.internal.enum_type_wrapper
import google.protobuf.message
import google.protobuf.timestamp_pb2
import google.rpc.status_pb2
import google.type.date_pb2
import typing
import typing_extensions

DESCRIPTOR: google.protobuf.descriptor.FileDescriptor = ...

class GcsSource(google.protobuf.message.Message):
    """Google Cloud Storage location for input content.
    format.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    INPUT_URIS_FIELD_NUMBER: builtins.int
    DATA_SCHEMA_FIELD_NUMBER: builtins.int
    @property
    def input_uris(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """Required. Google Cloud Storage URIs to input files. URI can be up to
        2000 characters long. URIs can match the full object path (for example,
        `gs://bucket/directory/object.json`) or a pattern matching one or more
        files, such as `gs://bucket/directory/*.json`. A request can
        contain at most 100 files, and each file can be up to 2 GB. See
        [Importing product
        information](https://cloud.google.com/retail/recommendations-ai/docs/upload-catalog)
        for the expected file format and setup instructions.
        """
        pass
    data_schema: typing.Text = ...
    """The schema to use when parsing the data from the source.

    Supported values for product imports:

    * `product` (default): One JSON [Product][google.cloud.retail.v2.Product]
    per line. Each product must
      have a valid [Product.id][google.cloud.retail.v2.Product.id].
    * `product_merchant_center`: See [Importing catalog data from Merchant
      Center](https://cloud.google.com/retail/recommendations-ai/docs/upload-catalog#mc).

    Supported values for user events imports:

    * `user_event` (default): One JSON
    [UserEvent][google.cloud.retail.v2.UserEvent] per line.
    * `user_event_ga360`: Using
      https://support.google.com/analytics/answer/3437719.
    """

    def __init__(self,
        *,
        input_uris : typing.Optional[typing.Iterable[typing.Text]] = ...,
        data_schema : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["data_schema",b"data_schema","input_uris",b"input_uris"]) -> None: ...
global___GcsSource = GcsSource

class BigQuerySource(google.protobuf.message.Message):
    """BigQuery source import data from."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    PARTITION_DATE_FIELD_NUMBER: builtins.int
    PROJECT_ID_FIELD_NUMBER: builtins.int
    DATASET_ID_FIELD_NUMBER: builtins.int
    TABLE_ID_FIELD_NUMBER: builtins.int
    GCS_STAGING_DIR_FIELD_NUMBER: builtins.int
    DATA_SCHEMA_FIELD_NUMBER: builtins.int
    @property
    def partition_date(self) -> google.type.date_pb2.Date:
        """BigQuery time partitioned table's _PARTITIONDATE in YYYY-MM-DD format.

        Only supported when
        [ImportProductsRequest.reconciliation_mode][google.cloud.retail.v2.ImportProductsRequest.reconciliation_mode]
        is set to `FULL`.
        """
        pass
    project_id: typing.Text = ...
    """The project ID (can be project # or ID) that the BigQuery source is in with
    a length limit of 128 characters. If not specified, inherits the project
    ID from the parent request.
    """

    dataset_id: typing.Text = ...
    """Required. The BigQuery data set to copy the data from with a length limit
    of 1,024 characters.
    """

    table_id: typing.Text = ...
    """Required. The BigQuery table to copy the data from with a length limit of
    1,024 characters.
    """

    gcs_staging_dir: typing.Text = ...
    """Intermediate Cloud Storage directory used for the import with a length
    limit of 2,000 characters. Can be specified if one wants to have the
    BigQuery export to a specific Cloud Storage directory.
    """

    data_schema: typing.Text = ...
    """The schema to use when parsing the data from the source.

    Supported values for product imports:

    * `product` (default): One JSON [Product][google.cloud.retail.v2.Product]
    per line. Each product must
      have a valid [Product.id][google.cloud.retail.v2.Product.id].
    * `product_merchant_center`: See [Importing catalog data from Merchant
      Center](https://cloud.google.com/retail/recommendations-ai/docs/upload-catalog#mc).

    Supported values for user events imports:

    * `user_event` (default): One JSON
    [UserEvent][google.cloud.retail.v2.UserEvent] per line.
    * `user_event_ga360`: Using
      https://support.google.com/analytics/answer/3437719.
    """

    def __init__(self,
        *,
        partition_date : typing.Optional[google.type.date_pb2.Date] = ...,
        project_id : typing.Text = ...,
        dataset_id : typing.Text = ...,
        table_id : typing.Text = ...,
        gcs_staging_dir : typing.Text = ...,
        data_schema : typing.Text = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["partition",b"partition","partition_date",b"partition_date"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["data_schema",b"data_schema","dataset_id",b"dataset_id","gcs_staging_dir",b"gcs_staging_dir","partition",b"partition","partition_date",b"partition_date","project_id",b"project_id","table_id",b"table_id"]) -> None: ...
    def WhichOneof(self, oneof_group: typing_extensions.Literal["partition",b"partition"]) -> typing.Optional[typing_extensions.Literal["partition_date"]]: ...
global___BigQuerySource = BigQuerySource

class ProductInlineSource(google.protobuf.message.Message):
    """The inline source for the input config for ImportProducts method."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    PRODUCTS_FIELD_NUMBER: builtins.int
    @property
    def products(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[google.cloud.retail.v2.product_pb2.Product]:
        """Required. A list of products to update/create. Each product must have a
        valid [Product.id][google.cloud.retail.v2.Product.id]. Recommended max of
        100 items.
        """
        pass
    def __init__(self,
        *,
        products : typing.Optional[typing.Iterable[google.cloud.retail.v2.product_pb2.Product]] = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["products",b"products"]) -> None: ...
global___ProductInlineSource = ProductInlineSource

class UserEventInlineSource(google.protobuf.message.Message):
    """The inline source for the input config for ImportUserEvents method."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    USER_EVENTS_FIELD_NUMBER: builtins.int
    @property
    def user_events(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[google.cloud.retail.v2.user_event_pb2.UserEvent]:
        """Required. A list of user events to import. Recommended max of 10k items."""
        pass
    def __init__(self,
        *,
        user_events : typing.Optional[typing.Iterable[google.cloud.retail.v2.user_event_pb2.UserEvent]] = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["user_events",b"user_events"]) -> None: ...
global___UserEventInlineSource = UserEventInlineSource

class ImportErrorsConfig(google.protobuf.message.Message):
    """Configuration of destination for Import related errors."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    GCS_PREFIX_FIELD_NUMBER: builtins.int
    gcs_prefix: typing.Text = ...
    """Google Cloud Storage path for import errors. This must be an empty,
    existing Cloud Storage bucket. Import errors will be written to a file in
    this bucket, one per line, as a JSON-encoded
    `google.rpc.Status` message.
    """

    def __init__(self,
        *,
        gcs_prefix : typing.Text = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["destination",b"destination","gcs_prefix",b"gcs_prefix"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["destination",b"destination","gcs_prefix",b"gcs_prefix"]) -> None: ...
    def WhichOneof(self, oneof_group: typing_extensions.Literal["destination",b"destination"]) -> typing.Optional[typing_extensions.Literal["gcs_prefix"]]: ...
global___ImportErrorsConfig = ImportErrorsConfig

class ImportProductsRequest(google.protobuf.message.Message):
    """Request message for Import methods."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class _ReconciliationMode:
        ValueType = typing.NewType('ValueType', builtins.int)
        V: typing_extensions.TypeAlias = ValueType
    class _ReconciliationModeEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_ReconciliationMode.ValueType], builtins.type):
        DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor = ...
        RECONCILIATION_MODE_UNSPECIFIED: ImportProductsRequest.ReconciliationMode.ValueType = ...  # 0
        """Defaults to INCREMENTAL."""

        INCREMENTAL: ImportProductsRequest.ReconciliationMode.ValueType = ...  # 1
        """Inserts new products or updates existing products."""

        FULL: ImportProductsRequest.ReconciliationMode.ValueType = ...  # 2
        """Calculates diff and replaces the entire product dataset. Existing
        products may be deleted if they are not present in the source location.

        Can only be while using
        [BigQuerySource][google.cloud.retail.v2.BigQuerySource].

        Add the IAM permission "BigQuery Data Viewer" for
        cloud-retail-customer-data-access@system.gserviceaccount.com before
        using this feature otherwise an error is thrown.

        This feature is only available for users who have Retail Search enabled.
        Please submit a form [here](https://cloud.google.com/contact) to contact
        cloud sales if you are interested in using Retail Search.
        """

    class ReconciliationMode(_ReconciliationMode, metaclass=_ReconciliationModeEnumTypeWrapper):
        """Indicates how imported products are reconciled with the existing products
        created or imported before.
        """
        pass

    RECONCILIATION_MODE_UNSPECIFIED: ImportProductsRequest.ReconciliationMode.ValueType = ...  # 0
    """Defaults to INCREMENTAL."""

    INCREMENTAL: ImportProductsRequest.ReconciliationMode.ValueType = ...  # 1
    """Inserts new products or updates existing products."""

    FULL: ImportProductsRequest.ReconciliationMode.ValueType = ...  # 2
    """Calculates diff and replaces the entire product dataset. Existing
    products may be deleted if they are not present in the source location.

    Can only be while using
    [BigQuerySource][google.cloud.retail.v2.BigQuerySource].

    Add the IAM permission "BigQuery Data Viewer" for
    cloud-retail-customer-data-access@system.gserviceaccount.com before
    using this feature otherwise an error is thrown.

    This feature is only available for users who have Retail Search enabled.
    Please submit a form [here](https://cloud.google.com/contact) to contact
    cloud sales if you are interested in using Retail Search.
    """


    PARENT_FIELD_NUMBER: builtins.int
    REQUEST_ID_FIELD_NUMBER: builtins.int
    INPUT_CONFIG_FIELD_NUMBER: builtins.int
    ERRORS_CONFIG_FIELD_NUMBER: builtins.int
    UPDATE_MASK_FIELD_NUMBER: builtins.int
    RECONCILIATION_MODE_FIELD_NUMBER: builtins.int
    NOTIFICATION_PUBSUB_TOPIC_FIELD_NUMBER: builtins.int
    parent: typing.Text = ...
    """Required.
    `projects/1234/locations/global/catalogs/default_catalog/branches/default_branch`

    If no updateMask is specified, requires products.create permission.
    If updateMask is specified, requires products.update permission.
    """

    request_id: typing.Text = ...
    """Unique identifier provided by client, within the ancestor
    dataset scope. Ensures idempotency and used for request deduplication.
    Server-generated if unspecified. Up to 128 characters long and must match
    the pattern: `[a-zA-Z0-9_]+`. This is returned as [Operation.name][] in
    [ImportMetadata][google.cloud.retail.v2.ImportMetadata].

    Only supported when
    [ImportProductsRequest.reconciliation_mode][google.cloud.retail.v2.ImportProductsRequest.reconciliation_mode]
    is set to `FULL`.
    """

    @property
    def input_config(self) -> global___ProductInputConfig:
        """Required. The desired input location of the data."""
        pass
    @property
    def errors_config(self) -> global___ImportErrorsConfig:
        """The desired location of errors incurred during the Import."""
        pass
    @property
    def update_mask(self) -> google.protobuf.field_mask_pb2.FieldMask:
        """Indicates which fields in the provided imported 'products' to update. If
        not set, will by default update all fields.
        """
        pass
    reconciliation_mode: global___ImportProductsRequest.ReconciliationMode.ValueType = ...
    """The mode of reconciliation between existing products and the products to be
    imported. Defaults to
    [ReconciliationMode.INCREMENTAL][google.cloud.retail.v2.ImportProductsRequest.ReconciliationMode.INCREMENTAL].
    """

    notification_pubsub_topic: typing.Text = ...
    """Pub/Sub topic for receiving notification. If this field is set,
    when the import is finished, a notification will be sent to
    specified Pub/Sub topic. The message data will be JSON string of a
    [Operation][google.longrunning.Operation].
    Format of the Pub/Sub topic is `projects/{project}/topics/{topic}`.

    Only supported when
    [ImportProductsRequest.reconciliation_mode][google.cloud.retail.v2.ImportProductsRequest.reconciliation_mode]
    is set to `FULL`.
    """

    def __init__(self,
        *,
        parent : typing.Text = ...,
        request_id : typing.Text = ...,
        input_config : typing.Optional[global___ProductInputConfig] = ...,
        errors_config : typing.Optional[global___ImportErrorsConfig] = ...,
        update_mask : typing.Optional[google.protobuf.field_mask_pb2.FieldMask] = ...,
        reconciliation_mode : global___ImportProductsRequest.ReconciliationMode.ValueType = ...,
        notification_pubsub_topic : typing.Text = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["errors_config",b"errors_config","input_config",b"input_config","update_mask",b"update_mask"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["errors_config",b"errors_config","input_config",b"input_config","notification_pubsub_topic",b"notification_pubsub_topic","parent",b"parent","reconciliation_mode",b"reconciliation_mode","request_id",b"request_id","update_mask",b"update_mask"]) -> None: ...
global___ImportProductsRequest = ImportProductsRequest

class ImportUserEventsRequest(google.protobuf.message.Message):
    """Request message for the ImportUserEvents request."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    PARENT_FIELD_NUMBER: builtins.int
    INPUT_CONFIG_FIELD_NUMBER: builtins.int
    ERRORS_CONFIG_FIELD_NUMBER: builtins.int
    parent: typing.Text = ...
    """Required. `projects/1234/locations/global/catalogs/default_catalog`"""

    @property
    def input_config(self) -> global___UserEventInputConfig:
        """Required. The desired input location of the data."""
        pass
    @property
    def errors_config(self) -> global___ImportErrorsConfig:
        """The desired location of errors incurred during the Import. Cannot be set
        for inline user event imports.
        """
        pass
    def __init__(self,
        *,
        parent : typing.Text = ...,
        input_config : typing.Optional[global___UserEventInputConfig] = ...,
        errors_config : typing.Optional[global___ImportErrorsConfig] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["errors_config",b"errors_config","input_config",b"input_config"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["errors_config",b"errors_config","input_config",b"input_config","parent",b"parent"]) -> None: ...
global___ImportUserEventsRequest = ImportUserEventsRequest

class ImportCompletionDataRequest(google.protobuf.message.Message):
    """Request message for ImportCompletionData methods."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    PARENT_FIELD_NUMBER: builtins.int
    INPUT_CONFIG_FIELD_NUMBER: builtins.int
    NOTIFICATION_PUBSUB_TOPIC_FIELD_NUMBER: builtins.int
    parent: typing.Text = ...
    """Required. The catalog which the suggestions dataset belongs to.

    Format: `projects/1234/locations/global/catalogs/default_catalog`.
    """

    @property
    def input_config(self) -> global___CompletionDataInputConfig:
        """Required. The desired input location of the data."""
        pass
    notification_pubsub_topic: typing.Text = ...
    """Pub/Sub topic for receiving notification. If this field is set,
    when the import is finished, a notification will be sent to
    specified Pub/Sub topic. The message data will be JSON string of a
    [Operation][google.longrunning.Operation].
    Format of the Pub/Sub topic is `projects/{project}/topics/{topic}`.
    """

    def __init__(self,
        *,
        parent : typing.Text = ...,
        input_config : typing.Optional[global___CompletionDataInputConfig] = ...,
        notification_pubsub_topic : typing.Text = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["input_config",b"input_config"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["input_config",b"input_config","notification_pubsub_topic",b"notification_pubsub_topic","parent",b"parent"]) -> None: ...
global___ImportCompletionDataRequest = ImportCompletionDataRequest

class ProductInputConfig(google.protobuf.message.Message):
    """The input config source for products."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    PRODUCT_INLINE_SOURCE_FIELD_NUMBER: builtins.int
    GCS_SOURCE_FIELD_NUMBER: builtins.int
    BIG_QUERY_SOURCE_FIELD_NUMBER: builtins.int
    @property
    def product_inline_source(self) -> global___ProductInlineSource:
        """The Inline source for the input content for products."""
        pass
    @property
    def gcs_source(self) -> global___GcsSource:
        """Google Cloud Storage location for the input content."""
        pass
    @property
    def big_query_source(self) -> global___BigQuerySource:
        """BigQuery input source."""
        pass
    def __init__(self,
        *,
        product_inline_source : typing.Optional[global___ProductInlineSource] = ...,
        gcs_source : typing.Optional[global___GcsSource] = ...,
        big_query_source : typing.Optional[global___BigQuerySource] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["big_query_source",b"big_query_source","gcs_source",b"gcs_source","product_inline_source",b"product_inline_source","source",b"source"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["big_query_source",b"big_query_source","gcs_source",b"gcs_source","product_inline_source",b"product_inline_source","source",b"source"]) -> None: ...
    def WhichOneof(self, oneof_group: typing_extensions.Literal["source",b"source"]) -> typing.Optional[typing_extensions.Literal["product_inline_source","gcs_source","big_query_source"]]: ...
global___ProductInputConfig = ProductInputConfig

class UserEventInputConfig(google.protobuf.message.Message):
    """The input config source for user events."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    USER_EVENT_INLINE_SOURCE_FIELD_NUMBER: builtins.int
    GCS_SOURCE_FIELD_NUMBER: builtins.int
    BIG_QUERY_SOURCE_FIELD_NUMBER: builtins.int
    @property
    def user_event_inline_source(self) -> global___UserEventInlineSource:
        """Required. The Inline source for the input content for UserEvents."""
        pass
    @property
    def gcs_source(self) -> global___GcsSource:
        """Required. Google Cloud Storage location for the input content."""
        pass
    @property
    def big_query_source(self) -> global___BigQuerySource:
        """Required. BigQuery input source."""
        pass
    def __init__(self,
        *,
        user_event_inline_source : typing.Optional[global___UserEventInlineSource] = ...,
        gcs_source : typing.Optional[global___GcsSource] = ...,
        big_query_source : typing.Optional[global___BigQuerySource] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["big_query_source",b"big_query_source","gcs_source",b"gcs_source","source",b"source","user_event_inline_source",b"user_event_inline_source"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["big_query_source",b"big_query_source","gcs_source",b"gcs_source","source",b"source","user_event_inline_source",b"user_event_inline_source"]) -> None: ...
    def WhichOneof(self, oneof_group: typing_extensions.Literal["source",b"source"]) -> typing.Optional[typing_extensions.Literal["user_event_inline_source","gcs_source","big_query_source"]]: ...
global___UserEventInputConfig = UserEventInputConfig

class CompletionDataInputConfig(google.protobuf.message.Message):
    """The input config source for completion data."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    BIG_QUERY_SOURCE_FIELD_NUMBER: builtins.int
    @property
    def big_query_source(self) -> global___BigQuerySource:
        """Required. BigQuery input source.

        Add the IAM permission "BigQuery Data Viewer" for
        cloud-retail-customer-data-access@system.gserviceaccount.com before
        using this feature otherwise an error is thrown.
        """
        pass
    def __init__(self,
        *,
        big_query_source : typing.Optional[global___BigQuerySource] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["big_query_source",b"big_query_source","source",b"source"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["big_query_source",b"big_query_source","source",b"source"]) -> None: ...
    def WhichOneof(self, oneof_group: typing_extensions.Literal["source",b"source"]) -> typing.Optional[typing_extensions.Literal["big_query_source"]]: ...
global___CompletionDataInputConfig = CompletionDataInputConfig

class ImportMetadata(google.protobuf.message.Message):
    """Metadata related to the progress of the Import operation. This will be
    returned by the google.longrunning.Operation.metadata field.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    CREATE_TIME_FIELD_NUMBER: builtins.int
    UPDATE_TIME_FIELD_NUMBER: builtins.int
    SUCCESS_COUNT_FIELD_NUMBER: builtins.int
    FAILURE_COUNT_FIELD_NUMBER: builtins.int
    REQUEST_ID_FIELD_NUMBER: builtins.int
    NOTIFICATION_PUBSUB_TOPIC_FIELD_NUMBER: builtins.int
    @property
    def create_time(self) -> google.protobuf.timestamp_pb2.Timestamp:
        """Operation create time."""
        pass
    @property
    def update_time(self) -> google.protobuf.timestamp_pb2.Timestamp:
        """Operation last update time. If the operation is done, this is also the
        finish time.
        """
        pass
    success_count: builtins.int = ...
    """Count of entries that were processed successfully."""

    failure_count: builtins.int = ...
    """Count of entries that encountered errors while processing."""

    request_id: typing.Text = ...
    """Id of the request / operation. This is parroting back the requestId
    that was passed in the request.
    """

    notification_pubsub_topic: typing.Text = ...
    """Pub/Sub topic for receiving notification. If this field is set,
    when the import is finished, a notification will be sent to
    specified Pub/Sub topic. The message data will be JSON string of a
    [Operation][google.longrunning.Operation].
    Format of the Pub/Sub topic is `projects/{project}/topics/{topic}`.
    """

    def __init__(self,
        *,
        create_time : typing.Optional[google.protobuf.timestamp_pb2.Timestamp] = ...,
        update_time : typing.Optional[google.protobuf.timestamp_pb2.Timestamp] = ...,
        success_count : builtins.int = ...,
        failure_count : builtins.int = ...,
        request_id : typing.Text = ...,
        notification_pubsub_topic : typing.Text = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["create_time",b"create_time","update_time",b"update_time"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["create_time",b"create_time","failure_count",b"failure_count","notification_pubsub_topic",b"notification_pubsub_topic","request_id",b"request_id","success_count",b"success_count","update_time",b"update_time"]) -> None: ...
global___ImportMetadata = ImportMetadata

class ImportProductsResponse(google.protobuf.message.Message):
    """Response of the
    [ImportProductsRequest][google.cloud.retail.v2.ImportProductsRequest]. If the
    long running operation is done, then this message is returned by the
    google.longrunning.Operations.response field if the operation was successful.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    ERROR_SAMPLES_FIELD_NUMBER: builtins.int
    ERRORS_CONFIG_FIELD_NUMBER: builtins.int
    @property
    def error_samples(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[google.rpc.status_pb2.Status]:
        """A sample of errors encountered while processing the request."""
        pass
    @property
    def errors_config(self) -> global___ImportErrorsConfig:
        """Echoes the destination for the complete errors in the request if set."""
        pass
    def __init__(self,
        *,
        error_samples : typing.Optional[typing.Iterable[google.rpc.status_pb2.Status]] = ...,
        errors_config : typing.Optional[global___ImportErrorsConfig] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["errors_config",b"errors_config"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["error_samples",b"error_samples","errors_config",b"errors_config"]) -> None: ...
global___ImportProductsResponse = ImportProductsResponse

class ImportUserEventsResponse(google.protobuf.message.Message):
    """Response of the ImportUserEventsRequest. If the long running
    operation was successful, then this message is returned by the
    google.longrunning.Operations.response field if the operation was successful.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    ERROR_SAMPLES_FIELD_NUMBER: builtins.int
    ERRORS_CONFIG_FIELD_NUMBER: builtins.int
    IMPORT_SUMMARY_FIELD_NUMBER: builtins.int
    @property
    def error_samples(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[google.rpc.status_pb2.Status]:
        """A sample of errors encountered while processing the request."""
        pass
    @property
    def errors_config(self) -> global___ImportErrorsConfig:
        """Echoes the destination for the complete errors if this field was set in
        the request.
        """
        pass
    @property
    def import_summary(self) -> global___UserEventImportSummary:
        """Aggregated statistics of user event import status."""
        pass
    def __init__(self,
        *,
        error_samples : typing.Optional[typing.Iterable[google.rpc.status_pb2.Status]] = ...,
        errors_config : typing.Optional[global___ImportErrorsConfig] = ...,
        import_summary : typing.Optional[global___UserEventImportSummary] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["errors_config",b"errors_config","import_summary",b"import_summary"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["error_samples",b"error_samples","errors_config",b"errors_config","import_summary",b"import_summary"]) -> None: ...
global___ImportUserEventsResponse = ImportUserEventsResponse

class UserEventImportSummary(google.protobuf.message.Message):
    """A summary of import result. The UserEventImportSummary summarizes
    the import status for user events.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    JOINED_EVENTS_COUNT_FIELD_NUMBER: builtins.int
    UNJOINED_EVENTS_COUNT_FIELD_NUMBER: builtins.int
    joined_events_count: builtins.int = ...
    """Count of user events imported with complete existing catalog information."""

    unjoined_events_count: builtins.int = ...
    """Count of user events imported, but with catalog information not found
    in the imported catalog.
    """

    def __init__(self,
        *,
        joined_events_count : builtins.int = ...,
        unjoined_events_count : builtins.int = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["joined_events_count",b"joined_events_count","unjoined_events_count",b"unjoined_events_count"]) -> None: ...
global___UserEventImportSummary = UserEventImportSummary

class ImportCompletionDataResponse(google.protobuf.message.Message):
    """Response of the
    [ImportCompletionDataRequest][google.cloud.retail.v2.ImportCompletionDataRequest].
    If the long running operation is done, this message is returned by the
    google.longrunning.Operations.response field if the operation is successful.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    ERROR_SAMPLES_FIELD_NUMBER: builtins.int
    @property
    def error_samples(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[google.rpc.status_pb2.Status]:
        """A sample of errors encountered while processing the request."""
        pass
    def __init__(self,
        *,
        error_samples : typing.Optional[typing.Iterable[google.rpc.status_pb2.Status]] = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["error_samples",b"error_samples"]) -> None: ...
global___ImportCompletionDataResponse = ImportCompletionDataResponse
