"""
@generated by mypy-protobuf.  Do not edit manually!
isort:skip_file
"""
import builtins
import google.protobuf.descriptor
import google.protobuf.duration_pb2
import google.protobuf.internal.containers
import google.protobuf.internal.enum_type_wrapper
import google.protobuf.message
import google.protobuf.timestamp_pb2
import google.rpc.code_pb2
import google.type.date_pb2
import google.type.timeofday_pb2
import typing
import typing_extensions

DESCRIPTOR: google.protobuf.descriptor.FileDescriptor = ...

class GoogleServiceAccount(google.protobuf.message.Message):
    """Google service account"""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    ACCOUNT_EMAIL_FIELD_NUMBER: builtins.int
    SUBJECT_ID_FIELD_NUMBER: builtins.int
    account_email: typing.Text = ...
    """Email address of the service account."""

    subject_id: typing.Text = ...
    """Unique identifier for the service account."""

    def __init__(self,
        *,
        account_email : typing.Text = ...,
        subject_id : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["account_email",b"account_email","subject_id",b"subject_id"]) -> None: ...
global___GoogleServiceAccount = GoogleServiceAccount

class AwsAccessKey(google.protobuf.message.Message):
    """AWS access key (see
    [AWS Security
    Credentials](https://docs.aws.amazon.com/general/latest/gr/aws-security-credentials.html)).

    For information on our data retention policy for user credentials, see
    [User credentials](/storage-transfer/docs/data-retention#user-credentials).
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    ACCESS_KEY_ID_FIELD_NUMBER: builtins.int
    SECRET_ACCESS_KEY_FIELD_NUMBER: builtins.int
    access_key_id: typing.Text = ...
    """Required. AWS access key ID."""

    secret_access_key: typing.Text = ...
    """Required. AWS secret access key. This field is not returned in RPC
    responses.
    """

    def __init__(self,
        *,
        access_key_id : typing.Text = ...,
        secret_access_key : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["access_key_id",b"access_key_id","secret_access_key",b"secret_access_key"]) -> None: ...
global___AwsAccessKey = AwsAccessKey

class AzureCredentials(google.protobuf.message.Message):
    """Azure credentials

    For information on our data retention policy for user credentials, see
    [User credentials](/storage-transfer/docs/data-retention#user-credentials).
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    SAS_TOKEN_FIELD_NUMBER: builtins.int
    sas_token: typing.Text = ...
    """Required. Azure shared access signature (SAS).

    <aside class="note">
    <strong>Note:</strong>Copying data from Azure Data Lake
    Storage (ADLS) Gen 2 is in [Preview](/products/#product-launch-stages).
    During Preview, if you are copying data from ADLS Gen 2, you must use an
    account SAS.
    </aside>

    For more information about SAS, see
    [Grant limited access to Azure Storage resources using shared access
    signatures
    (SAS)](https://docs.microsoft.com/en-us/azure/storage/common/storage-sas-overview).
    """

    def __init__(self,
        *,
        sas_token : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["sas_token",b"sas_token"]) -> None: ...
global___AzureCredentials = AzureCredentials

class ObjectConditions(google.protobuf.message.Message):
    """Conditions that determine which objects will be transferred. Applies only
    to Cloud Data Sources such as S3, Azure, and Cloud Storage.

    The "last modification time" refers to the time of the
    last change to the object's content or metadata — specifically, this is
    the `updated` property of Cloud Storage objects, the `LastModified` field
    of S3 objects, and the `Last-Modified` header of Azure blobs.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    MIN_TIME_ELAPSED_SINCE_LAST_MODIFICATION_FIELD_NUMBER: builtins.int
    MAX_TIME_ELAPSED_SINCE_LAST_MODIFICATION_FIELD_NUMBER: builtins.int
    INCLUDE_PREFIXES_FIELD_NUMBER: builtins.int
    EXCLUDE_PREFIXES_FIELD_NUMBER: builtins.int
    LAST_MODIFIED_SINCE_FIELD_NUMBER: builtins.int
    LAST_MODIFIED_BEFORE_FIELD_NUMBER: builtins.int
    @property
    def min_time_elapsed_since_last_modification(self) -> google.protobuf.duration_pb2.Duration:
        """If specified, only objects with a "last modification time" before
        `NOW` - `min_time_elapsed_since_last_modification` and objects that don't
         have a "last modification time" are transferred.

        For each [TransferOperation][google.storagetransfer.v1.TransferOperation]
        started by this [TransferJob][google.storagetransfer.v1.TransferJob], `NOW`
        refers to the [start_time]
        [google.storagetransfer.v1.TransferOperation.start_time] of the
        `TransferOperation`.
        """
        pass
    @property
    def max_time_elapsed_since_last_modification(self) -> google.protobuf.duration_pb2.Duration:
        """If specified, only objects with a "last modification time" on or after
        `NOW` - `max_time_elapsed_since_last_modification` and objects that don't
        have a "last modification time" are transferred.

        For each [TransferOperation][google.storagetransfer.v1.TransferOperation]
        started by this [TransferJob][google.storagetransfer.v1.TransferJob],
        `NOW` refers to the [start_time]
        [google.storagetransfer.v1.TransferOperation.start_time] of the
        `TransferOperation`.
        """
        pass
    @property
    def include_prefixes(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """If you specify `include_prefixes`, Storage Transfer Service uses the items
        in the `include_prefixes` array to determine which objects to include in a
        transfer. Objects must start with one of the matching `include_prefixes`
        for inclusion in the transfer. If [exclude_prefixes]
        [google.storagetransfer.v1.ObjectConditions.exclude_prefixes] is specified,
        objects must not start with any of the `exclude_prefixes` specified for
        inclusion in the transfer.

        The following are requirements of `include_prefixes`:

          * Each include-prefix can contain any sequence of Unicode characters, to
            a max length of 1024 bytes when UTF8-encoded, and must not contain
            Carriage Return or Line Feed characters.  Wildcard matching and regular
            expression matching are not supported.

          * Each include-prefix must omit the leading slash. For example, to
            include the object `s3://my-aws-bucket/logs/y=2015/requests.gz`,
            specify the include-prefix as `logs/y=2015/requests.gz`.

          * None of the include-prefix values can be empty, if specified.

          * Each include-prefix must include a distinct portion of the object
            namespace. No include-prefix may be a prefix of another
            include-prefix.

        The max size of `include_prefixes` is 1000.

        For more information, see [Filtering objects from
        transfers](/storage-transfer/docs/filtering-objects-from-transfers).
        """
        pass
    @property
    def exclude_prefixes(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """If you specify `exclude_prefixes`, Storage Transfer Service uses the items
        in the `exclude_prefixes` array to determine which objects to exclude from
        a transfer. Objects must not start with one of the matching
        `exclude_prefixes` for inclusion in a transfer.

        The following are requirements of `exclude_prefixes`:

          * Each exclude-prefix can contain any sequence of Unicode characters, to
            a max length of 1024 bytes when UTF8-encoded, and must not contain
            Carriage Return or Line Feed characters.  Wildcard matching and regular
            expression matching are not supported.

          * Each exclude-prefix must omit the leading slash. For example, to
            exclude the object `s3://my-aws-bucket/logs/y=2015/requests.gz`,
            specify the exclude-prefix as `logs/y=2015/requests.gz`.

          * None of the exclude-prefix values can be empty, if specified.

          * Each exclude-prefix must exclude a distinct portion of the object
            namespace. No exclude-prefix may be a prefix of another
            exclude-prefix.

          * If [include_prefixes]
            [google.storagetransfer.v1.ObjectConditions.include_prefixes] is
            specified, then each exclude-prefix must start with the value of a path
            explicitly included by `include_prefixes`.

        The max size of `exclude_prefixes` is 1000.

        For more information, see [Filtering objects from
        transfers](/storage-transfer/docs/filtering-objects-from-transfers).
        """
        pass
    @property
    def last_modified_since(self) -> google.protobuf.timestamp_pb2.Timestamp:
        """If specified, only objects with a "last modification time" on or after
        this timestamp and objects that don't have a "last modification time" are
        transferred.

        The `last_modified_since` and `last_modified_before` fields can be used
        together for chunked data processing. For example, consider a script that
        processes each day's worth of data at a time. For that you'd set each
        of the fields as follows:

        *  `last_modified_since` to the start of the day

        *  `last_modified_before` to the end of the day
        """
        pass
    @property
    def last_modified_before(self) -> google.protobuf.timestamp_pb2.Timestamp:
        """If specified, only objects with a "last modification time" before this
        timestamp and objects that don't have a "last modification time" will be
        transferred.
        """
        pass
    def __init__(self,
        *,
        min_time_elapsed_since_last_modification : typing.Optional[google.protobuf.duration_pb2.Duration] = ...,
        max_time_elapsed_since_last_modification : typing.Optional[google.protobuf.duration_pb2.Duration] = ...,
        include_prefixes : typing.Optional[typing.Iterable[typing.Text]] = ...,
        exclude_prefixes : typing.Optional[typing.Iterable[typing.Text]] = ...,
        last_modified_since : typing.Optional[google.protobuf.timestamp_pb2.Timestamp] = ...,
        last_modified_before : typing.Optional[google.protobuf.timestamp_pb2.Timestamp] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["last_modified_before",b"last_modified_before","last_modified_since",b"last_modified_since","max_time_elapsed_since_last_modification",b"max_time_elapsed_since_last_modification","min_time_elapsed_since_last_modification",b"min_time_elapsed_since_last_modification"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["exclude_prefixes",b"exclude_prefixes","include_prefixes",b"include_prefixes","last_modified_before",b"last_modified_before","last_modified_since",b"last_modified_since","max_time_elapsed_since_last_modification",b"max_time_elapsed_since_last_modification","min_time_elapsed_since_last_modification",b"min_time_elapsed_since_last_modification"]) -> None: ...
global___ObjectConditions = ObjectConditions

class GcsData(google.protobuf.message.Message):
    """In a GcsData resource, an object's name is the Cloud Storage object's
    name and its "last modification time" refers to the object's `updated`
    property of Cloud Storage objects, which changes when the content or the
    metadata of the object is updated.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    BUCKET_NAME_FIELD_NUMBER: builtins.int
    PATH_FIELD_NUMBER: builtins.int
    bucket_name: typing.Text = ...
    """Required. Cloud Storage bucket name. Must meet
    [Bucket Name Requirements](/storage/docs/naming#requirements).
    """

    path: typing.Text = ...
    """Root path to transfer objects.

    Must be an empty string or full path name that ends with a '/'. This field
    is treated as an object prefix. As such, it should generally not begin with
    a '/'.

    The root path value must meet
    [Object Name Requirements](/storage/docs/naming#objectnames).
    """

    def __init__(self,
        *,
        bucket_name : typing.Text = ...,
        path : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["bucket_name",b"bucket_name","path",b"path"]) -> None: ...
global___GcsData = GcsData

class AwsS3Data(google.protobuf.message.Message):
    """An AwsS3Data resource can be a data source, but not a data sink.
    In an AwsS3Data resource, an object's name is the S3 object's key name.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    BUCKET_NAME_FIELD_NUMBER: builtins.int
    AWS_ACCESS_KEY_FIELD_NUMBER: builtins.int
    PATH_FIELD_NUMBER: builtins.int
    ROLE_ARN_FIELD_NUMBER: builtins.int
    bucket_name: typing.Text = ...
    """Required. S3 Bucket name (see
    [Creating a
    bucket](https://docs.aws.amazon.com/AmazonS3/latest/dev/create-bucket-get-location-example.html)).
    """

    @property
    def aws_access_key(self) -> global___AwsAccessKey:
        """Input only. AWS access key used to sign the API requests to the AWS S3
        bucket. Permissions on the bucket must be granted to the access ID of the
        AWS access key. This field is required.

        For information on our data retention policy for user credentials, see
        [User credentials](/storage-transfer/docs/data-retention#user-credentials).
        """
        pass
    path: typing.Text = ...
    """Root path to transfer objects.

    Must be an empty string or full path name that ends with a '/'. This field
    is treated as an object prefix. As such, it should generally not begin with
    a '/'.
    """

    role_arn: typing.Text = ...
    """Input only. Role arn to support temporary credentials via
    AssumeRoleWithWebIdentity.

    When role arn is provided, transfer service will fetch temporary
    credentials for the session using AssumeRoleWithWebIdentity call for the
    provided role using the [GoogleServiceAccount] for this project.
    """

    def __init__(self,
        *,
        bucket_name : typing.Text = ...,
        aws_access_key : typing.Optional[global___AwsAccessKey] = ...,
        path : typing.Text = ...,
        role_arn : typing.Text = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["aws_access_key",b"aws_access_key"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["aws_access_key",b"aws_access_key","bucket_name",b"bucket_name","path",b"path","role_arn",b"role_arn"]) -> None: ...
global___AwsS3Data = AwsS3Data

class AzureBlobStorageData(google.protobuf.message.Message):
    """An AzureBlobStorageData resource can be a data source, but not a data sink.
    An AzureBlobStorageData resource represents one Azure container. The storage
    account determines the [Azure
    endpoint](https://docs.microsoft.com/en-us/azure/storage/common/storage-create-storage-account#storage-account-endpoints).
    In an AzureBlobStorageData resource, a blobs's name is the [Azure Blob
    Storage blob's key
    name](https://docs.microsoft.com/en-us/rest/api/storageservices/naming-and-referencing-containers--blobs--and-metadata#blob-names).
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    STORAGE_ACCOUNT_FIELD_NUMBER: builtins.int
    AZURE_CREDENTIALS_FIELD_NUMBER: builtins.int
    CONTAINER_FIELD_NUMBER: builtins.int
    PATH_FIELD_NUMBER: builtins.int
    storage_account: typing.Text = ...
    """Required. The name of the Azure Storage account."""

    @property
    def azure_credentials(self) -> global___AzureCredentials:
        """Required. Input only. Credentials used to authenticate API requests to
        Azure.

        For information on our data retention policy for user credentials, see
        [User credentials](/storage-transfer/docs/data-retention#user-credentials).
        """
        pass
    container: typing.Text = ...
    """Required. The container to transfer from the Azure Storage account."""

    path: typing.Text = ...
    """Root path to transfer objects.

    Must be an empty string or full path name that ends with a '/'. This field
    is treated as an object prefix. As such, it should generally not begin with
    a '/'.
    """

    def __init__(self,
        *,
        storage_account : typing.Text = ...,
        azure_credentials : typing.Optional[global___AzureCredentials] = ...,
        container : typing.Text = ...,
        path : typing.Text = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["azure_credentials",b"azure_credentials"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["azure_credentials",b"azure_credentials","container",b"container","path",b"path","storage_account",b"storage_account"]) -> None: ...
global___AzureBlobStorageData = AzureBlobStorageData

class HttpData(google.protobuf.message.Message):
    """An HttpData resource specifies a list of objects on the web to be transferred
    over HTTP.  The information of the objects to be transferred is contained in
    a file referenced by a URL. The first line in the file must be
    `"TsvHttpData-1.0"`, which specifies the format of the file.  Subsequent
    lines specify the information of the list of objects, one object per list
    entry. Each entry has the following tab-delimited fields:

    * **HTTP URL** — The location of the object.

    * **Length** — The size of the object in bytes.

    * **MD5** — The base64-encoded MD5 hash of the object.

    For an example of a valid TSV file, see
    [Transferring data from
    URLs](https://cloud.google.com/storage-transfer/docs/create-url-list).

    When transferring data based on a URL list, keep the following in mind:

    * When an object located at `http(s)://hostname:port/<URL-path>` is
    transferred to a data sink, the name of the object at the data sink is
    `<hostname>/<URL-path>`.

    * If the specified size of an object does not match the actual size of the
    object fetched, the object will not be transferred.

    * If the specified MD5 does not match the MD5 computed from the transferred
    bytes, the object transfer will fail.

    * Ensure that each URL you specify is publicly accessible. For
    example, in Cloud Storage you can
    [share an object publicly]
    (/storage/docs/cloud-console#_sharingdata) and get a link to it.

    * Storage Transfer Service obeys `robots.txt` rules and requires the source
    HTTP server to support `Range` requests and to return a `Content-Length`
    header in each response.

    * [ObjectConditions][google.storagetransfer.v1.ObjectConditions] have no
    effect when filtering objects to transfer.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    LIST_URL_FIELD_NUMBER: builtins.int
    list_url: typing.Text = ...
    """Required. The URL that points to the file that stores the object list
    entries. This file must allow public access.  Currently, only URLs with
    HTTP and HTTPS schemes are supported.
    """

    def __init__(self,
        *,
        list_url : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["list_url",b"list_url"]) -> None: ...
global___HttpData = HttpData

class TransferOptions(google.protobuf.message.Message):
    """TransferOptions define the actions to be performed on objects in a transfer."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    OVERWRITE_OBJECTS_ALREADY_EXISTING_IN_SINK_FIELD_NUMBER: builtins.int
    DELETE_OBJECTS_UNIQUE_IN_SINK_FIELD_NUMBER: builtins.int
    DELETE_OBJECTS_FROM_SOURCE_AFTER_TRANSFER_FIELD_NUMBER: builtins.int
    overwrite_objects_already_existing_in_sink: builtins.bool = ...
    """When to overwrite objects that already exist in the sink. The default is
    that only objects that are different from the source are ovewritten. If
    true, all objects in the sink whose name matches an object in the source
    will be overwritten with the source object.
    """

    delete_objects_unique_in_sink: builtins.bool = ...
    """Whether objects that exist only in the sink should be deleted.

    **Note:** This option and [delete_objects_from_source_after_transfer]
    [google.storagetransfer.v1.TransferOptions.delete_objects_from_source_after_transfer]
    are mutually exclusive.
    """

    delete_objects_from_source_after_transfer: builtins.bool = ...
    """Whether objects should be deleted from the source after they are
    transferred to the sink.

    **Note:** This option and [delete_objects_unique_in_sink]
    [google.storagetransfer.v1.TransferOptions.delete_objects_unique_in_sink]
    are mutually exclusive.
    """

    def __init__(self,
        *,
        overwrite_objects_already_existing_in_sink : builtins.bool = ...,
        delete_objects_unique_in_sink : builtins.bool = ...,
        delete_objects_from_source_after_transfer : builtins.bool = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["delete_objects_from_source_after_transfer",b"delete_objects_from_source_after_transfer","delete_objects_unique_in_sink",b"delete_objects_unique_in_sink","overwrite_objects_already_existing_in_sink",b"overwrite_objects_already_existing_in_sink"]) -> None: ...
global___TransferOptions = TransferOptions

class TransferSpec(google.protobuf.message.Message):
    """Configuration for running a transfer."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    GCS_DATA_SINK_FIELD_NUMBER: builtins.int
    GCS_DATA_SOURCE_FIELD_NUMBER: builtins.int
    AWS_S3_DATA_SOURCE_FIELD_NUMBER: builtins.int
    HTTP_DATA_SOURCE_FIELD_NUMBER: builtins.int
    AZURE_BLOB_STORAGE_DATA_SOURCE_FIELD_NUMBER: builtins.int
    OBJECT_CONDITIONS_FIELD_NUMBER: builtins.int
    TRANSFER_OPTIONS_FIELD_NUMBER: builtins.int
    @property
    def gcs_data_sink(self) -> global___GcsData:
        """A Cloud Storage data sink."""
        pass
    @property
    def gcs_data_source(self) -> global___GcsData:
        """A Cloud Storage data source."""
        pass
    @property
    def aws_s3_data_source(self) -> global___AwsS3Data:
        """An AWS S3 data source."""
        pass
    @property
    def http_data_source(self) -> global___HttpData:
        """An HTTP URL data source."""
        pass
    @property
    def azure_blob_storage_data_source(self) -> global___AzureBlobStorageData:
        """An Azure Blob Storage data source."""
        pass
    @property
    def object_conditions(self) -> global___ObjectConditions:
        """Only objects that satisfy these object conditions are included in the set
        of data source and data sink objects.  Object conditions based on
        objects' "last modification time" do not exclude objects in a data sink.
        """
        pass
    @property
    def transfer_options(self) -> global___TransferOptions:
        """If the option
        [delete_objects_unique_in_sink][google.storagetransfer.v1.TransferOptions.delete_objects_unique_in_sink]
        is `true` and time-based object conditions such as 'last modification time'
        are specified, the request fails with an
        [INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT] error.
        """
        pass
    def __init__(self,
        *,
        gcs_data_sink : typing.Optional[global___GcsData] = ...,
        gcs_data_source : typing.Optional[global___GcsData] = ...,
        aws_s3_data_source : typing.Optional[global___AwsS3Data] = ...,
        http_data_source : typing.Optional[global___HttpData] = ...,
        azure_blob_storage_data_source : typing.Optional[global___AzureBlobStorageData] = ...,
        object_conditions : typing.Optional[global___ObjectConditions] = ...,
        transfer_options : typing.Optional[global___TransferOptions] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["aws_s3_data_source",b"aws_s3_data_source","azure_blob_storage_data_source",b"azure_blob_storage_data_source","data_sink",b"data_sink","data_source",b"data_source","gcs_data_sink",b"gcs_data_sink","gcs_data_source",b"gcs_data_source","http_data_source",b"http_data_source","object_conditions",b"object_conditions","transfer_options",b"transfer_options"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["aws_s3_data_source",b"aws_s3_data_source","azure_blob_storage_data_source",b"azure_blob_storage_data_source","data_sink",b"data_sink","data_source",b"data_source","gcs_data_sink",b"gcs_data_sink","gcs_data_source",b"gcs_data_source","http_data_source",b"http_data_source","object_conditions",b"object_conditions","transfer_options",b"transfer_options"]) -> None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing_extensions.Literal["data_sink",b"data_sink"]) -> typing.Optional[typing_extensions.Literal["gcs_data_sink"]]: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing_extensions.Literal["data_source",b"data_source"]) -> typing.Optional[typing_extensions.Literal["gcs_data_source","aws_s3_data_source","http_data_source","azure_blob_storage_data_source"]]: ...
global___TransferSpec = TransferSpec

class Schedule(google.protobuf.message.Message):
    """Transfers can be scheduled to recur or to run just once."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    SCHEDULE_START_DATE_FIELD_NUMBER: builtins.int
    SCHEDULE_END_DATE_FIELD_NUMBER: builtins.int
    START_TIME_OF_DAY_FIELD_NUMBER: builtins.int
    END_TIME_OF_DAY_FIELD_NUMBER: builtins.int
    REPEAT_INTERVAL_FIELD_NUMBER: builtins.int
    @property
    def schedule_start_date(self) -> google.type.date_pb2.Date:
        """Required. The start date of a transfer. Date boundaries are determined
        relative to UTC time. If `schedule_start_date` and
        [start_time_of_day][google.storagetransfer.v1.Schedule.start_time_of_day]
        are in the past relative to the job's creation time, the transfer starts
        the day after you schedule the transfer request.

        **Note:** When starting jobs at or near midnight UTC it is possible that
        a job will start later than expected. For example, if you send an outbound
        request on June 1 one millisecond prior to midnight UTC and the Storage
        Transfer Service server receives the request on June 2, then it will create
        a TransferJob with `schedule_start_date` set to June 2 and a
        `start_time_of_day` set to midnight UTC. The first scheduled
        [TransferOperation][google.storagetransfer.v1.TransferOperation] will take
        place on June 3 at midnight UTC.
        """
        pass
    @property
    def schedule_end_date(self) -> google.type.date_pb2.Date:
        """The last day a transfer runs. Date boundaries are determined relative to
        UTC time. A job will run once per 24 hours within the following guidelines:

        *   If `schedule_end_date` and
        [schedule_start_date][google.storagetransfer.v1.Schedule.schedule_start_date]
        are the same and in
            the future relative to UTC, the transfer is executed only one time.
        *   If `schedule_end_date` is later than `schedule_start_date`  and
            `schedule_end_date` is in the future relative to UTC, the job will
            run each day at
            [start_time_of_day][google.storagetransfer.v1.Schedule.start_time_of_day]
            through `schedule_end_date`.
        """
        pass
    @property
    def start_time_of_day(self) -> google.type.timeofday_pb2.TimeOfDay:
        """The time in UTC that a transfer job is scheduled to run. Transfers may
        start later than this time.

        If `start_time_of_day` is not specified:

        *   One-time transfers run immediately.
        *   Recurring transfers run immediately, and each day at midnight UTC,
            through
            [schedule_end_date][google.storagetransfer.v1.Schedule.schedule_end_date].

        If `start_time_of_day` is specified:

        *   One-time transfers run at the specified time.
        *   Recurring transfers run at the specified time each day, through
            `schedule_end_date`.
        """
        pass
    @property
    def end_time_of_day(self) -> google.type.timeofday_pb2.TimeOfDay:
        """The time in UTC that no further transfer operations are scheduled. Combined
        with
        [schedule_end_date][google.storagetransfer.v1.Schedule.schedule_end_date],
        `end_time_of_day` specifies the end date and time for starting new transfer
        operations. This field must be greater than or equal to the timestamp
        corresponding to the combintation of
        [schedule_start_date][google.storagetransfer.v1.Schedule.schedule_start_date]
        and
        [start_time_of_day][google.storagetransfer.v1.Schedule.start_time_of_day],
        and is subject to the following:

        *   If `end_time_of_day` is not set and `schedule_end_date` is set, then
            a default value of `23:59:59` is used for `end_time_of_day`.

        *   If `end_time_of_day` is set and `schedule_end_date` is not set, then
            [INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT] is returned.
        """
        pass
    @property
    def repeat_interval(self) -> google.protobuf.duration_pb2.Duration:
        """Interval between the start of each scheduled TransferOperation. If
        unspecified, the default value is 24 hours. This value may not be less than
        1 hour.
        """
        pass
    def __init__(self,
        *,
        schedule_start_date : typing.Optional[google.type.date_pb2.Date] = ...,
        schedule_end_date : typing.Optional[google.type.date_pb2.Date] = ...,
        start_time_of_day : typing.Optional[google.type.timeofday_pb2.TimeOfDay] = ...,
        end_time_of_day : typing.Optional[google.type.timeofday_pb2.TimeOfDay] = ...,
        repeat_interval : typing.Optional[google.protobuf.duration_pb2.Duration] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["end_time_of_day",b"end_time_of_day","repeat_interval",b"repeat_interval","schedule_end_date",b"schedule_end_date","schedule_start_date",b"schedule_start_date","start_time_of_day",b"start_time_of_day"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["end_time_of_day",b"end_time_of_day","repeat_interval",b"repeat_interval","schedule_end_date",b"schedule_end_date","schedule_start_date",b"schedule_start_date","start_time_of_day",b"start_time_of_day"]) -> None: ...
global___Schedule = Schedule

class TransferJob(google.protobuf.message.Message):
    """This resource represents the configuration of a transfer job that runs
    periodically.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class _Status:
        ValueType = typing.NewType('ValueType', builtins.int)
        V: typing_extensions.TypeAlias = ValueType
    class _StatusEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_Status.ValueType], builtins.type):
        DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor = ...
        STATUS_UNSPECIFIED: TransferJob.Status.ValueType = ...  # 0
        """Zero is an illegal value."""

        ENABLED: TransferJob.Status.ValueType = ...  # 1
        """New transfers will be performed based on the schedule."""

        DISABLED: TransferJob.Status.ValueType = ...  # 2
        """New transfers will not be scheduled."""

        DELETED: TransferJob.Status.ValueType = ...  # 3
        """This is a soft delete state. After a transfer job is set to this
        state, the job and all the transfer executions are subject to
        garbage collection. Transfer jobs become eligible for garbage collection
        30 days after their status is set to `DELETED`.
        """

    class Status(_Status, metaclass=_StatusEnumTypeWrapper):
        """The status of the transfer job."""
        pass

    STATUS_UNSPECIFIED: TransferJob.Status.ValueType = ...  # 0
    """Zero is an illegal value."""

    ENABLED: TransferJob.Status.ValueType = ...  # 1
    """New transfers will be performed based on the schedule."""

    DISABLED: TransferJob.Status.ValueType = ...  # 2
    """New transfers will not be scheduled."""

    DELETED: TransferJob.Status.ValueType = ...  # 3
    """This is a soft delete state. After a transfer job is set to this
    state, the job and all the transfer executions are subject to
    garbage collection. Transfer jobs become eligible for garbage collection
    30 days after their status is set to `DELETED`.
    """


    NAME_FIELD_NUMBER: builtins.int
    DESCRIPTION_FIELD_NUMBER: builtins.int
    PROJECT_ID_FIELD_NUMBER: builtins.int
    TRANSFER_SPEC_FIELD_NUMBER: builtins.int
    NOTIFICATION_CONFIG_FIELD_NUMBER: builtins.int
    SCHEDULE_FIELD_NUMBER: builtins.int
    STATUS_FIELD_NUMBER: builtins.int
    CREATION_TIME_FIELD_NUMBER: builtins.int
    LAST_MODIFICATION_TIME_FIELD_NUMBER: builtins.int
    DELETION_TIME_FIELD_NUMBER: builtins.int
    LATEST_OPERATION_NAME_FIELD_NUMBER: builtins.int
    name: typing.Text = ...
    """A unique name (within the transfer project) assigned when the job is
    created.  If this field is empty in a CreateTransferJobRequest, Storage
    Transfer Service will assign a unique name. Otherwise, the specified name
    is used as the unique name for this job.

    If the specified name is in use by a job, the creation request fails with
    an [ALREADY_EXISTS][google.rpc.Code.ALREADY_EXISTS] error.

    This name must start with `"transferJobs/"` prefix and end with a letter or
    a number, and should be no more than 128 characters. This name must not
    start with 'transferJobs/OPI'. 'transferJobs/OPI' is a reserved prefix.
    Example:
    `"transferJobs/^(?!OPI)[A-Za-z0-9-._~]*[A-Za-z0-9]$"`

    Invalid job names will fail with an
    [INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT] error.
    """

    description: typing.Text = ...
    """A description provided by the user for the job. Its max length is 1024
    bytes when Unicode-encoded.
    """

    project_id: typing.Text = ...
    """The ID of the Google Cloud Platform Project that owns the job."""

    @property
    def transfer_spec(self) -> global___TransferSpec:
        """Transfer specification."""
        pass
    @property
    def notification_config(self) -> global___NotificationConfig:
        """Notification configuration."""
        pass
    @property
    def schedule(self) -> global___Schedule:
        """Specifies schedule for the transfer job.
        This is an optional field. When the field is not set, the job will never
        execute a transfer, unless you invoke RunTransferJob or update the job to
        have a non-empty schedule.
        """
        pass
    status: global___TransferJob.Status.ValueType = ...
    """Status of the job. This value MUST be specified for
    `CreateTransferJobRequests`.

    **Note:** The effect of the new job status takes place during a subsequent
    job run. For example, if you change the job status from
    [ENABLED][google.storagetransfer.v1.TransferJob.Status.ENABLED] to
    [DISABLED][google.storagetransfer.v1.TransferJob.Status.DISABLED], and an
    operation spawned by the transfer is running, the status change would not
    affect the current operation.
    """

    @property
    def creation_time(self) -> google.protobuf.timestamp_pb2.Timestamp:
        """Output only. The time that the transfer job was created."""
        pass
    @property
    def last_modification_time(self) -> google.protobuf.timestamp_pb2.Timestamp:
        """Output only. The time that the transfer job was last modified."""
        pass
    @property
    def deletion_time(self) -> google.protobuf.timestamp_pb2.Timestamp:
        """Output only. The time that the transfer job was deleted."""
        pass
    latest_operation_name: typing.Text = ...
    """The name of the most recently started TransferOperation of this JobConfig.
    Present if a TransferOperation has been created for this JobConfig.
    """

    def __init__(self,
        *,
        name : typing.Text = ...,
        description : typing.Text = ...,
        project_id : typing.Text = ...,
        transfer_spec : typing.Optional[global___TransferSpec] = ...,
        notification_config : typing.Optional[global___NotificationConfig] = ...,
        schedule : typing.Optional[global___Schedule] = ...,
        status : global___TransferJob.Status.ValueType = ...,
        creation_time : typing.Optional[google.protobuf.timestamp_pb2.Timestamp] = ...,
        last_modification_time : typing.Optional[google.protobuf.timestamp_pb2.Timestamp] = ...,
        deletion_time : typing.Optional[google.protobuf.timestamp_pb2.Timestamp] = ...,
        latest_operation_name : typing.Text = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["creation_time",b"creation_time","deletion_time",b"deletion_time","last_modification_time",b"last_modification_time","notification_config",b"notification_config","schedule",b"schedule","transfer_spec",b"transfer_spec"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["creation_time",b"creation_time","deletion_time",b"deletion_time","description",b"description","last_modification_time",b"last_modification_time","latest_operation_name",b"latest_operation_name","name",b"name","notification_config",b"notification_config","project_id",b"project_id","schedule",b"schedule","status",b"status","transfer_spec",b"transfer_spec"]) -> None: ...
global___TransferJob = TransferJob

class ErrorLogEntry(google.protobuf.message.Message):
    """An entry describing an error that has occurred."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    URL_FIELD_NUMBER: builtins.int
    ERROR_DETAILS_FIELD_NUMBER: builtins.int
    url: typing.Text = ...
    """Required. A URL that refers to the target (a data source, a data sink,
    or an object) with which the error is associated.
    """

    @property
    def error_details(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """A list of messages that carry the error details."""
        pass
    def __init__(self,
        *,
        url : typing.Text = ...,
        error_details : typing.Optional[typing.Iterable[typing.Text]] = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["error_details",b"error_details","url",b"url"]) -> None: ...
global___ErrorLogEntry = ErrorLogEntry

class ErrorSummary(google.protobuf.message.Message):
    """A summary of errors by error code, plus a count and sample error log
    entries.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    ERROR_CODE_FIELD_NUMBER: builtins.int
    ERROR_COUNT_FIELD_NUMBER: builtins.int
    ERROR_LOG_ENTRIES_FIELD_NUMBER: builtins.int
    error_code: google.rpc.code_pb2.Code.ValueType = ...
    """Required."""

    error_count: builtins.int = ...
    """Required. Count of this type of error."""

    @property
    def error_log_entries(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___ErrorLogEntry]:
        """Error samples.

        At most 5 error log entries will be recorded for a given
        error code for a single transfer operation.
        """
        pass
    def __init__(self,
        *,
        error_code : google.rpc.code_pb2.Code.ValueType = ...,
        error_count : builtins.int = ...,
        error_log_entries : typing.Optional[typing.Iterable[global___ErrorLogEntry]] = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["error_code",b"error_code","error_count",b"error_count","error_log_entries",b"error_log_entries"]) -> None: ...
global___ErrorSummary = ErrorSummary

class TransferCounters(google.protobuf.message.Message):
    """A collection of counters that report the progress of a transfer operation."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    OBJECTS_FOUND_FROM_SOURCE_FIELD_NUMBER: builtins.int
    BYTES_FOUND_FROM_SOURCE_FIELD_NUMBER: builtins.int
    OBJECTS_FOUND_ONLY_FROM_SINK_FIELD_NUMBER: builtins.int
    BYTES_FOUND_ONLY_FROM_SINK_FIELD_NUMBER: builtins.int
    OBJECTS_FROM_SOURCE_SKIPPED_BY_SYNC_FIELD_NUMBER: builtins.int
    BYTES_FROM_SOURCE_SKIPPED_BY_SYNC_FIELD_NUMBER: builtins.int
    OBJECTS_COPIED_TO_SINK_FIELD_NUMBER: builtins.int
    BYTES_COPIED_TO_SINK_FIELD_NUMBER: builtins.int
    OBJECTS_DELETED_FROM_SOURCE_FIELD_NUMBER: builtins.int
    BYTES_DELETED_FROM_SOURCE_FIELD_NUMBER: builtins.int
    OBJECTS_DELETED_FROM_SINK_FIELD_NUMBER: builtins.int
    BYTES_DELETED_FROM_SINK_FIELD_NUMBER: builtins.int
    OBJECTS_FROM_SOURCE_FAILED_FIELD_NUMBER: builtins.int
    BYTES_FROM_SOURCE_FAILED_FIELD_NUMBER: builtins.int
    OBJECTS_FAILED_TO_DELETE_FROM_SINK_FIELD_NUMBER: builtins.int
    BYTES_FAILED_TO_DELETE_FROM_SINK_FIELD_NUMBER: builtins.int
    objects_found_from_source: builtins.int = ...
    """Objects found in the data source that are scheduled to be transferred,
    excluding any that are filtered based on object conditions or skipped due
    to sync.
    """

    bytes_found_from_source: builtins.int = ...
    """Bytes found in the data source that are scheduled to be transferred,
    excluding any that are filtered based on object conditions or skipped due
    to sync.
    """

    objects_found_only_from_sink: builtins.int = ...
    """Objects found only in the data sink that are scheduled to be deleted."""

    bytes_found_only_from_sink: builtins.int = ...
    """Bytes found only in the data sink that are scheduled to be deleted."""

    objects_from_source_skipped_by_sync: builtins.int = ...
    """Objects in the data source that are not transferred because they already
    exist in the data sink.
    """

    bytes_from_source_skipped_by_sync: builtins.int = ...
    """Bytes in the data source that are not transferred because they already
    exist in the data sink.
    """

    objects_copied_to_sink: builtins.int = ...
    """Objects that are copied to the data sink."""

    bytes_copied_to_sink: builtins.int = ...
    """Bytes that are copied to the data sink."""

    objects_deleted_from_source: builtins.int = ...
    """Objects that are deleted from the data source."""

    bytes_deleted_from_source: builtins.int = ...
    """Bytes that are deleted from the data source."""

    objects_deleted_from_sink: builtins.int = ...
    """Objects that are deleted from the data sink."""

    bytes_deleted_from_sink: builtins.int = ...
    """Bytes that are deleted from the data sink."""

    objects_from_source_failed: builtins.int = ...
    """Objects in the data source that failed to be transferred or that failed
    to be deleted after being transferred.
    """

    bytes_from_source_failed: builtins.int = ...
    """Bytes in the data source that failed to be transferred or that failed to
    be deleted after being transferred.
    """

    objects_failed_to_delete_from_sink: builtins.int = ...
    """Objects that failed to be deleted from the data sink."""

    bytes_failed_to_delete_from_sink: builtins.int = ...
    """Bytes that failed to be deleted from the data sink."""

    def __init__(self,
        *,
        objects_found_from_source : builtins.int = ...,
        bytes_found_from_source : builtins.int = ...,
        objects_found_only_from_sink : builtins.int = ...,
        bytes_found_only_from_sink : builtins.int = ...,
        objects_from_source_skipped_by_sync : builtins.int = ...,
        bytes_from_source_skipped_by_sync : builtins.int = ...,
        objects_copied_to_sink : builtins.int = ...,
        bytes_copied_to_sink : builtins.int = ...,
        objects_deleted_from_source : builtins.int = ...,
        bytes_deleted_from_source : builtins.int = ...,
        objects_deleted_from_sink : builtins.int = ...,
        bytes_deleted_from_sink : builtins.int = ...,
        objects_from_source_failed : builtins.int = ...,
        bytes_from_source_failed : builtins.int = ...,
        objects_failed_to_delete_from_sink : builtins.int = ...,
        bytes_failed_to_delete_from_sink : builtins.int = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["bytes_copied_to_sink",b"bytes_copied_to_sink","bytes_deleted_from_sink",b"bytes_deleted_from_sink","bytes_deleted_from_source",b"bytes_deleted_from_source","bytes_failed_to_delete_from_sink",b"bytes_failed_to_delete_from_sink","bytes_found_from_source",b"bytes_found_from_source","bytes_found_only_from_sink",b"bytes_found_only_from_sink","bytes_from_source_failed",b"bytes_from_source_failed","bytes_from_source_skipped_by_sync",b"bytes_from_source_skipped_by_sync","objects_copied_to_sink",b"objects_copied_to_sink","objects_deleted_from_sink",b"objects_deleted_from_sink","objects_deleted_from_source",b"objects_deleted_from_source","objects_failed_to_delete_from_sink",b"objects_failed_to_delete_from_sink","objects_found_from_source",b"objects_found_from_source","objects_found_only_from_sink",b"objects_found_only_from_sink","objects_from_source_failed",b"objects_from_source_failed","objects_from_source_skipped_by_sync",b"objects_from_source_skipped_by_sync"]) -> None: ...
global___TransferCounters = TransferCounters

class NotificationConfig(google.protobuf.message.Message):
    """Specification to configure notifications published to Cloud Pub/Sub.
    Notifications will be published to the customer-provided topic using the
    following `PubsubMessage.attributes`:

    * `"eventType"`: one of the
    [EventType][google.storagetransfer.v1.NotificationConfig.EventType] values
    * `"payloadFormat"`: one of the
    [PayloadFormat][google.storagetransfer.v1.NotificationConfig.PayloadFormat]
    values
    * `"projectId"`: the
    [project_id][google.storagetransfer.v1.TransferOperation.project_id] of the
    `TransferOperation`
    * `"transferJobName"`: the
    [transfer_job_name][google.storagetransfer.v1.TransferOperation.transfer_job_name]
    of the `TransferOperation`
    * `"transferOperationName"`: the
    [name][google.storagetransfer.v1.TransferOperation.name] of the
    `TransferOperation`

    The `PubsubMessage.data` will contain a
    [TransferOperation][google.storagetransfer.v1.TransferOperation] resource
    formatted according to the specified `PayloadFormat`.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class _EventType:
        ValueType = typing.NewType('ValueType', builtins.int)
        V: typing_extensions.TypeAlias = ValueType
    class _EventTypeEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_EventType.ValueType], builtins.type):
        DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor = ...
        EVENT_TYPE_UNSPECIFIED: NotificationConfig.EventType.ValueType = ...  # 0
        """Illegal value, to avoid allowing a default."""

        TRANSFER_OPERATION_SUCCESS: NotificationConfig.EventType.ValueType = ...  # 1
        """`TransferOperation` completed with status
        [SUCCESS][google.storagetransfer.v1.TransferOperation.Status.SUCCESS].
        """

        TRANSFER_OPERATION_FAILED: NotificationConfig.EventType.ValueType = ...  # 2
        """`TransferOperation` completed with status
        [FAILED][google.storagetransfer.v1.TransferOperation.Status.FAILED].
        """

        TRANSFER_OPERATION_ABORTED: NotificationConfig.EventType.ValueType = ...  # 3
        """`TransferOperation` completed with status
        [ABORTED][google.storagetransfer.v1.TransferOperation.Status.ABORTED].
        """

    class EventType(_EventType, metaclass=_EventTypeEnumTypeWrapper):
        """Enum for specifying event types for which notifications are to be
        published.

        Additional event types may be added in the future. Clients should either
        safely ignore unrecognized event types or explicitly specify which event
        types they are prepared to accept.
        """
        pass

    EVENT_TYPE_UNSPECIFIED: NotificationConfig.EventType.ValueType = ...  # 0
    """Illegal value, to avoid allowing a default."""

    TRANSFER_OPERATION_SUCCESS: NotificationConfig.EventType.ValueType = ...  # 1
    """`TransferOperation` completed with status
    [SUCCESS][google.storagetransfer.v1.TransferOperation.Status.SUCCESS].
    """

    TRANSFER_OPERATION_FAILED: NotificationConfig.EventType.ValueType = ...  # 2
    """`TransferOperation` completed with status
    [FAILED][google.storagetransfer.v1.TransferOperation.Status.FAILED].
    """

    TRANSFER_OPERATION_ABORTED: NotificationConfig.EventType.ValueType = ...  # 3
    """`TransferOperation` completed with status
    [ABORTED][google.storagetransfer.v1.TransferOperation.Status.ABORTED].
    """


    class _PayloadFormat:
        ValueType = typing.NewType('ValueType', builtins.int)
        V: typing_extensions.TypeAlias = ValueType
    class _PayloadFormatEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_PayloadFormat.ValueType], builtins.type):
        DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor = ...
        PAYLOAD_FORMAT_UNSPECIFIED: NotificationConfig.PayloadFormat.ValueType = ...  # 0
        """Illegal value, to avoid allowing a default."""

        NONE: NotificationConfig.PayloadFormat.ValueType = ...  # 1
        """No payload is included with the notification."""

        JSON: NotificationConfig.PayloadFormat.ValueType = ...  # 2
        """`TransferOperation` is [formatted as a JSON
        response](https://developers.google.com/protocol-buffers/docs/proto3#json),
        in application/json.
        """

    class PayloadFormat(_PayloadFormat, metaclass=_PayloadFormatEnumTypeWrapper):
        """Enum for specifying the format of a notification message's payload."""
        pass

    PAYLOAD_FORMAT_UNSPECIFIED: NotificationConfig.PayloadFormat.ValueType = ...  # 0
    """Illegal value, to avoid allowing a default."""

    NONE: NotificationConfig.PayloadFormat.ValueType = ...  # 1
    """No payload is included with the notification."""

    JSON: NotificationConfig.PayloadFormat.ValueType = ...  # 2
    """`TransferOperation` is [formatted as a JSON
    response](https://developers.google.com/protocol-buffers/docs/proto3#json),
    in application/json.
    """


    PUBSUB_TOPIC_FIELD_NUMBER: builtins.int
    EVENT_TYPES_FIELD_NUMBER: builtins.int
    PAYLOAD_FORMAT_FIELD_NUMBER: builtins.int
    pubsub_topic: typing.Text = ...
    """Required. The `Topic.name` of the Cloud Pub/Sub topic to which to publish
    notifications. Must be of the format: `projects/{project}/topics/{topic}`.
    Not matching this format will result in an
    [INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT] error.
    """

    @property
    def event_types(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[global___NotificationConfig.EventType.ValueType]:
        """Event types for which a notification is desired. If empty, send
        notifications for all event types.
        """
        pass
    payload_format: global___NotificationConfig.PayloadFormat.ValueType = ...
    """Required. The desired format of the notification message payloads."""

    def __init__(self,
        *,
        pubsub_topic : typing.Text = ...,
        event_types : typing.Optional[typing.Iterable[global___NotificationConfig.EventType.ValueType]] = ...,
        payload_format : global___NotificationConfig.PayloadFormat.ValueType = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["event_types",b"event_types","payload_format",b"payload_format","pubsub_topic",b"pubsub_topic"]) -> None: ...
global___NotificationConfig = NotificationConfig

class TransferOperation(google.protobuf.message.Message):
    """A description of the execution of a transfer."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class _Status:
        ValueType = typing.NewType('ValueType', builtins.int)
        V: typing_extensions.TypeAlias = ValueType
    class _StatusEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_Status.ValueType], builtins.type):
        DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor = ...
        STATUS_UNSPECIFIED: TransferOperation.Status.ValueType = ...  # 0
        """Zero is an illegal value."""

        IN_PROGRESS: TransferOperation.Status.ValueType = ...  # 1
        """In progress."""

        PAUSED: TransferOperation.Status.ValueType = ...  # 2
        """Paused."""

        SUCCESS: TransferOperation.Status.ValueType = ...  # 3
        """Completed successfully."""

        FAILED: TransferOperation.Status.ValueType = ...  # 4
        """Terminated due to an unrecoverable failure."""

        ABORTED: TransferOperation.Status.ValueType = ...  # 5
        """Aborted by the user."""

        QUEUED: TransferOperation.Status.ValueType = ...  # 6
        """Temporarily delayed by the system. No user action is required."""

    class Status(_Status, metaclass=_StatusEnumTypeWrapper):
        """The status of a TransferOperation."""
        pass

    STATUS_UNSPECIFIED: TransferOperation.Status.ValueType = ...  # 0
    """Zero is an illegal value."""

    IN_PROGRESS: TransferOperation.Status.ValueType = ...  # 1
    """In progress."""

    PAUSED: TransferOperation.Status.ValueType = ...  # 2
    """Paused."""

    SUCCESS: TransferOperation.Status.ValueType = ...  # 3
    """Completed successfully."""

    FAILED: TransferOperation.Status.ValueType = ...  # 4
    """Terminated due to an unrecoverable failure."""

    ABORTED: TransferOperation.Status.ValueType = ...  # 5
    """Aborted by the user."""

    QUEUED: TransferOperation.Status.ValueType = ...  # 6
    """Temporarily delayed by the system. No user action is required."""


    NAME_FIELD_NUMBER: builtins.int
    PROJECT_ID_FIELD_NUMBER: builtins.int
    TRANSFER_SPEC_FIELD_NUMBER: builtins.int
    NOTIFICATION_CONFIG_FIELD_NUMBER: builtins.int
    START_TIME_FIELD_NUMBER: builtins.int
    END_TIME_FIELD_NUMBER: builtins.int
    STATUS_FIELD_NUMBER: builtins.int
    COUNTERS_FIELD_NUMBER: builtins.int
    ERROR_BREAKDOWNS_FIELD_NUMBER: builtins.int
    TRANSFER_JOB_NAME_FIELD_NUMBER: builtins.int
    name: typing.Text = ...
    """A globally unique ID assigned by the system."""

    project_id: typing.Text = ...
    """The ID of the Google Cloud Platform Project that owns the operation."""

    @property
    def transfer_spec(self) -> global___TransferSpec:
        """Transfer specification."""
        pass
    @property
    def notification_config(self) -> global___NotificationConfig:
        """Notification configuration."""
        pass
    @property
    def start_time(self) -> google.protobuf.timestamp_pb2.Timestamp:
        """Start time of this transfer execution."""
        pass
    @property
    def end_time(self) -> google.protobuf.timestamp_pb2.Timestamp:
        """End time of this transfer execution."""
        pass
    status: global___TransferOperation.Status.ValueType = ...
    """Status of the transfer operation."""

    @property
    def counters(self) -> global___TransferCounters:
        """Information about the progress of the transfer operation."""
        pass
    @property
    def error_breakdowns(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___ErrorSummary]:
        """Summarizes errors encountered with sample error log entries."""
        pass
    transfer_job_name: typing.Text = ...
    """The name of the transfer job that triggers this transfer operation."""

    def __init__(self,
        *,
        name : typing.Text = ...,
        project_id : typing.Text = ...,
        transfer_spec : typing.Optional[global___TransferSpec] = ...,
        notification_config : typing.Optional[global___NotificationConfig] = ...,
        start_time : typing.Optional[google.protobuf.timestamp_pb2.Timestamp] = ...,
        end_time : typing.Optional[google.protobuf.timestamp_pb2.Timestamp] = ...,
        status : global___TransferOperation.Status.ValueType = ...,
        counters : typing.Optional[global___TransferCounters] = ...,
        error_breakdowns : typing.Optional[typing.Iterable[global___ErrorSummary]] = ...,
        transfer_job_name : typing.Text = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["counters",b"counters","end_time",b"end_time","notification_config",b"notification_config","start_time",b"start_time","transfer_spec",b"transfer_spec"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["counters",b"counters","end_time",b"end_time","error_breakdowns",b"error_breakdowns","name",b"name","notification_config",b"notification_config","project_id",b"project_id","start_time",b"start_time","status",b"status","transfer_job_name",b"transfer_job_name","transfer_spec",b"transfer_spec"]) -> None: ...
global___TransferOperation = TransferOperation
