"""
@generated by mypy-protobuf.  Do not edit manually!
isort:skip_file
"""
import builtins
import google.protobuf.any_pb2
import google.protobuf.descriptor
import google.protobuf.internal.containers
import google.protobuf.internal.enum_type_wrapper
import google.protobuf.message
import google.protobuf.struct_pb2
import typing
import typing_extensions

DESCRIPTOR: google.protobuf.descriptor.FileDescriptor = ...

class _JobType:
    ValueType = typing.NewType('ValueType', builtins.int)
    V: typing_extensions.TypeAlias = ValueType
class _JobTypeEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_JobType.ValueType], builtins.type):
    DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor = ...
    JOB_TYPE_UNKNOWN: JobType.ValueType = ...  # 0
    """The type of the job is unspecified, or unknown."""

    JOB_TYPE_BATCH: JobType.ValueType = ...  # 1
    """A batch job with a well-defined end point: data is read, data is
    processed, data is written, and the job is done.
    """

    JOB_TYPE_STREAMING: JobType.ValueType = ...  # 2
    """A continuously streaming job with no end: data is read,
    processed, and written continuously.
    """

class JobType(_JobType, metaclass=_JobTypeEnumTypeWrapper):
    """Specifies the processing model used by a
    [google.dataflow.v1beta3.Job], which determines the way the Job is
    managed by the Cloud Dataflow service (how workers are scheduled, how
    inputs are sharded, etc).
    """
    pass

JOB_TYPE_UNKNOWN: JobType.ValueType = ...  # 0
"""The type of the job is unspecified, or unknown."""

JOB_TYPE_BATCH: JobType.ValueType = ...  # 1
"""A batch job with a well-defined end point: data is read, data is
processed, data is written, and the job is done.
"""

JOB_TYPE_STREAMING: JobType.ValueType = ...  # 2
"""A continuously streaming job with no end: data is read,
processed, and written continuously.
"""

global___JobType = JobType


class _FlexResourceSchedulingGoal:
    ValueType = typing.NewType('ValueType', builtins.int)
    V: typing_extensions.TypeAlias = ValueType
class _FlexResourceSchedulingGoalEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_FlexResourceSchedulingGoal.ValueType], builtins.type):
    DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor = ...
    FLEXRS_UNSPECIFIED: FlexResourceSchedulingGoal.ValueType = ...  # 0
    """Run in the default mode."""

    FLEXRS_SPEED_OPTIMIZED: FlexResourceSchedulingGoal.ValueType = ...  # 1
    """Optimize for lower execution time."""

    FLEXRS_COST_OPTIMIZED: FlexResourceSchedulingGoal.ValueType = ...  # 2
    """Optimize for lower cost."""

class FlexResourceSchedulingGoal(_FlexResourceSchedulingGoal, metaclass=_FlexResourceSchedulingGoalEnumTypeWrapper):
    """Specifies the resource to optimize for in Flexible Resource Scheduling."""
    pass

FLEXRS_UNSPECIFIED: FlexResourceSchedulingGoal.ValueType = ...  # 0
"""Run in the default mode."""

FLEXRS_SPEED_OPTIMIZED: FlexResourceSchedulingGoal.ValueType = ...  # 1
"""Optimize for lower execution time."""

FLEXRS_COST_OPTIMIZED: FlexResourceSchedulingGoal.ValueType = ...  # 2
"""Optimize for lower cost."""

global___FlexResourceSchedulingGoal = FlexResourceSchedulingGoal


class _TeardownPolicy:
    ValueType = typing.NewType('ValueType', builtins.int)
    V: typing_extensions.TypeAlias = ValueType
class _TeardownPolicyEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_TeardownPolicy.ValueType], builtins.type):
    DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor = ...
    TEARDOWN_POLICY_UNKNOWN: TeardownPolicy.ValueType = ...  # 0
    """The teardown policy isn't specified, or is unknown."""

    TEARDOWN_ALWAYS: TeardownPolicy.ValueType = ...  # 1
    """Always teardown the resource."""

    TEARDOWN_ON_SUCCESS: TeardownPolicy.ValueType = ...  # 2
    """Teardown the resource on success. This is useful for debugging
    failures.
    """

    TEARDOWN_NEVER: TeardownPolicy.ValueType = ...  # 3
    """Never teardown the resource. This is useful for debugging and
    development.
    """

class TeardownPolicy(_TeardownPolicy, metaclass=_TeardownPolicyEnumTypeWrapper):
    """Specifies what happens to a resource when a Cloud Dataflow
    [google.dataflow.v1beta3.Job][google.dataflow.v1beta3.Job] has completed.
    """
    pass

TEARDOWN_POLICY_UNKNOWN: TeardownPolicy.ValueType = ...  # 0
"""The teardown policy isn't specified, or is unknown."""

TEARDOWN_ALWAYS: TeardownPolicy.ValueType = ...  # 1
"""Always teardown the resource."""

TEARDOWN_ON_SUCCESS: TeardownPolicy.ValueType = ...  # 2
"""Teardown the resource on success. This is useful for debugging
failures.
"""

TEARDOWN_NEVER: TeardownPolicy.ValueType = ...  # 3
"""Never teardown the resource. This is useful for debugging and
development.
"""

global___TeardownPolicy = TeardownPolicy


class _DefaultPackageSet:
    ValueType = typing.NewType('ValueType', builtins.int)
    V: typing_extensions.TypeAlias = ValueType
class _DefaultPackageSetEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_DefaultPackageSet.ValueType], builtins.type):
    DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor = ...
    DEFAULT_PACKAGE_SET_UNKNOWN: DefaultPackageSet.ValueType = ...  # 0
    """The default set of packages to stage is unknown, or unspecified."""

    DEFAULT_PACKAGE_SET_NONE: DefaultPackageSet.ValueType = ...  # 1
    """Indicates that no packages should be staged at the worker unless
    explicitly specified by the job.
    """

    DEFAULT_PACKAGE_SET_JAVA: DefaultPackageSet.ValueType = ...  # 2
    """Stage packages typically useful to workers written in Java."""

    DEFAULT_PACKAGE_SET_PYTHON: DefaultPackageSet.ValueType = ...  # 3
    """Stage packages typically useful to workers written in Python."""

class DefaultPackageSet(_DefaultPackageSet, metaclass=_DefaultPackageSetEnumTypeWrapper):
    """The default set of packages to be staged on a pool of workers."""
    pass

DEFAULT_PACKAGE_SET_UNKNOWN: DefaultPackageSet.ValueType = ...  # 0
"""The default set of packages to stage is unknown, or unspecified."""

DEFAULT_PACKAGE_SET_NONE: DefaultPackageSet.ValueType = ...  # 1
"""Indicates that no packages should be staged at the worker unless
explicitly specified by the job.
"""

DEFAULT_PACKAGE_SET_JAVA: DefaultPackageSet.ValueType = ...  # 2
"""Stage packages typically useful to workers written in Java."""

DEFAULT_PACKAGE_SET_PYTHON: DefaultPackageSet.ValueType = ...  # 3
"""Stage packages typically useful to workers written in Python."""

global___DefaultPackageSet = DefaultPackageSet


class _AutoscalingAlgorithm:
    ValueType = typing.NewType('ValueType', builtins.int)
    V: typing_extensions.TypeAlias = ValueType
class _AutoscalingAlgorithmEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_AutoscalingAlgorithm.ValueType], builtins.type):
    DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor = ...
    AUTOSCALING_ALGORITHM_UNKNOWN: AutoscalingAlgorithm.ValueType = ...  # 0
    """The algorithm is unknown, or unspecified."""

    AUTOSCALING_ALGORITHM_NONE: AutoscalingAlgorithm.ValueType = ...  # 1
    """Disable autoscaling."""

    AUTOSCALING_ALGORITHM_BASIC: AutoscalingAlgorithm.ValueType = ...  # 2
    """Increase worker count over time to reduce job execution time."""

class AutoscalingAlgorithm(_AutoscalingAlgorithm, metaclass=_AutoscalingAlgorithmEnumTypeWrapper):
    """Specifies the algorithm used to determine the number of worker
    processes to run at any given point in time, based on the amount of
    data left to process, the number of workers, and how quickly
    existing workers are processing data.
    """
    pass

AUTOSCALING_ALGORITHM_UNKNOWN: AutoscalingAlgorithm.ValueType = ...  # 0
"""The algorithm is unknown, or unspecified."""

AUTOSCALING_ALGORITHM_NONE: AutoscalingAlgorithm.ValueType = ...  # 1
"""Disable autoscaling."""

AUTOSCALING_ALGORITHM_BASIC: AutoscalingAlgorithm.ValueType = ...  # 2
"""Increase worker count over time to reduce job execution time."""

global___AutoscalingAlgorithm = AutoscalingAlgorithm


class _WorkerIPAddressConfiguration:
    ValueType = typing.NewType('ValueType', builtins.int)
    V: typing_extensions.TypeAlias = ValueType
class _WorkerIPAddressConfigurationEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_WorkerIPAddressConfiguration.ValueType], builtins.type):
    DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor = ...
    WORKER_IP_UNSPECIFIED: WorkerIPAddressConfiguration.ValueType = ...  # 0
    """The configuration is unknown, or unspecified."""

    WORKER_IP_PUBLIC: WorkerIPAddressConfiguration.ValueType = ...  # 1
    """Workers should have public IP addresses."""

    WORKER_IP_PRIVATE: WorkerIPAddressConfiguration.ValueType = ...  # 2
    """Workers should have private IP addresses."""

class WorkerIPAddressConfiguration(_WorkerIPAddressConfiguration, metaclass=_WorkerIPAddressConfigurationEnumTypeWrapper):
    """Specifies how IP addresses should be allocated to the worker machines."""
    pass

WORKER_IP_UNSPECIFIED: WorkerIPAddressConfiguration.ValueType = ...  # 0
"""The configuration is unknown, or unspecified."""

WORKER_IP_PUBLIC: WorkerIPAddressConfiguration.ValueType = ...  # 1
"""Workers should have public IP addresses."""

WORKER_IP_PRIVATE: WorkerIPAddressConfiguration.ValueType = ...  # 2
"""Workers should have private IP addresses."""

global___WorkerIPAddressConfiguration = WorkerIPAddressConfiguration


class _ShuffleMode:
    ValueType = typing.NewType('ValueType', builtins.int)
    V: typing_extensions.TypeAlias = ValueType
class _ShuffleModeEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_ShuffleMode.ValueType], builtins.type):
    DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor = ...
    SHUFFLE_MODE_UNSPECIFIED: ShuffleMode.ValueType = ...  # 0
    """Shuffle mode information is not available."""

    VM_BASED: ShuffleMode.ValueType = ...  # 1
    """Shuffle is done on the worker VMs."""

    SERVICE_BASED: ShuffleMode.ValueType = ...  # 2
    """Shuffle is done on the service side."""

class ShuffleMode(_ShuffleMode, metaclass=_ShuffleModeEnumTypeWrapper):
    """Specifies the shuffle mode used by a
    [google.dataflow.v1beta3.Job], which determines the approach data is shuffled
    during processing. More details in:
    https://cloud.google.com/dataflow/docs/guides/deploying-a-pipeline#dataflow-shuffle
    """
    pass

SHUFFLE_MODE_UNSPECIFIED: ShuffleMode.ValueType = ...  # 0
"""Shuffle mode information is not available."""

VM_BASED: ShuffleMode.ValueType = ...  # 1
"""Shuffle is done on the worker VMs."""

SERVICE_BASED: ShuffleMode.ValueType = ...  # 2
"""Shuffle is done on the service side."""

global___ShuffleMode = ShuffleMode


class Environment(google.protobuf.message.Message):
    """Describes the environment in which a Dataflow Job runs."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    TEMP_STORAGE_PREFIX_FIELD_NUMBER: builtins.int
    CLUSTER_MANAGER_API_SERVICE_FIELD_NUMBER: builtins.int
    EXPERIMENTS_FIELD_NUMBER: builtins.int
    SERVICE_OPTIONS_FIELD_NUMBER: builtins.int
    SERVICE_KMS_KEY_NAME_FIELD_NUMBER: builtins.int
    WORKER_POOLS_FIELD_NUMBER: builtins.int
    USER_AGENT_FIELD_NUMBER: builtins.int
    VERSION_FIELD_NUMBER: builtins.int
    DATASET_FIELD_NUMBER: builtins.int
    SDK_PIPELINE_OPTIONS_FIELD_NUMBER: builtins.int
    INTERNAL_EXPERIMENTS_FIELD_NUMBER: builtins.int
    SERVICE_ACCOUNT_EMAIL_FIELD_NUMBER: builtins.int
    FLEX_RESOURCE_SCHEDULING_GOAL_FIELD_NUMBER: builtins.int
    WORKER_REGION_FIELD_NUMBER: builtins.int
    WORKER_ZONE_FIELD_NUMBER: builtins.int
    SHUFFLE_MODE_FIELD_NUMBER: builtins.int
    DEBUG_OPTIONS_FIELD_NUMBER: builtins.int
    temp_storage_prefix: typing.Text = ...
    """The prefix of the resources the system should use for temporary
    storage.  The system will append the suffix "/temp-{JOBNAME} to
    this resource prefix, where {JOBNAME} is the value of the
    job_name field.  The resulting bucket and object prefix is used
    as the prefix of the resources used to store temporary data
    needed during the job execution.  NOTE: This will override the
    value in taskrunner_settings.
    The supported resource type is:

    Google Cloud Storage:

      storage.googleapis.com/{bucket}/{object}
      bucket.storage.googleapis.com/{object}
    """

    cluster_manager_api_service: typing.Text = ...
    """The type of cluster manager API to use.  If unknown or
    unspecified, the service will attempt to choose a reasonable
    default.  This should be in the form of the API service name,
    e.g. "compute.googleapis.com".
    """

    @property
    def experiments(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """The list of experiments to enable. This field should be used for SDK
        related experiments and not for service related experiments. The proper
        field for service related experiments is service_options.
        """
        pass
    @property
    def service_options(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """The list of service options to enable. This field should be used for
        service related experiments only. These experiments, when graduating to GA,
        should be replaced by dedicated fields or become default (i.e. always on).
        """
        pass
    service_kms_key_name: typing.Text = ...
    """If set, contains the Cloud KMS key identifier used to encrypt data
    at rest, AKA a Customer Managed Encryption Key (CMEK).

    Format:
      projects/PROJECT_ID/locations/LOCATION/keyRings/KEY_RING/cryptoKeys/KEY
    """

    @property
    def worker_pools(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___WorkerPool]:
        """The worker pools. At least one "harness" worker pool must be
        specified in order for the job to have workers.
        """
        pass
    @property
    def user_agent(self) -> google.protobuf.struct_pb2.Struct:
        """A description of the process that generated the request."""
        pass
    @property
    def version(self) -> google.protobuf.struct_pb2.Struct:
        """A structure describing which components and their versions of the service
        are required in order to run the job.
        """
        pass
    dataset: typing.Text = ...
    """The dataset for the current project where various workflow
    related tables are stored.

    The supported resource type is:

    Google BigQuery:
      bigquery.googleapis.com/{dataset}
    """

    @property
    def sdk_pipeline_options(self) -> google.protobuf.struct_pb2.Struct:
        """The Cloud Dataflow SDK pipeline options specified by the user. These
        options are passed through the service and are used to recreate the
        SDK pipeline options on the worker in a language agnostic and platform
        independent way.
        """
        pass
    @property
    def internal_experiments(self) -> google.protobuf.any_pb2.Any:
        """Experimental settings."""
        pass
    service_account_email: typing.Text = ...
    """Identity to run virtual machines as. Defaults to the default account."""

    flex_resource_scheduling_goal: global___FlexResourceSchedulingGoal.ValueType = ...
    """Which Flexible Resource Scheduling mode to run in."""

    worker_region: typing.Text = ...
    """The Compute Engine region
    (https://cloud.google.com/compute/docs/regions-zones/regions-zones) in
    which worker processing should occur, e.g. "us-west1". Mutually exclusive
    with worker_zone. If neither worker_region nor worker_zone is specified,
    default to the control plane's region.
    """

    worker_zone: typing.Text = ...
    """The Compute Engine zone
    (https://cloud.google.com/compute/docs/regions-zones/regions-zones) in
    which worker processing should occur, e.g. "us-west1-a". Mutually exclusive
    with worker_region. If neither worker_region nor worker_zone is specified,
    a zone in the control plane's region is chosen based on available capacity.
    """

    shuffle_mode: global___ShuffleMode.ValueType = ...
    """Output only. The shuffle mode used for the job."""

    @property
    def debug_options(self) -> global___DebugOptions:
        """Any debugging options to be supplied to the job."""
        pass
    def __init__(self,
        *,
        temp_storage_prefix : typing.Text = ...,
        cluster_manager_api_service : typing.Text = ...,
        experiments : typing.Optional[typing.Iterable[typing.Text]] = ...,
        service_options : typing.Optional[typing.Iterable[typing.Text]] = ...,
        service_kms_key_name : typing.Text = ...,
        worker_pools : typing.Optional[typing.Iterable[global___WorkerPool]] = ...,
        user_agent : typing.Optional[google.protobuf.struct_pb2.Struct] = ...,
        version : typing.Optional[google.protobuf.struct_pb2.Struct] = ...,
        dataset : typing.Text = ...,
        sdk_pipeline_options : typing.Optional[google.protobuf.struct_pb2.Struct] = ...,
        internal_experiments : typing.Optional[google.protobuf.any_pb2.Any] = ...,
        service_account_email : typing.Text = ...,
        flex_resource_scheduling_goal : global___FlexResourceSchedulingGoal.ValueType = ...,
        worker_region : typing.Text = ...,
        worker_zone : typing.Text = ...,
        shuffle_mode : global___ShuffleMode.ValueType = ...,
        debug_options : typing.Optional[global___DebugOptions] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["debug_options",b"debug_options","internal_experiments",b"internal_experiments","sdk_pipeline_options",b"sdk_pipeline_options","user_agent",b"user_agent","version",b"version"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["cluster_manager_api_service",b"cluster_manager_api_service","dataset",b"dataset","debug_options",b"debug_options","experiments",b"experiments","flex_resource_scheduling_goal",b"flex_resource_scheduling_goal","internal_experiments",b"internal_experiments","sdk_pipeline_options",b"sdk_pipeline_options","service_account_email",b"service_account_email","service_kms_key_name",b"service_kms_key_name","service_options",b"service_options","shuffle_mode",b"shuffle_mode","temp_storage_prefix",b"temp_storage_prefix","user_agent",b"user_agent","version",b"version","worker_pools",b"worker_pools","worker_region",b"worker_region","worker_zone",b"worker_zone"]) -> None: ...
global___Environment = Environment

class Package(google.protobuf.message.Message):
    """The packages that must be installed in order for a worker to run the
    steps of the Cloud Dataflow job that will be assigned to its worker
    pool.

    This is the mechanism by which the Cloud Dataflow SDK causes code to
    be loaded onto the workers. For example, the Cloud Dataflow Java SDK
    might use this to install jars containing the user's code and all of the
    various dependencies (libraries, data files, etc.) required in order
    for that code to run.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    NAME_FIELD_NUMBER: builtins.int
    LOCATION_FIELD_NUMBER: builtins.int
    name: typing.Text = ...
    """The name of the package."""

    location: typing.Text = ...
    """The resource to read the package from. The supported resource type is:

    Google Cloud Storage:

      storage.googleapis.com/{bucket}
      bucket.storage.googleapis.com/
    """

    def __init__(self,
        *,
        name : typing.Text = ...,
        location : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["location",b"location","name",b"name"]) -> None: ...
global___Package = Package

class Disk(google.protobuf.message.Message):
    """Describes the data disk used by a workflow job."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    SIZE_GB_FIELD_NUMBER: builtins.int
    DISK_TYPE_FIELD_NUMBER: builtins.int
    MOUNT_POINT_FIELD_NUMBER: builtins.int
    size_gb: builtins.int = ...
    """Size of disk in GB.  If zero or unspecified, the service will
    attempt to choose a reasonable default.
    """

    disk_type: typing.Text = ...
    """Disk storage type, as defined by Google Compute Engine.  This
    must be a disk type appropriate to the project and zone in which
    the workers will run.  If unknown or unspecified, the service
    will attempt to choose a reasonable default.

    For example, the standard persistent disk type is a resource name
    typically ending in "pd-standard".  If SSD persistent disks are
    available, the resource name typically ends with "pd-ssd".  The
    actual valid values are defined the Google Compute Engine API,
    not by the Cloud Dataflow API; consult the Google Compute Engine
    documentation for more information about determining the set of
    available disk types for a particular project and zone.

    Google Compute Engine Disk types are local to a particular
    project in a particular zone, and so the resource name will
    typically look something like this:

    compute.googleapis.com/projects/project-id/zones/zone/diskTypes/pd-standard
    """

    mount_point: typing.Text = ...
    """Directory in a VM where disk is mounted."""

    def __init__(self,
        *,
        size_gb : builtins.int = ...,
        disk_type : typing.Text = ...,
        mount_point : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["disk_type",b"disk_type","mount_point",b"mount_point","size_gb",b"size_gb"]) -> None: ...
global___Disk = Disk

class WorkerSettings(google.protobuf.message.Message):
    """Provides data to pass through to the worker harness."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    BASE_URL_FIELD_NUMBER: builtins.int
    REPORTING_ENABLED_FIELD_NUMBER: builtins.int
    SERVICE_PATH_FIELD_NUMBER: builtins.int
    SHUFFLE_SERVICE_PATH_FIELD_NUMBER: builtins.int
    WORKER_ID_FIELD_NUMBER: builtins.int
    TEMP_STORAGE_PREFIX_FIELD_NUMBER: builtins.int
    base_url: typing.Text = ...
    """The base URL for accessing Google Cloud APIs.

    When workers access Google Cloud APIs, they logically do so via
    relative URLs.  If this field is specified, it supplies the base
    URL to use for resolving these relative URLs.  The normative
    algorithm used is defined by RFC 1808, "Relative Uniform Resource
    Locators".

    If not specified, the default value is "http://www.googleapis.com/"
    """

    reporting_enabled: builtins.bool = ...
    """Whether to send work progress updates to the service."""

    service_path: typing.Text = ...
    """The Cloud Dataflow service path relative to the root URL, for example,
    "dataflow/v1b3/projects".
    """

    shuffle_service_path: typing.Text = ...
    """The Shuffle service path relative to the root URL, for example,
    "shuffle/v1beta1".
    """

    worker_id: typing.Text = ...
    """The ID of the worker running this pipeline."""

    temp_storage_prefix: typing.Text = ...
    """The prefix of the resources the system should use for temporary
    storage.

    The supported resource type is:

    Google Cloud Storage:

      storage.googleapis.com/{bucket}/{object}
      bucket.storage.googleapis.com/{object}
    """

    def __init__(self,
        *,
        base_url : typing.Text = ...,
        reporting_enabled : builtins.bool = ...,
        service_path : typing.Text = ...,
        shuffle_service_path : typing.Text = ...,
        worker_id : typing.Text = ...,
        temp_storage_prefix : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["base_url",b"base_url","reporting_enabled",b"reporting_enabled","service_path",b"service_path","shuffle_service_path",b"shuffle_service_path","temp_storage_prefix",b"temp_storage_prefix","worker_id",b"worker_id"]) -> None: ...
global___WorkerSettings = WorkerSettings

class TaskRunnerSettings(google.protobuf.message.Message):
    """Taskrunner configuration settings."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    TASK_USER_FIELD_NUMBER: builtins.int
    TASK_GROUP_FIELD_NUMBER: builtins.int
    OAUTH_SCOPES_FIELD_NUMBER: builtins.int
    BASE_URL_FIELD_NUMBER: builtins.int
    DATAFLOW_API_VERSION_FIELD_NUMBER: builtins.int
    PARALLEL_WORKER_SETTINGS_FIELD_NUMBER: builtins.int
    BASE_TASK_DIR_FIELD_NUMBER: builtins.int
    CONTINUE_ON_EXCEPTION_FIELD_NUMBER: builtins.int
    LOG_TO_SERIALCONSOLE_FIELD_NUMBER: builtins.int
    ALSOLOGTOSTDERR_FIELD_NUMBER: builtins.int
    LOG_UPLOAD_LOCATION_FIELD_NUMBER: builtins.int
    LOG_DIR_FIELD_NUMBER: builtins.int
    TEMP_STORAGE_PREFIX_FIELD_NUMBER: builtins.int
    HARNESS_COMMAND_FIELD_NUMBER: builtins.int
    WORKFLOW_FILE_NAME_FIELD_NUMBER: builtins.int
    COMMANDLINES_FILE_NAME_FIELD_NUMBER: builtins.int
    VM_ID_FIELD_NUMBER: builtins.int
    LANGUAGE_HINT_FIELD_NUMBER: builtins.int
    STREAMING_WORKER_MAIN_CLASS_FIELD_NUMBER: builtins.int
    task_user: typing.Text = ...
    """The UNIX user ID on the worker VM to use for tasks launched by
    taskrunner; e.g. "root".
    """

    task_group: typing.Text = ...
    """The UNIX group ID on the worker VM to use for tasks launched by
    taskrunner; e.g. "wheel".
    """

    @property
    def oauth_scopes(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """The OAuth2 scopes to be requested by the taskrunner in order to
        access the Cloud Dataflow API.
        """
        pass
    base_url: typing.Text = ...
    """The base URL for the taskrunner to use when accessing Google Cloud APIs.

    When workers access Google Cloud APIs, they logically do so via
    relative URLs.  If this field is specified, it supplies the base
    URL to use for resolving these relative URLs.  The normative
    algorithm used is defined by RFC 1808, "Relative Uniform Resource
    Locators".

    If not specified, the default value is "http://www.googleapis.com/"
    """

    dataflow_api_version: typing.Text = ...
    """The API version of endpoint, e.g. "v1b3" """

    @property
    def parallel_worker_settings(self) -> global___WorkerSettings:
        """The settings to pass to the parallel worker harness."""
        pass
    base_task_dir: typing.Text = ...
    """The location on the worker for task-specific subdirectories."""

    continue_on_exception: builtins.bool = ...
    """Whether to continue taskrunner if an exception is hit."""

    log_to_serialconsole: builtins.bool = ...
    """Whether to send taskrunner log info to Google Compute Engine VM serial
    console.
    """

    alsologtostderr: builtins.bool = ...
    """Whether to also send taskrunner log info to stderr."""

    log_upload_location: typing.Text = ...
    """Indicates where to put logs.  If this is not specified, the logs
    will not be uploaded.

    The supported resource type is:

    Google Cloud Storage:
      storage.googleapis.com/{bucket}/{object}
      bucket.storage.googleapis.com/{object}
    """

    log_dir: typing.Text = ...
    """The directory on the VM to store logs."""

    temp_storage_prefix: typing.Text = ...
    """The prefix of the resources the taskrunner should use for
    temporary storage.

    The supported resource type is:

    Google Cloud Storage:
      storage.googleapis.com/{bucket}/{object}
      bucket.storage.googleapis.com/{object}
    """

    harness_command: typing.Text = ...
    """The command to launch the worker harness."""

    workflow_file_name: typing.Text = ...
    """The file to store the workflow in."""

    commandlines_file_name: typing.Text = ...
    """The file to store preprocessing commands in."""

    vm_id: typing.Text = ...
    """The ID string of the VM."""

    language_hint: typing.Text = ...
    """The suggested backend language."""

    streaming_worker_main_class: typing.Text = ...
    """The streaming worker main class name."""

    def __init__(self,
        *,
        task_user : typing.Text = ...,
        task_group : typing.Text = ...,
        oauth_scopes : typing.Optional[typing.Iterable[typing.Text]] = ...,
        base_url : typing.Text = ...,
        dataflow_api_version : typing.Text = ...,
        parallel_worker_settings : typing.Optional[global___WorkerSettings] = ...,
        base_task_dir : typing.Text = ...,
        continue_on_exception : builtins.bool = ...,
        log_to_serialconsole : builtins.bool = ...,
        alsologtostderr : builtins.bool = ...,
        log_upload_location : typing.Text = ...,
        log_dir : typing.Text = ...,
        temp_storage_prefix : typing.Text = ...,
        harness_command : typing.Text = ...,
        workflow_file_name : typing.Text = ...,
        commandlines_file_name : typing.Text = ...,
        vm_id : typing.Text = ...,
        language_hint : typing.Text = ...,
        streaming_worker_main_class : typing.Text = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["parallel_worker_settings",b"parallel_worker_settings"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["alsologtostderr",b"alsologtostderr","base_task_dir",b"base_task_dir","base_url",b"base_url","commandlines_file_name",b"commandlines_file_name","continue_on_exception",b"continue_on_exception","dataflow_api_version",b"dataflow_api_version","harness_command",b"harness_command","language_hint",b"language_hint","log_dir",b"log_dir","log_to_serialconsole",b"log_to_serialconsole","log_upload_location",b"log_upload_location","oauth_scopes",b"oauth_scopes","parallel_worker_settings",b"parallel_worker_settings","streaming_worker_main_class",b"streaming_worker_main_class","task_group",b"task_group","task_user",b"task_user","temp_storage_prefix",b"temp_storage_prefix","vm_id",b"vm_id","workflow_file_name",b"workflow_file_name"]) -> None: ...
global___TaskRunnerSettings = TaskRunnerSettings

class AutoscalingSettings(google.protobuf.message.Message):
    """Settings for WorkerPool autoscaling."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    ALGORITHM_FIELD_NUMBER: builtins.int
    MAX_NUM_WORKERS_FIELD_NUMBER: builtins.int
    algorithm: global___AutoscalingAlgorithm.ValueType = ...
    """The algorithm to use for autoscaling."""

    max_num_workers: builtins.int = ...
    """The maximum number of workers to cap scaling at."""

    def __init__(self,
        *,
        algorithm : global___AutoscalingAlgorithm.ValueType = ...,
        max_num_workers : builtins.int = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["algorithm",b"algorithm","max_num_workers",b"max_num_workers"]) -> None: ...
global___AutoscalingSettings = AutoscalingSettings

class SdkHarnessContainerImage(google.protobuf.message.Message):
    """Defines a SDK harness container for executing Dataflow pipelines."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    CONTAINER_IMAGE_FIELD_NUMBER: builtins.int
    USE_SINGLE_CORE_PER_CONTAINER_FIELD_NUMBER: builtins.int
    ENVIRONMENT_ID_FIELD_NUMBER: builtins.int
    container_image: typing.Text = ...
    """A docker container image that resides in Google Container Registry."""

    use_single_core_per_container: builtins.bool = ...
    """If true, recommends the Dataflow service to use only one core per SDK
    container instance with this image. If false (or unset) recommends using
    more than one core per SDK container instance with this image for
    efficiency. Note that Dataflow service may choose to override this property
    if needed.
    """

    environment_id: typing.Text = ...
    """Environment ID for the Beam runner API proto Environment that corresponds
    to the current SDK Harness.
    """

    def __init__(self,
        *,
        container_image : typing.Text = ...,
        use_single_core_per_container : builtins.bool = ...,
        environment_id : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["container_image",b"container_image","environment_id",b"environment_id","use_single_core_per_container",b"use_single_core_per_container"]) -> None: ...
global___SdkHarnessContainerImage = SdkHarnessContainerImage

class WorkerPool(google.protobuf.message.Message):
    """Describes one particular pool of Cloud Dataflow workers to be
    instantiated by the Cloud Dataflow service in order to perform the
    computations required by a job.  Note that a workflow job may use
    multiple pools, in order to match the various computational
    requirements of the various stages of the job.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class MetadataEntry(google.protobuf.message.Message):
        DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
        KEY_FIELD_NUMBER: builtins.int
        VALUE_FIELD_NUMBER: builtins.int
        key: typing.Text = ...
        value: typing.Text = ...
        def __init__(self,
            *,
            key : typing.Text = ...,
            value : typing.Text = ...,
            ) -> None: ...
        def ClearField(self, field_name: typing_extensions.Literal["key",b"key","value",b"value"]) -> None: ...

    KIND_FIELD_NUMBER: builtins.int
    NUM_WORKERS_FIELD_NUMBER: builtins.int
    PACKAGES_FIELD_NUMBER: builtins.int
    DEFAULT_PACKAGE_SET_FIELD_NUMBER: builtins.int
    MACHINE_TYPE_FIELD_NUMBER: builtins.int
    TEARDOWN_POLICY_FIELD_NUMBER: builtins.int
    DISK_SIZE_GB_FIELD_NUMBER: builtins.int
    DISK_TYPE_FIELD_NUMBER: builtins.int
    DISK_SOURCE_IMAGE_FIELD_NUMBER: builtins.int
    ZONE_FIELD_NUMBER: builtins.int
    TASKRUNNER_SETTINGS_FIELD_NUMBER: builtins.int
    ON_HOST_MAINTENANCE_FIELD_NUMBER: builtins.int
    DATA_DISKS_FIELD_NUMBER: builtins.int
    METADATA_FIELD_NUMBER: builtins.int
    AUTOSCALING_SETTINGS_FIELD_NUMBER: builtins.int
    POOL_ARGS_FIELD_NUMBER: builtins.int
    NETWORK_FIELD_NUMBER: builtins.int
    SUBNETWORK_FIELD_NUMBER: builtins.int
    WORKER_HARNESS_CONTAINER_IMAGE_FIELD_NUMBER: builtins.int
    NUM_THREADS_PER_WORKER_FIELD_NUMBER: builtins.int
    IP_CONFIGURATION_FIELD_NUMBER: builtins.int
    SDK_HARNESS_CONTAINER_IMAGES_FIELD_NUMBER: builtins.int
    kind: typing.Text = ...
    """The kind of the worker pool; currently only `harness` and `shuffle`
    are supported.
    """

    num_workers: builtins.int = ...
    """Number of Google Compute Engine workers in this pool needed to
    execute the job.  If zero or unspecified, the service will
    attempt to choose a reasonable default.
    """

    @property
    def packages(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___Package]:
        """Packages to be installed on workers."""
        pass
    default_package_set: global___DefaultPackageSet.ValueType = ...
    """The default package set to install.  This allows the service to
    select a default set of packages which are useful to worker
    harnesses written in a particular language.
    """

    machine_type: typing.Text = ...
    """Machine type (e.g. "n1-standard-1").  If empty or unspecified, the
    service will attempt to choose a reasonable default.
    """

    teardown_policy: global___TeardownPolicy.ValueType = ...
    """Sets the policy for determining when to turndown worker pool.
    Allowed values are: `TEARDOWN_ALWAYS`, `TEARDOWN_ON_SUCCESS`, and
    `TEARDOWN_NEVER`.
    `TEARDOWN_ALWAYS` means workers are always torn down regardless of whether
    the job succeeds. `TEARDOWN_ON_SUCCESS` means workers are torn down
    if the job succeeds. `TEARDOWN_NEVER` means the workers are never torn
    down.

    If the workers are not torn down by the service, they will
    continue to run and use Google Compute Engine VM resources in the
    user's project until they are explicitly terminated by the user.
    Because of this, Google recommends using the `TEARDOWN_ALWAYS`
    policy except for small, manually supervised test jobs.

    If unknown or unspecified, the service will attempt to choose a reasonable
    default.
    """

    disk_size_gb: builtins.int = ...
    """Size of root disk for VMs, in GB.  If zero or unspecified, the service will
    attempt to choose a reasonable default.
    """

    disk_type: typing.Text = ...
    """Type of root disk for VMs.  If empty or unspecified, the service will
    attempt to choose a reasonable default.
    """

    disk_source_image: typing.Text = ...
    """Fully qualified source image for disks."""

    zone: typing.Text = ...
    """Zone to run the worker pools in.  If empty or unspecified, the service
    will attempt to choose a reasonable default.
    """

    @property
    def taskrunner_settings(self) -> global___TaskRunnerSettings:
        """Settings passed through to Google Compute Engine workers when
        using the standard Dataflow task runner.  Users should ignore
        this field.
        """
        pass
    on_host_maintenance: typing.Text = ...
    """The action to take on host maintenance, as defined by the Google
    Compute Engine API.
    """

    @property
    def data_disks(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___Disk]:
        """Data disks that are used by a VM in this workflow."""
        pass
    @property
    def metadata(self) -> google.protobuf.internal.containers.ScalarMap[typing.Text, typing.Text]:
        """Metadata to set on the Google Compute Engine VMs."""
        pass
    @property
    def autoscaling_settings(self) -> global___AutoscalingSettings:
        """Settings for autoscaling of this WorkerPool."""
        pass
    @property
    def pool_args(self) -> google.protobuf.any_pb2.Any:
        """Extra arguments for this worker pool."""
        pass
    network: typing.Text = ...
    """Network to which VMs will be assigned.  If empty or unspecified,
    the service will use the network "default".
    """

    subnetwork: typing.Text = ...
    """Subnetwork to which VMs will be assigned, if desired.  Expected to be of
    the form "regions/REGION/subnetworks/SUBNETWORK".
    """

    worker_harness_container_image: typing.Text = ...
    """Required. Docker container image that executes the Cloud Dataflow worker
    harness, residing in Google Container Registry.

    Deprecated for the Fn API path. Use sdk_harness_container_images instead.
    """

    num_threads_per_worker: builtins.int = ...
    """The number of threads per worker harness. If empty or unspecified, the
    service will choose a number of threads (according to the number of cores
    on the selected machine type for batch, or 1 by convention for streaming).
    """

    ip_configuration: global___WorkerIPAddressConfiguration.ValueType = ...
    """Configuration for VM IPs."""

    @property
    def sdk_harness_container_images(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___SdkHarnessContainerImage]:
        """Set of SDK harness containers needed to execute this pipeline. This will
        only be set in the Fn API path. For non-cross-language pipelines this
        should have only one entry. Cross-language pipelines will have two or more
        entries.
        """
        pass
    def __init__(self,
        *,
        kind : typing.Text = ...,
        num_workers : builtins.int = ...,
        packages : typing.Optional[typing.Iterable[global___Package]] = ...,
        default_package_set : global___DefaultPackageSet.ValueType = ...,
        machine_type : typing.Text = ...,
        teardown_policy : global___TeardownPolicy.ValueType = ...,
        disk_size_gb : builtins.int = ...,
        disk_type : typing.Text = ...,
        disk_source_image : typing.Text = ...,
        zone : typing.Text = ...,
        taskrunner_settings : typing.Optional[global___TaskRunnerSettings] = ...,
        on_host_maintenance : typing.Text = ...,
        data_disks : typing.Optional[typing.Iterable[global___Disk]] = ...,
        metadata : typing.Optional[typing.Mapping[typing.Text, typing.Text]] = ...,
        autoscaling_settings : typing.Optional[global___AutoscalingSettings] = ...,
        pool_args : typing.Optional[google.protobuf.any_pb2.Any] = ...,
        network : typing.Text = ...,
        subnetwork : typing.Text = ...,
        worker_harness_container_image : typing.Text = ...,
        num_threads_per_worker : builtins.int = ...,
        ip_configuration : global___WorkerIPAddressConfiguration.ValueType = ...,
        sdk_harness_container_images : typing.Optional[typing.Iterable[global___SdkHarnessContainerImage]] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["autoscaling_settings",b"autoscaling_settings","pool_args",b"pool_args","taskrunner_settings",b"taskrunner_settings"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["autoscaling_settings",b"autoscaling_settings","data_disks",b"data_disks","default_package_set",b"default_package_set","disk_size_gb",b"disk_size_gb","disk_source_image",b"disk_source_image","disk_type",b"disk_type","ip_configuration",b"ip_configuration","kind",b"kind","machine_type",b"machine_type","metadata",b"metadata","network",b"network","num_threads_per_worker",b"num_threads_per_worker","num_workers",b"num_workers","on_host_maintenance",b"on_host_maintenance","packages",b"packages","pool_args",b"pool_args","sdk_harness_container_images",b"sdk_harness_container_images","subnetwork",b"subnetwork","taskrunner_settings",b"taskrunner_settings","teardown_policy",b"teardown_policy","worker_harness_container_image",b"worker_harness_container_image","zone",b"zone"]) -> None: ...
global___WorkerPool = WorkerPool

class DebugOptions(google.protobuf.message.Message):
    """Describes any options that have an effect on the debugging of pipelines."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    ENABLE_HOT_KEY_LOGGING_FIELD_NUMBER: builtins.int
    enable_hot_key_logging: builtins.bool = ...
    """When true, enables the logging of the literal hot key to the user's Cloud
    Logging.
    """

    def __init__(self,
        *,
        enable_hot_key_logging : builtins.bool = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["enable_hot_key_logging",b"enable_hot_key_logging"]) -> None: ...
global___DebugOptions = DebugOptions
