"""
@generated by mypy-protobuf.  Do not edit manually!
isort:skip_file
"""
import builtins
import google.protobuf.descriptor
import google.protobuf.field_mask_pb2
import google.protobuf.internal.containers
import google.protobuf.internal.enum_type_wrapper
import google.protobuf.message
import google.protobuf.timestamp_pb2
import typing
import typing_extensions

DESCRIPTOR: google.protobuf.descriptor.FileDescriptor = ...

class LoggingConfig(google.protobuf.message.Message):
    """The runtime logging config of the job."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class _Level:
        ValueType = typing.NewType('ValueType', builtins.int)
        V: typing_extensions.TypeAlias = ValueType
    class _LevelEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_Level.ValueType], builtins.type):
        DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor = ...
        LEVEL_UNSPECIFIED: LoggingConfig.Level.ValueType = ...  # 0
        """Level is unspecified. Use default level for log4j."""

        ALL: LoggingConfig.Level.ValueType = ...  # 1
        """Use ALL level for log4j."""

        TRACE: LoggingConfig.Level.ValueType = ...  # 2
        """Use TRACE level for log4j."""

        DEBUG: LoggingConfig.Level.ValueType = ...  # 3
        """Use DEBUG level for log4j."""

        INFO: LoggingConfig.Level.ValueType = ...  # 4
        """Use INFO level for log4j."""

        WARN: LoggingConfig.Level.ValueType = ...  # 5
        """Use WARN level for log4j."""

        ERROR: LoggingConfig.Level.ValueType = ...  # 6
        """Use ERROR level for log4j."""

        FATAL: LoggingConfig.Level.ValueType = ...  # 7
        """Use FATAL level for log4j."""

        OFF: LoggingConfig.Level.ValueType = ...  # 8
        """Turn off log4j."""

    class Level(_Level, metaclass=_LevelEnumTypeWrapper):
        """The Log4j level for job execution. When running an
        [Apache Hive](https://hive.apache.org/) job, Cloud
        Dataproc configures the Hive client to an equivalent verbosity level.
        """
        pass

    LEVEL_UNSPECIFIED: LoggingConfig.Level.ValueType = ...  # 0
    """Level is unspecified. Use default level for log4j."""

    ALL: LoggingConfig.Level.ValueType = ...  # 1
    """Use ALL level for log4j."""

    TRACE: LoggingConfig.Level.ValueType = ...  # 2
    """Use TRACE level for log4j."""

    DEBUG: LoggingConfig.Level.ValueType = ...  # 3
    """Use DEBUG level for log4j."""

    INFO: LoggingConfig.Level.ValueType = ...  # 4
    """Use INFO level for log4j."""

    WARN: LoggingConfig.Level.ValueType = ...  # 5
    """Use WARN level for log4j."""

    ERROR: LoggingConfig.Level.ValueType = ...  # 6
    """Use ERROR level for log4j."""

    FATAL: LoggingConfig.Level.ValueType = ...  # 7
    """Use FATAL level for log4j."""

    OFF: LoggingConfig.Level.ValueType = ...  # 8
    """Turn off log4j."""


    class DriverLogLevelsEntry(google.protobuf.message.Message):
        DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
        KEY_FIELD_NUMBER: builtins.int
        VALUE_FIELD_NUMBER: builtins.int
        key: typing.Text = ...
        value: global___LoggingConfig.Level.ValueType = ...
        def __init__(self,
            *,
            key : typing.Text = ...,
            value : global___LoggingConfig.Level.ValueType = ...,
            ) -> None: ...
        def ClearField(self, field_name: typing_extensions.Literal["key",b"key","value",b"value"]) -> None: ...

    DRIVER_LOG_LEVELS_FIELD_NUMBER: builtins.int
    @property
    def driver_log_levels(self) -> google.protobuf.internal.containers.ScalarMap[typing.Text, global___LoggingConfig.Level.ValueType]:
        """The per-package log levels for the driver. This may include
        "root" package name to configure rootLogger.
        Examples:
          'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
        """
        pass
    def __init__(self,
        *,
        driver_log_levels : typing.Optional[typing.Mapping[typing.Text, global___LoggingConfig.Level.ValueType]] = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["driver_log_levels",b"driver_log_levels"]) -> None: ...
global___LoggingConfig = LoggingConfig

class HadoopJob(google.protobuf.message.Message):
    """A Dataproc job for running
    [Apache Hadoop
    MapReduce](https://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html)
    jobs on [Apache Hadoop
    YARN](https://hadoop.apache.org/docs/r2.7.1/hadoop-yarn/hadoop-yarn-site/YARN.html).
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class PropertiesEntry(google.protobuf.message.Message):
        DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
        KEY_FIELD_NUMBER: builtins.int
        VALUE_FIELD_NUMBER: builtins.int
        key: typing.Text = ...
        value: typing.Text = ...
        def __init__(self,
            *,
            key : typing.Text = ...,
            value : typing.Text = ...,
            ) -> None: ...
        def ClearField(self, field_name: typing_extensions.Literal["key",b"key","value",b"value"]) -> None: ...

    MAIN_JAR_FILE_URI_FIELD_NUMBER: builtins.int
    MAIN_CLASS_FIELD_NUMBER: builtins.int
    ARGS_FIELD_NUMBER: builtins.int
    JAR_FILE_URIS_FIELD_NUMBER: builtins.int
    FILE_URIS_FIELD_NUMBER: builtins.int
    ARCHIVE_URIS_FIELD_NUMBER: builtins.int
    PROPERTIES_FIELD_NUMBER: builtins.int
    LOGGING_CONFIG_FIELD_NUMBER: builtins.int
    main_jar_file_uri: typing.Text = ...
    """The HCFS URI of the jar file containing the main class.
    Examples:
        'gs://foo-bucket/analytics-binaries/extract-useful-metrics-mr.jar'
        'hdfs:/tmp/test-samples/custom-wordcount.jar'
        'file:///home/usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'
    """

    main_class: typing.Text = ...
    """The name of the driver's main class. The jar file containing the class
    must be in the default CLASSPATH or specified in `jar_file_uris`.
    """

    @property
    def args(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """Optional. The arguments to pass to the driver. Do not
        include arguments, such as `-libjars` or `-Dfoo=bar`, that can be set as
        job properties, since a collision may occur that causes an incorrect job
        submission.
        """
        pass
    @property
    def jar_file_uris(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """Optional. Jar file URIs to add to the CLASSPATHs of the
        Hadoop driver and tasks.
        """
        pass
    @property
    def file_uris(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """Optional. HCFS (Hadoop Compatible Filesystem) URIs of files to be copied
        to the working directory of Hadoop drivers and distributed tasks. Useful
        for naively parallel tasks.
        """
        pass
    @property
    def archive_uris(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """Optional. HCFS URIs of archives to be extracted in the working directory of
        Hadoop drivers and tasks. Supported file types:
        .jar, .tar, .tar.gz, .tgz, or .zip.
        """
        pass
    @property
    def properties(self) -> google.protobuf.internal.containers.ScalarMap[typing.Text, typing.Text]:
        """Optional. A mapping of property names to values, used to configure Hadoop.
        Properties that conflict with values set by the Dataproc API may be
        overwritten. Can include properties set in /etc/hadoop/conf/*-site and
        classes in user code.
        """
        pass
    @property
    def logging_config(self) -> global___LoggingConfig:
        """Optional. The runtime log config for job execution."""
        pass
    def __init__(self,
        *,
        main_jar_file_uri : typing.Text = ...,
        main_class : typing.Text = ...,
        args : typing.Optional[typing.Iterable[typing.Text]] = ...,
        jar_file_uris : typing.Optional[typing.Iterable[typing.Text]] = ...,
        file_uris : typing.Optional[typing.Iterable[typing.Text]] = ...,
        archive_uris : typing.Optional[typing.Iterable[typing.Text]] = ...,
        properties : typing.Optional[typing.Mapping[typing.Text, typing.Text]] = ...,
        logging_config : typing.Optional[global___LoggingConfig] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["driver",b"driver","logging_config",b"logging_config","main_class",b"main_class","main_jar_file_uri",b"main_jar_file_uri"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["archive_uris",b"archive_uris","args",b"args","driver",b"driver","file_uris",b"file_uris","jar_file_uris",b"jar_file_uris","logging_config",b"logging_config","main_class",b"main_class","main_jar_file_uri",b"main_jar_file_uri","properties",b"properties"]) -> None: ...
    def WhichOneof(self, oneof_group: typing_extensions.Literal["driver",b"driver"]) -> typing.Optional[typing_extensions.Literal["main_jar_file_uri","main_class"]]: ...
global___HadoopJob = HadoopJob

class SparkJob(google.protobuf.message.Message):
    """A Dataproc job for running [Apache Spark](http://spark.apache.org/)
    applications on YARN.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class PropertiesEntry(google.protobuf.message.Message):
        DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
        KEY_FIELD_NUMBER: builtins.int
        VALUE_FIELD_NUMBER: builtins.int
        key: typing.Text = ...
        value: typing.Text = ...
        def __init__(self,
            *,
            key : typing.Text = ...,
            value : typing.Text = ...,
            ) -> None: ...
        def ClearField(self, field_name: typing_extensions.Literal["key",b"key","value",b"value"]) -> None: ...

    MAIN_JAR_FILE_URI_FIELD_NUMBER: builtins.int
    MAIN_CLASS_FIELD_NUMBER: builtins.int
    ARGS_FIELD_NUMBER: builtins.int
    JAR_FILE_URIS_FIELD_NUMBER: builtins.int
    FILE_URIS_FIELD_NUMBER: builtins.int
    ARCHIVE_URIS_FIELD_NUMBER: builtins.int
    PROPERTIES_FIELD_NUMBER: builtins.int
    LOGGING_CONFIG_FIELD_NUMBER: builtins.int
    main_jar_file_uri: typing.Text = ...
    """The HCFS URI of the jar file that contains the main class."""

    main_class: typing.Text = ...
    """The name of the driver's main class. The jar file that contains the class
    must be in the default CLASSPATH or specified in `jar_file_uris`.
    """

    @property
    def args(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """Optional. The arguments to pass to the driver. Do not include arguments,
        such as `--conf`, that can be set as job properties, since a collision may
        occur that causes an incorrect job submission.
        """
        pass
    @property
    def jar_file_uris(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """Optional. HCFS URIs of jar files to add to the CLASSPATHs of the
        Spark driver and tasks.
        """
        pass
    @property
    def file_uris(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """Optional. HCFS URIs of files to be placed in the working directory of
        each executor. Useful for naively parallel tasks.
        """
        pass
    @property
    def archive_uris(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """Optional. HCFS URIs of archives to be extracted into the working directory
        of each executor. Supported file types:
        .jar, .tar, .tar.gz, .tgz, and .zip.
        """
        pass
    @property
    def properties(self) -> google.protobuf.internal.containers.ScalarMap[typing.Text, typing.Text]:
        """Optional. A mapping of property names to values, used to configure Spark.
        Properties that conflict with values set by the Dataproc API may be
        overwritten. Can include properties set in
        /etc/spark/conf/spark-defaults.conf and classes in user code.
        """
        pass
    @property
    def logging_config(self) -> global___LoggingConfig:
        """Optional. The runtime log config for job execution."""
        pass
    def __init__(self,
        *,
        main_jar_file_uri : typing.Text = ...,
        main_class : typing.Text = ...,
        args : typing.Optional[typing.Iterable[typing.Text]] = ...,
        jar_file_uris : typing.Optional[typing.Iterable[typing.Text]] = ...,
        file_uris : typing.Optional[typing.Iterable[typing.Text]] = ...,
        archive_uris : typing.Optional[typing.Iterable[typing.Text]] = ...,
        properties : typing.Optional[typing.Mapping[typing.Text, typing.Text]] = ...,
        logging_config : typing.Optional[global___LoggingConfig] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["driver",b"driver","logging_config",b"logging_config","main_class",b"main_class","main_jar_file_uri",b"main_jar_file_uri"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["archive_uris",b"archive_uris","args",b"args","driver",b"driver","file_uris",b"file_uris","jar_file_uris",b"jar_file_uris","logging_config",b"logging_config","main_class",b"main_class","main_jar_file_uri",b"main_jar_file_uri","properties",b"properties"]) -> None: ...
    def WhichOneof(self, oneof_group: typing_extensions.Literal["driver",b"driver"]) -> typing.Optional[typing_extensions.Literal["main_jar_file_uri","main_class"]]: ...
global___SparkJob = SparkJob

class PySparkJob(google.protobuf.message.Message):
    """A Dataproc job for running
    [Apache
    PySpark](https://spark.apache.org/docs/0.9.0/python-programming-guide.html)
    applications on YARN.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class PropertiesEntry(google.protobuf.message.Message):
        DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
        KEY_FIELD_NUMBER: builtins.int
        VALUE_FIELD_NUMBER: builtins.int
        key: typing.Text = ...
        value: typing.Text = ...
        def __init__(self,
            *,
            key : typing.Text = ...,
            value : typing.Text = ...,
            ) -> None: ...
        def ClearField(self, field_name: typing_extensions.Literal["key",b"key","value",b"value"]) -> None: ...

    MAIN_PYTHON_FILE_URI_FIELD_NUMBER: builtins.int
    ARGS_FIELD_NUMBER: builtins.int
    PYTHON_FILE_URIS_FIELD_NUMBER: builtins.int
    JAR_FILE_URIS_FIELD_NUMBER: builtins.int
    FILE_URIS_FIELD_NUMBER: builtins.int
    ARCHIVE_URIS_FIELD_NUMBER: builtins.int
    PROPERTIES_FIELD_NUMBER: builtins.int
    LOGGING_CONFIG_FIELD_NUMBER: builtins.int
    main_python_file_uri: typing.Text = ...
    """Required. The HCFS URI of the main Python file to use as the driver. Must
    be a .py file.
    """

    @property
    def args(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """Optional. The arguments to pass to the driver.  Do not include arguments,
        such as `--conf`, that can be set as job properties, since a collision may
        occur that causes an incorrect job submission.
        """
        pass
    @property
    def python_file_uris(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """Optional. HCFS file URIs of Python files to pass to the PySpark
        framework. Supported file types: .py, .egg, and .zip.
        """
        pass
    @property
    def jar_file_uris(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """Optional. HCFS URIs of jar files to add to the CLASSPATHs of the
        Python driver and tasks.
        """
        pass
    @property
    def file_uris(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """Optional. HCFS URIs of files to be placed in the working directory of
        each executor. Useful for naively parallel tasks.
        """
        pass
    @property
    def archive_uris(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """Optional. HCFS URIs of archives to be extracted into the working directory
        of each executor. Supported file types:
        .jar, .tar, .tar.gz, .tgz, and .zip.
        """
        pass
    @property
    def properties(self) -> google.protobuf.internal.containers.ScalarMap[typing.Text, typing.Text]:
        """Optional. A mapping of property names to values, used to configure PySpark.
        Properties that conflict with values set by the Dataproc API may be
        overwritten. Can include properties set in
        /etc/spark/conf/spark-defaults.conf and classes in user code.
        """
        pass
    @property
    def logging_config(self) -> global___LoggingConfig:
        """Optional. The runtime log config for job execution."""
        pass
    def __init__(self,
        *,
        main_python_file_uri : typing.Text = ...,
        args : typing.Optional[typing.Iterable[typing.Text]] = ...,
        python_file_uris : typing.Optional[typing.Iterable[typing.Text]] = ...,
        jar_file_uris : typing.Optional[typing.Iterable[typing.Text]] = ...,
        file_uris : typing.Optional[typing.Iterable[typing.Text]] = ...,
        archive_uris : typing.Optional[typing.Iterable[typing.Text]] = ...,
        properties : typing.Optional[typing.Mapping[typing.Text, typing.Text]] = ...,
        logging_config : typing.Optional[global___LoggingConfig] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["logging_config",b"logging_config"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["archive_uris",b"archive_uris","args",b"args","file_uris",b"file_uris","jar_file_uris",b"jar_file_uris","logging_config",b"logging_config","main_python_file_uri",b"main_python_file_uri","properties",b"properties","python_file_uris",b"python_file_uris"]) -> None: ...
global___PySparkJob = PySparkJob

class QueryList(google.protobuf.message.Message):
    """A list of queries to run on a cluster."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    QUERIES_FIELD_NUMBER: builtins.int
    @property
    def queries(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """Required. The queries to execute. You do not need to end a query expression
        with a semicolon. Multiple queries can be specified in one
        string by separating each with a semicolon. Here is an example of a
        Dataproc API snippet that uses a QueryList to specify a HiveJob:

            "hiveJob": {
              "queryList": {
                "queries": [
                  "query1",
                  "query2",
                  "query3;query4",
                ]
              }
            }
        """
        pass
    def __init__(self,
        *,
        queries : typing.Optional[typing.Iterable[typing.Text]] = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["queries",b"queries"]) -> None: ...
global___QueryList = QueryList

class HiveJob(google.protobuf.message.Message):
    """A Dataproc job for running [Apache Hive](https://hive.apache.org/)
    queries on YARN.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class ScriptVariablesEntry(google.protobuf.message.Message):
        DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
        KEY_FIELD_NUMBER: builtins.int
        VALUE_FIELD_NUMBER: builtins.int
        key: typing.Text = ...
        value: typing.Text = ...
        def __init__(self,
            *,
            key : typing.Text = ...,
            value : typing.Text = ...,
            ) -> None: ...
        def ClearField(self, field_name: typing_extensions.Literal["key",b"key","value",b"value"]) -> None: ...

    class PropertiesEntry(google.protobuf.message.Message):
        DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
        KEY_FIELD_NUMBER: builtins.int
        VALUE_FIELD_NUMBER: builtins.int
        key: typing.Text = ...
        value: typing.Text = ...
        def __init__(self,
            *,
            key : typing.Text = ...,
            value : typing.Text = ...,
            ) -> None: ...
        def ClearField(self, field_name: typing_extensions.Literal["key",b"key","value",b"value"]) -> None: ...

    QUERY_FILE_URI_FIELD_NUMBER: builtins.int
    QUERY_LIST_FIELD_NUMBER: builtins.int
    CONTINUE_ON_FAILURE_FIELD_NUMBER: builtins.int
    SCRIPT_VARIABLES_FIELD_NUMBER: builtins.int
    PROPERTIES_FIELD_NUMBER: builtins.int
    JAR_FILE_URIS_FIELD_NUMBER: builtins.int
    query_file_uri: typing.Text = ...
    """The HCFS URI of the script that contains Hive queries."""

    @property
    def query_list(self) -> global___QueryList:
        """A list of queries."""
        pass
    continue_on_failure: builtins.bool = ...
    """Optional. Whether to continue executing queries if a query fails.
    The default value is `false`. Setting to `true` can be useful when
    executing independent parallel queries.
    """

    @property
    def script_variables(self) -> google.protobuf.internal.containers.ScalarMap[typing.Text, typing.Text]:
        """Optional. Mapping of query variable names to values (equivalent to the
        Hive command: `SET name="value";`).
        """
        pass
    @property
    def properties(self) -> google.protobuf.internal.containers.ScalarMap[typing.Text, typing.Text]:
        """Optional. A mapping of property names and values, used to configure Hive.
        Properties that conflict with values set by the Dataproc API may be
        overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml,
        /etc/hive/conf/hive-site.xml, and classes in user code.
        """
        pass
    @property
    def jar_file_uris(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """Optional. HCFS URIs of jar files to add to the CLASSPATH of the
        Hive server and Hadoop MapReduce (MR) tasks. Can contain Hive SerDes
        and UDFs.
        """
        pass
    def __init__(self,
        *,
        query_file_uri : typing.Text = ...,
        query_list : typing.Optional[global___QueryList] = ...,
        continue_on_failure : builtins.bool = ...,
        script_variables : typing.Optional[typing.Mapping[typing.Text, typing.Text]] = ...,
        properties : typing.Optional[typing.Mapping[typing.Text, typing.Text]] = ...,
        jar_file_uris : typing.Optional[typing.Iterable[typing.Text]] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["queries",b"queries","query_file_uri",b"query_file_uri","query_list",b"query_list"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["continue_on_failure",b"continue_on_failure","jar_file_uris",b"jar_file_uris","properties",b"properties","queries",b"queries","query_file_uri",b"query_file_uri","query_list",b"query_list","script_variables",b"script_variables"]) -> None: ...
    def WhichOneof(self, oneof_group: typing_extensions.Literal["queries",b"queries"]) -> typing.Optional[typing_extensions.Literal["query_file_uri","query_list"]]: ...
global___HiveJob = HiveJob

class SparkSqlJob(google.protobuf.message.Message):
    """A Dataproc job for running [Apache Spark
    SQL](http://spark.apache.org/sql/) queries.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class ScriptVariablesEntry(google.protobuf.message.Message):
        DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
        KEY_FIELD_NUMBER: builtins.int
        VALUE_FIELD_NUMBER: builtins.int
        key: typing.Text = ...
        value: typing.Text = ...
        def __init__(self,
            *,
            key : typing.Text = ...,
            value : typing.Text = ...,
            ) -> None: ...
        def ClearField(self, field_name: typing_extensions.Literal["key",b"key","value",b"value"]) -> None: ...

    class PropertiesEntry(google.protobuf.message.Message):
        DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
        KEY_FIELD_NUMBER: builtins.int
        VALUE_FIELD_NUMBER: builtins.int
        key: typing.Text = ...
        value: typing.Text = ...
        def __init__(self,
            *,
            key : typing.Text = ...,
            value : typing.Text = ...,
            ) -> None: ...
        def ClearField(self, field_name: typing_extensions.Literal["key",b"key","value",b"value"]) -> None: ...

    QUERY_FILE_URI_FIELD_NUMBER: builtins.int
    QUERY_LIST_FIELD_NUMBER: builtins.int
    SCRIPT_VARIABLES_FIELD_NUMBER: builtins.int
    PROPERTIES_FIELD_NUMBER: builtins.int
    JAR_FILE_URIS_FIELD_NUMBER: builtins.int
    LOGGING_CONFIG_FIELD_NUMBER: builtins.int
    query_file_uri: typing.Text = ...
    """The HCFS URI of the script that contains SQL queries."""

    @property
    def query_list(self) -> global___QueryList:
        """A list of queries."""
        pass
    @property
    def script_variables(self) -> google.protobuf.internal.containers.ScalarMap[typing.Text, typing.Text]:
        """Optional. Mapping of query variable names to values (equivalent to the
        Spark SQL command: SET `name="value";`).
        """
        pass
    @property
    def properties(self) -> google.protobuf.internal.containers.ScalarMap[typing.Text, typing.Text]:
        """Optional. A mapping of property names to values, used to configure
        Spark SQL's SparkConf. Properties that conflict with values set by the
        Dataproc API may be overwritten.
        """
        pass
    @property
    def jar_file_uris(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """Optional. HCFS URIs of jar files to be added to the Spark CLASSPATH."""
        pass
    @property
    def logging_config(self) -> global___LoggingConfig:
        """Optional. The runtime log config for job execution."""
        pass
    def __init__(self,
        *,
        query_file_uri : typing.Text = ...,
        query_list : typing.Optional[global___QueryList] = ...,
        script_variables : typing.Optional[typing.Mapping[typing.Text, typing.Text]] = ...,
        properties : typing.Optional[typing.Mapping[typing.Text, typing.Text]] = ...,
        jar_file_uris : typing.Optional[typing.Iterable[typing.Text]] = ...,
        logging_config : typing.Optional[global___LoggingConfig] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["logging_config",b"logging_config","queries",b"queries","query_file_uri",b"query_file_uri","query_list",b"query_list"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["jar_file_uris",b"jar_file_uris","logging_config",b"logging_config","properties",b"properties","queries",b"queries","query_file_uri",b"query_file_uri","query_list",b"query_list","script_variables",b"script_variables"]) -> None: ...
    def WhichOneof(self, oneof_group: typing_extensions.Literal["queries",b"queries"]) -> typing.Optional[typing_extensions.Literal["query_file_uri","query_list"]]: ...
global___SparkSqlJob = SparkSqlJob

class PigJob(google.protobuf.message.Message):
    """A Dataproc job for running [Apache Pig](https://pig.apache.org/)
    queries on YARN.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class ScriptVariablesEntry(google.protobuf.message.Message):
        DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
        KEY_FIELD_NUMBER: builtins.int
        VALUE_FIELD_NUMBER: builtins.int
        key: typing.Text = ...
        value: typing.Text = ...
        def __init__(self,
            *,
            key : typing.Text = ...,
            value : typing.Text = ...,
            ) -> None: ...
        def ClearField(self, field_name: typing_extensions.Literal["key",b"key","value",b"value"]) -> None: ...

    class PropertiesEntry(google.protobuf.message.Message):
        DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
        KEY_FIELD_NUMBER: builtins.int
        VALUE_FIELD_NUMBER: builtins.int
        key: typing.Text = ...
        value: typing.Text = ...
        def __init__(self,
            *,
            key : typing.Text = ...,
            value : typing.Text = ...,
            ) -> None: ...
        def ClearField(self, field_name: typing_extensions.Literal["key",b"key","value",b"value"]) -> None: ...

    QUERY_FILE_URI_FIELD_NUMBER: builtins.int
    QUERY_LIST_FIELD_NUMBER: builtins.int
    CONTINUE_ON_FAILURE_FIELD_NUMBER: builtins.int
    SCRIPT_VARIABLES_FIELD_NUMBER: builtins.int
    PROPERTIES_FIELD_NUMBER: builtins.int
    JAR_FILE_URIS_FIELD_NUMBER: builtins.int
    LOGGING_CONFIG_FIELD_NUMBER: builtins.int
    query_file_uri: typing.Text = ...
    """The HCFS URI of the script that contains the Pig queries."""

    @property
    def query_list(self) -> global___QueryList:
        """A list of queries."""
        pass
    continue_on_failure: builtins.bool = ...
    """Optional. Whether to continue executing queries if a query fails.
    The default value is `false`. Setting to `true` can be useful when
    executing independent parallel queries.
    """

    @property
    def script_variables(self) -> google.protobuf.internal.containers.ScalarMap[typing.Text, typing.Text]:
        """Optional. Mapping of query variable names to values (equivalent to the Pig
        command: `name=[value]`).
        """
        pass
    @property
    def properties(self) -> google.protobuf.internal.containers.ScalarMap[typing.Text, typing.Text]:
        """Optional. A mapping of property names to values, used to configure Pig.
        Properties that conflict with values set by the Dataproc API may be
        overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml,
        /etc/pig/conf/pig.properties, and classes in user code.
        """
        pass
    @property
    def jar_file_uris(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """Optional. HCFS URIs of jar files to add to the CLASSPATH of
        the Pig Client and Hadoop MapReduce (MR) tasks. Can contain Pig UDFs.
        """
        pass
    @property
    def logging_config(self) -> global___LoggingConfig:
        """Optional. The runtime log config for job execution."""
        pass
    def __init__(self,
        *,
        query_file_uri : typing.Text = ...,
        query_list : typing.Optional[global___QueryList] = ...,
        continue_on_failure : builtins.bool = ...,
        script_variables : typing.Optional[typing.Mapping[typing.Text, typing.Text]] = ...,
        properties : typing.Optional[typing.Mapping[typing.Text, typing.Text]] = ...,
        jar_file_uris : typing.Optional[typing.Iterable[typing.Text]] = ...,
        logging_config : typing.Optional[global___LoggingConfig] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["logging_config",b"logging_config","queries",b"queries","query_file_uri",b"query_file_uri","query_list",b"query_list"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["continue_on_failure",b"continue_on_failure","jar_file_uris",b"jar_file_uris","logging_config",b"logging_config","properties",b"properties","queries",b"queries","query_file_uri",b"query_file_uri","query_list",b"query_list","script_variables",b"script_variables"]) -> None: ...
    def WhichOneof(self, oneof_group: typing_extensions.Literal["queries",b"queries"]) -> typing.Optional[typing_extensions.Literal["query_file_uri","query_list"]]: ...
global___PigJob = PigJob

class SparkRJob(google.protobuf.message.Message):
    """A Dataproc job for running
    [Apache SparkR](https://spark.apache.org/docs/latest/sparkr.html)
    applications on YARN.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class PropertiesEntry(google.protobuf.message.Message):
        DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
        KEY_FIELD_NUMBER: builtins.int
        VALUE_FIELD_NUMBER: builtins.int
        key: typing.Text = ...
        value: typing.Text = ...
        def __init__(self,
            *,
            key : typing.Text = ...,
            value : typing.Text = ...,
            ) -> None: ...
        def ClearField(self, field_name: typing_extensions.Literal["key",b"key","value",b"value"]) -> None: ...

    MAIN_R_FILE_URI_FIELD_NUMBER: builtins.int
    ARGS_FIELD_NUMBER: builtins.int
    FILE_URIS_FIELD_NUMBER: builtins.int
    ARCHIVE_URIS_FIELD_NUMBER: builtins.int
    PROPERTIES_FIELD_NUMBER: builtins.int
    LOGGING_CONFIG_FIELD_NUMBER: builtins.int
    main_r_file_uri: typing.Text = ...
    """Required. The HCFS URI of the main R file to use as the driver.
    Must be a .R file.
    """

    @property
    def args(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """Optional. The arguments to pass to the driver.  Do not include arguments,
        such as `--conf`, that can be set as job properties, since a collision may
        occur that causes an incorrect job submission.
        """
        pass
    @property
    def file_uris(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """Optional. HCFS URIs of files to be placed in the working directory of
        each executor. Useful for naively parallel tasks.
        """
        pass
    @property
    def archive_uris(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """Optional. HCFS URIs of archives to be extracted into the working directory
        of each executor. Supported file types:
        .jar, .tar, .tar.gz, .tgz, and .zip.
        """
        pass
    @property
    def properties(self) -> google.protobuf.internal.containers.ScalarMap[typing.Text, typing.Text]:
        """Optional. A mapping of property names to values, used to configure SparkR.
        Properties that conflict with values set by the Dataproc API may be
        overwritten. Can include properties set in
        /etc/spark/conf/spark-defaults.conf and classes in user code.
        """
        pass
    @property
    def logging_config(self) -> global___LoggingConfig:
        """Optional. The runtime log config for job execution."""
        pass
    def __init__(self,
        *,
        main_r_file_uri : typing.Text = ...,
        args : typing.Optional[typing.Iterable[typing.Text]] = ...,
        file_uris : typing.Optional[typing.Iterable[typing.Text]] = ...,
        archive_uris : typing.Optional[typing.Iterable[typing.Text]] = ...,
        properties : typing.Optional[typing.Mapping[typing.Text, typing.Text]] = ...,
        logging_config : typing.Optional[global___LoggingConfig] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["logging_config",b"logging_config"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["archive_uris",b"archive_uris","args",b"args","file_uris",b"file_uris","logging_config",b"logging_config","main_r_file_uri",b"main_r_file_uri","properties",b"properties"]) -> None: ...
global___SparkRJob = SparkRJob

class PrestoJob(google.protobuf.message.Message):
    """A Dataproc job for running [Presto](https://prestosql.io/) queries.
    **IMPORTANT**: The [Dataproc Presto Optional
    Component](https://cloud.google.com/dataproc/docs/concepts/components/presto)
    must be enabled when the cluster is created to submit a Presto job to the
    cluster.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class PropertiesEntry(google.protobuf.message.Message):
        DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
        KEY_FIELD_NUMBER: builtins.int
        VALUE_FIELD_NUMBER: builtins.int
        key: typing.Text = ...
        value: typing.Text = ...
        def __init__(self,
            *,
            key : typing.Text = ...,
            value : typing.Text = ...,
            ) -> None: ...
        def ClearField(self, field_name: typing_extensions.Literal["key",b"key","value",b"value"]) -> None: ...

    QUERY_FILE_URI_FIELD_NUMBER: builtins.int
    QUERY_LIST_FIELD_NUMBER: builtins.int
    CONTINUE_ON_FAILURE_FIELD_NUMBER: builtins.int
    OUTPUT_FORMAT_FIELD_NUMBER: builtins.int
    CLIENT_TAGS_FIELD_NUMBER: builtins.int
    PROPERTIES_FIELD_NUMBER: builtins.int
    LOGGING_CONFIG_FIELD_NUMBER: builtins.int
    query_file_uri: typing.Text = ...
    """The HCFS URI of the script that contains SQL queries."""

    @property
    def query_list(self) -> global___QueryList:
        """A list of queries."""
        pass
    continue_on_failure: builtins.bool = ...
    """Optional. Whether to continue executing queries if a query fails.
    The default value is `false`. Setting to `true` can be useful when
    executing independent parallel queries.
    """

    output_format: typing.Text = ...
    """Optional. The format in which query output will be displayed. See the
    Presto documentation for supported output formats
    """

    @property
    def client_tags(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """Optional. Presto client tags to attach to this query"""
        pass
    @property
    def properties(self) -> google.protobuf.internal.containers.ScalarMap[typing.Text, typing.Text]:
        """Optional. A mapping of property names to values. Used to set Presto
        [session properties](https://prestodb.io/docs/current/sql/set-session.html)
        Equivalent to using the --session flag in the Presto CLI
        """
        pass
    @property
    def logging_config(self) -> global___LoggingConfig:
        """Optional. The runtime log config for job execution."""
        pass
    def __init__(self,
        *,
        query_file_uri : typing.Text = ...,
        query_list : typing.Optional[global___QueryList] = ...,
        continue_on_failure : builtins.bool = ...,
        output_format : typing.Text = ...,
        client_tags : typing.Optional[typing.Iterable[typing.Text]] = ...,
        properties : typing.Optional[typing.Mapping[typing.Text, typing.Text]] = ...,
        logging_config : typing.Optional[global___LoggingConfig] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["logging_config",b"logging_config","queries",b"queries","query_file_uri",b"query_file_uri","query_list",b"query_list"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["client_tags",b"client_tags","continue_on_failure",b"continue_on_failure","logging_config",b"logging_config","output_format",b"output_format","properties",b"properties","queries",b"queries","query_file_uri",b"query_file_uri","query_list",b"query_list"]) -> None: ...
    def WhichOneof(self, oneof_group: typing_extensions.Literal["queries",b"queries"]) -> typing.Optional[typing_extensions.Literal["query_file_uri","query_list"]]: ...
global___PrestoJob = PrestoJob

class JobPlacement(google.protobuf.message.Message):
    """Dataproc job config."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class ClusterLabelsEntry(google.protobuf.message.Message):
        DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
        KEY_FIELD_NUMBER: builtins.int
        VALUE_FIELD_NUMBER: builtins.int
        key: typing.Text = ...
        value: typing.Text = ...
        def __init__(self,
            *,
            key : typing.Text = ...,
            value : typing.Text = ...,
            ) -> None: ...
        def ClearField(self, field_name: typing_extensions.Literal["key",b"key","value",b"value"]) -> None: ...

    CLUSTER_NAME_FIELD_NUMBER: builtins.int
    CLUSTER_UUID_FIELD_NUMBER: builtins.int
    CLUSTER_LABELS_FIELD_NUMBER: builtins.int
    cluster_name: typing.Text = ...
    """Required. The name of the cluster where the job will be submitted."""

    cluster_uuid: typing.Text = ...
    """Output only. A cluster UUID generated by the Dataproc service when
    the job is submitted.
    """

    @property
    def cluster_labels(self) -> google.protobuf.internal.containers.ScalarMap[typing.Text, typing.Text]:
        """Optional. Cluster labels to identify a cluster where the job will be submitted."""
        pass
    def __init__(self,
        *,
        cluster_name : typing.Text = ...,
        cluster_uuid : typing.Text = ...,
        cluster_labels : typing.Optional[typing.Mapping[typing.Text, typing.Text]] = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["cluster_labels",b"cluster_labels","cluster_name",b"cluster_name","cluster_uuid",b"cluster_uuid"]) -> None: ...
global___JobPlacement = JobPlacement

class JobStatus(google.protobuf.message.Message):
    """Dataproc job status."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class _State:
        ValueType = typing.NewType('ValueType', builtins.int)
        V: typing_extensions.TypeAlias = ValueType
    class _StateEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_State.ValueType], builtins.type):
        DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor = ...
        STATE_UNSPECIFIED: JobStatus.State.ValueType = ...  # 0
        """The job state is unknown."""

        PENDING: JobStatus.State.ValueType = ...  # 1
        """The job is pending; it has been submitted, but is not yet running."""

        SETUP_DONE: JobStatus.State.ValueType = ...  # 8
        """Job has been received by the service and completed initial setup;
        it will soon be submitted to the cluster.
        """

        RUNNING: JobStatus.State.ValueType = ...  # 2
        """The job is running on the cluster."""

        CANCEL_PENDING: JobStatus.State.ValueType = ...  # 3
        """A CancelJob request has been received, but is pending."""

        CANCEL_STARTED: JobStatus.State.ValueType = ...  # 7
        """Transient in-flight resources have been canceled, and the request to
        cancel the running job has been issued to the cluster.
        """

        CANCELLED: JobStatus.State.ValueType = ...  # 4
        """The job cancellation was successful."""

        DONE: JobStatus.State.ValueType = ...  # 5
        """The job has completed successfully."""

        ERROR: JobStatus.State.ValueType = ...  # 6
        """The job has completed, but encountered an error."""

        ATTEMPT_FAILURE: JobStatus.State.ValueType = ...  # 9
        """Job attempt has failed. The detail field contains failure details for
        this attempt.

        Applies to restartable jobs only.
        """

    class State(_State, metaclass=_StateEnumTypeWrapper):
        """The job state."""
        pass

    STATE_UNSPECIFIED: JobStatus.State.ValueType = ...  # 0
    """The job state is unknown."""

    PENDING: JobStatus.State.ValueType = ...  # 1
    """The job is pending; it has been submitted, but is not yet running."""

    SETUP_DONE: JobStatus.State.ValueType = ...  # 8
    """Job has been received by the service and completed initial setup;
    it will soon be submitted to the cluster.
    """

    RUNNING: JobStatus.State.ValueType = ...  # 2
    """The job is running on the cluster."""

    CANCEL_PENDING: JobStatus.State.ValueType = ...  # 3
    """A CancelJob request has been received, but is pending."""

    CANCEL_STARTED: JobStatus.State.ValueType = ...  # 7
    """Transient in-flight resources have been canceled, and the request to
    cancel the running job has been issued to the cluster.
    """

    CANCELLED: JobStatus.State.ValueType = ...  # 4
    """The job cancellation was successful."""

    DONE: JobStatus.State.ValueType = ...  # 5
    """The job has completed successfully."""

    ERROR: JobStatus.State.ValueType = ...  # 6
    """The job has completed, but encountered an error."""

    ATTEMPT_FAILURE: JobStatus.State.ValueType = ...  # 9
    """Job attempt has failed. The detail field contains failure details for
    this attempt.

    Applies to restartable jobs only.
    """


    class _Substate:
        ValueType = typing.NewType('ValueType', builtins.int)
        V: typing_extensions.TypeAlias = ValueType
    class _SubstateEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_Substate.ValueType], builtins.type):
        DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor = ...
        UNSPECIFIED: JobStatus.Substate.ValueType = ...  # 0
        """The job substate is unknown."""

        SUBMITTED: JobStatus.Substate.ValueType = ...  # 1
        """The Job is submitted to the agent.

        Applies to RUNNING state.
        """

        QUEUED: JobStatus.Substate.ValueType = ...  # 2
        """The Job has been received and is awaiting execution (it may be waiting
        for a condition to be met). See the "details" field for the reason for
        the delay.

        Applies to RUNNING state.
        """

        STALE_STATUS: JobStatus.Substate.ValueType = ...  # 3
        """The agent-reported status is out of date, which may be caused by a
        loss of communication between the agent and Dataproc. If the
        agent does not send a timely update, the job will fail.

        Applies to RUNNING state.
        """

    class Substate(_Substate, metaclass=_SubstateEnumTypeWrapper):
        """The job substate."""
        pass

    UNSPECIFIED: JobStatus.Substate.ValueType = ...  # 0
    """The job substate is unknown."""

    SUBMITTED: JobStatus.Substate.ValueType = ...  # 1
    """The Job is submitted to the agent.

    Applies to RUNNING state.
    """

    QUEUED: JobStatus.Substate.ValueType = ...  # 2
    """The Job has been received and is awaiting execution (it may be waiting
    for a condition to be met). See the "details" field for the reason for
    the delay.

    Applies to RUNNING state.
    """

    STALE_STATUS: JobStatus.Substate.ValueType = ...  # 3
    """The agent-reported status is out of date, which may be caused by a
    loss of communication between the agent and Dataproc. If the
    agent does not send a timely update, the job will fail.

    Applies to RUNNING state.
    """


    STATE_FIELD_NUMBER: builtins.int
    DETAILS_FIELD_NUMBER: builtins.int
    STATE_START_TIME_FIELD_NUMBER: builtins.int
    SUBSTATE_FIELD_NUMBER: builtins.int
    state: global___JobStatus.State.ValueType = ...
    """Output only. A state message specifying the overall job state."""

    details: typing.Text = ...
    """Optional. Output only. Job state details, such as an error
    description if the state is <code>ERROR</code>.
    """

    @property
    def state_start_time(self) -> google.protobuf.timestamp_pb2.Timestamp:
        """Output only. The time when this state was entered."""
        pass
    substate: global___JobStatus.Substate.ValueType = ...
    """Output only. Additional state information, which includes
    status reported by the agent.
    """

    def __init__(self,
        *,
        state : global___JobStatus.State.ValueType = ...,
        details : typing.Text = ...,
        state_start_time : typing.Optional[google.protobuf.timestamp_pb2.Timestamp] = ...,
        substate : global___JobStatus.Substate.ValueType = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["state_start_time",b"state_start_time"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["details",b"details","state",b"state","state_start_time",b"state_start_time","substate",b"substate"]) -> None: ...
global___JobStatus = JobStatus

class JobReference(google.protobuf.message.Message):
    """Encapsulates the full scoping used to reference a job."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    PROJECT_ID_FIELD_NUMBER: builtins.int
    JOB_ID_FIELD_NUMBER: builtins.int
    project_id: typing.Text = ...
    """Optional. The ID of the Google Cloud Platform project that the job belongs to. If
    specified, must match the request project ID.
    """

    job_id: typing.Text = ...
    """Optional. The job ID, which must be unique within the project.

    The ID must contain only letters (a-z, A-Z), numbers (0-9),
    underscores (_), or hyphens (-). The maximum length is 100 characters.

    If not specified by the caller, the job ID will be provided by the server.
    """

    def __init__(self,
        *,
        project_id : typing.Text = ...,
        job_id : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["job_id",b"job_id","project_id",b"project_id"]) -> None: ...
global___JobReference = JobReference

class YarnApplication(google.protobuf.message.Message):
    """A YARN application created by a job. Application information is a subset of
    <code>org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto</code>.

    **Beta Feature**: This report is available for testing purposes only. It may
    be changed before final release.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class _State:
        ValueType = typing.NewType('ValueType', builtins.int)
        V: typing_extensions.TypeAlias = ValueType
    class _StateEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_State.ValueType], builtins.type):
        DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor = ...
        STATE_UNSPECIFIED: YarnApplication.State.ValueType = ...  # 0
        """Status is unspecified."""

        NEW: YarnApplication.State.ValueType = ...  # 1
        """Status is NEW."""

        NEW_SAVING: YarnApplication.State.ValueType = ...  # 2
        """Status is NEW_SAVING."""

        SUBMITTED: YarnApplication.State.ValueType = ...  # 3
        """Status is SUBMITTED."""

        ACCEPTED: YarnApplication.State.ValueType = ...  # 4
        """Status is ACCEPTED."""

        RUNNING: YarnApplication.State.ValueType = ...  # 5
        """Status is RUNNING."""

        FINISHED: YarnApplication.State.ValueType = ...  # 6
        """Status is FINISHED."""

        FAILED: YarnApplication.State.ValueType = ...  # 7
        """Status is FAILED."""

        KILLED: YarnApplication.State.ValueType = ...  # 8
        """Status is KILLED."""

    class State(_State, metaclass=_StateEnumTypeWrapper):
        """The application state, corresponding to
        <code>YarnProtos.YarnApplicationStateProto</code>.
        """
        pass

    STATE_UNSPECIFIED: YarnApplication.State.ValueType = ...  # 0
    """Status is unspecified."""

    NEW: YarnApplication.State.ValueType = ...  # 1
    """Status is NEW."""

    NEW_SAVING: YarnApplication.State.ValueType = ...  # 2
    """Status is NEW_SAVING."""

    SUBMITTED: YarnApplication.State.ValueType = ...  # 3
    """Status is SUBMITTED."""

    ACCEPTED: YarnApplication.State.ValueType = ...  # 4
    """Status is ACCEPTED."""

    RUNNING: YarnApplication.State.ValueType = ...  # 5
    """Status is RUNNING."""

    FINISHED: YarnApplication.State.ValueType = ...  # 6
    """Status is FINISHED."""

    FAILED: YarnApplication.State.ValueType = ...  # 7
    """Status is FAILED."""

    KILLED: YarnApplication.State.ValueType = ...  # 8
    """Status is KILLED."""


    NAME_FIELD_NUMBER: builtins.int
    STATE_FIELD_NUMBER: builtins.int
    PROGRESS_FIELD_NUMBER: builtins.int
    TRACKING_URL_FIELD_NUMBER: builtins.int
    name: typing.Text = ...
    """Required. The application name."""

    state: global___YarnApplication.State.ValueType = ...
    """Required. The application state."""

    progress: builtins.float = ...
    """Required. The numerical progress of the application, from 1 to 100."""

    tracking_url: typing.Text = ...
    """Optional. The HTTP URL of the ApplicationMaster, HistoryServer, or
    TimelineServer that provides application-specific information. The URL uses
    the internal hostname, and requires a proxy server for resolution and,
    possibly, access.
    """

    def __init__(self,
        *,
        name : typing.Text = ...,
        state : global___YarnApplication.State.ValueType = ...,
        progress : builtins.float = ...,
        tracking_url : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["name",b"name","progress",b"progress","state",b"state","tracking_url",b"tracking_url"]) -> None: ...
global___YarnApplication = YarnApplication

class Job(google.protobuf.message.Message):
    """A Dataproc job resource."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class LabelsEntry(google.protobuf.message.Message):
        DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
        KEY_FIELD_NUMBER: builtins.int
        VALUE_FIELD_NUMBER: builtins.int
        key: typing.Text = ...
        value: typing.Text = ...
        def __init__(self,
            *,
            key : typing.Text = ...,
            value : typing.Text = ...,
            ) -> None: ...
        def ClearField(self, field_name: typing_extensions.Literal["key",b"key","value",b"value"]) -> None: ...

    REFERENCE_FIELD_NUMBER: builtins.int
    PLACEMENT_FIELD_NUMBER: builtins.int
    HADOOP_JOB_FIELD_NUMBER: builtins.int
    SPARK_JOB_FIELD_NUMBER: builtins.int
    PYSPARK_JOB_FIELD_NUMBER: builtins.int
    HIVE_JOB_FIELD_NUMBER: builtins.int
    PIG_JOB_FIELD_NUMBER: builtins.int
    SPARK_R_JOB_FIELD_NUMBER: builtins.int
    SPARK_SQL_JOB_FIELD_NUMBER: builtins.int
    PRESTO_JOB_FIELD_NUMBER: builtins.int
    STATUS_FIELD_NUMBER: builtins.int
    STATUS_HISTORY_FIELD_NUMBER: builtins.int
    YARN_APPLICATIONS_FIELD_NUMBER: builtins.int
    DRIVER_OUTPUT_RESOURCE_URI_FIELD_NUMBER: builtins.int
    DRIVER_CONTROL_FILES_URI_FIELD_NUMBER: builtins.int
    LABELS_FIELD_NUMBER: builtins.int
    SCHEDULING_FIELD_NUMBER: builtins.int
    JOB_UUID_FIELD_NUMBER: builtins.int
    DONE_FIELD_NUMBER: builtins.int
    @property
    def reference(self) -> global___JobReference:
        """Optional. The fully qualified reference to the job, which can be used to
        obtain the equivalent REST path of the job resource. If this property
        is not specified when a job is created, the server generates a
        <code>job_id</code>.
        """
        pass
    @property
    def placement(self) -> global___JobPlacement:
        """Required. Job information, including how, when, and where to
        run the job.
        """
        pass
    @property
    def hadoop_job(self) -> global___HadoopJob:
        """Optional. Job is a Hadoop job."""
        pass
    @property
    def spark_job(self) -> global___SparkJob:
        """Optional. Job is a Spark job."""
        pass
    @property
    def pyspark_job(self) -> global___PySparkJob:
        """Optional. Job is a PySpark job."""
        pass
    @property
    def hive_job(self) -> global___HiveJob:
        """Optional. Job is a Hive job."""
        pass
    @property
    def pig_job(self) -> global___PigJob:
        """Optional. Job is a Pig job."""
        pass
    @property
    def spark_r_job(self) -> global___SparkRJob:
        """Optional. Job is a SparkR job."""
        pass
    @property
    def spark_sql_job(self) -> global___SparkSqlJob:
        """Optional. Job is a SparkSql job."""
        pass
    @property
    def presto_job(self) -> global___PrestoJob:
        """Optional. Job is a Presto job."""
        pass
    @property
    def status(self) -> global___JobStatus:
        """Output only. The job status. Additional application-specific
        status information may be contained in the <code>type_job</code>
        and <code>yarn_applications</code> fields.
        """
        pass
    @property
    def status_history(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___JobStatus]:
        """Output only. The previous job status."""
        pass
    @property
    def yarn_applications(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___YarnApplication]:
        """Output only. The collection of YARN applications spun up by this job.

        **Beta** Feature: This report is available for testing purposes only. It
        may be changed before final release.
        """
        pass
    driver_output_resource_uri: typing.Text = ...
    """Output only. A URI pointing to the location of the stdout of the job's
    driver program.
    """

    driver_control_files_uri: typing.Text = ...
    """Output only. If present, the location of miscellaneous control files
    which may be used as part of job setup and handling. If not present,
    control files may be placed in the same location as `driver_output_uri`.
    """

    @property
    def labels(self) -> google.protobuf.internal.containers.ScalarMap[typing.Text, typing.Text]:
        """Optional. The labels to associate with this job.
        Label **keys** must contain 1 to 63 characters, and must conform to
        [RFC 1035](https://www.ietf.org/rfc/rfc1035.txt).
        Label **values** may be empty, but, if present, must contain 1 to 63
        characters, and must conform to [RFC
        1035](https://www.ietf.org/rfc/rfc1035.txt). No more than 32 labels can be
        associated with a job.
        """
        pass
    @property
    def scheduling(self) -> global___JobScheduling:
        """Optional. Job scheduling configuration."""
        pass
    job_uuid: typing.Text = ...
    """Output only. A UUID that uniquely identifies a job within the project
    over time. This is in contrast to a user-settable reference.job_id that
    may be reused over time.
    """

    done: builtins.bool = ...
    """Output only. Indicates whether the job is completed. If the value is `false`,
    the job is still in progress. If `true`, the job is completed, and
    `status.state` field will indicate if it was successful, failed,
    or cancelled.
    """

    def __init__(self,
        *,
        reference : typing.Optional[global___JobReference] = ...,
        placement : typing.Optional[global___JobPlacement] = ...,
        hadoop_job : typing.Optional[global___HadoopJob] = ...,
        spark_job : typing.Optional[global___SparkJob] = ...,
        pyspark_job : typing.Optional[global___PySparkJob] = ...,
        hive_job : typing.Optional[global___HiveJob] = ...,
        pig_job : typing.Optional[global___PigJob] = ...,
        spark_r_job : typing.Optional[global___SparkRJob] = ...,
        spark_sql_job : typing.Optional[global___SparkSqlJob] = ...,
        presto_job : typing.Optional[global___PrestoJob] = ...,
        status : typing.Optional[global___JobStatus] = ...,
        status_history : typing.Optional[typing.Iterable[global___JobStatus]] = ...,
        yarn_applications : typing.Optional[typing.Iterable[global___YarnApplication]] = ...,
        driver_output_resource_uri : typing.Text = ...,
        driver_control_files_uri : typing.Text = ...,
        labels : typing.Optional[typing.Mapping[typing.Text, typing.Text]] = ...,
        scheduling : typing.Optional[global___JobScheduling] = ...,
        job_uuid : typing.Text = ...,
        done : builtins.bool = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["hadoop_job",b"hadoop_job","hive_job",b"hive_job","pig_job",b"pig_job","placement",b"placement","presto_job",b"presto_job","pyspark_job",b"pyspark_job","reference",b"reference","scheduling",b"scheduling","spark_job",b"spark_job","spark_r_job",b"spark_r_job","spark_sql_job",b"spark_sql_job","status",b"status","type_job",b"type_job"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["done",b"done","driver_control_files_uri",b"driver_control_files_uri","driver_output_resource_uri",b"driver_output_resource_uri","hadoop_job",b"hadoop_job","hive_job",b"hive_job","job_uuid",b"job_uuid","labels",b"labels","pig_job",b"pig_job","placement",b"placement","presto_job",b"presto_job","pyspark_job",b"pyspark_job","reference",b"reference","scheduling",b"scheduling","spark_job",b"spark_job","spark_r_job",b"spark_r_job","spark_sql_job",b"spark_sql_job","status",b"status","status_history",b"status_history","type_job",b"type_job","yarn_applications",b"yarn_applications"]) -> None: ...
    def WhichOneof(self, oneof_group: typing_extensions.Literal["type_job",b"type_job"]) -> typing.Optional[typing_extensions.Literal["hadoop_job","spark_job","pyspark_job","hive_job","pig_job","spark_r_job","spark_sql_job","presto_job"]]: ...
global___Job = Job

class JobScheduling(google.protobuf.message.Message):
    """Job scheduling options."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    MAX_FAILURES_PER_HOUR_FIELD_NUMBER: builtins.int
    MAX_FAILURES_TOTAL_FIELD_NUMBER: builtins.int
    max_failures_per_hour: builtins.int = ...
    """Optional. Maximum number of times per hour a driver may be restarted as
    a result of driver exiting with non-zero code before job is
    reported failed.

    A job may be reported as thrashing if driver exits with non-zero code
    4 times within 10 minute window.

    Maximum value is 10.

    **Note:** Currently, this restartable job option is
    not supported in Dataproc
    [workflow
    template](https://cloud.google.com/dataproc/docs/concepts/workflows/using-workflows#adding_jobs_to_a_template)
    jobs.
    """

    max_failures_total: builtins.int = ...
    """Optional. Maximum number of times in total a driver may be restarted as a result of
    driver exiting with non-zero code before job is reported failed.
    Maximum value is 240.

    **Note:** Currently, this restartable job option is
    not supported in Dataproc
    [workflow
    template](https://cloud.google.com/dataproc/docs/concepts/workflows/using-workflows#adding_jobs_to_a_template)
    jobs.
    """

    def __init__(self,
        *,
        max_failures_per_hour : builtins.int = ...,
        max_failures_total : builtins.int = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["max_failures_per_hour",b"max_failures_per_hour","max_failures_total",b"max_failures_total"]) -> None: ...
global___JobScheduling = JobScheduling

class SubmitJobRequest(google.protobuf.message.Message):
    """A request to submit a job."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    PROJECT_ID_FIELD_NUMBER: builtins.int
    REGION_FIELD_NUMBER: builtins.int
    JOB_FIELD_NUMBER: builtins.int
    REQUEST_ID_FIELD_NUMBER: builtins.int
    project_id: typing.Text = ...
    """Required. The ID of the Google Cloud Platform project that the job
    belongs to.
    """

    region: typing.Text = ...
    """Required. The Dataproc region in which to handle the request."""

    @property
    def job(self) -> global___Job:
        """Required. The job resource."""
        pass
    request_id: typing.Text = ...
    """Optional. A unique id used to identify the request. If the server
    receives two
    [SubmitJobRequest](https://cloud.google.com/dataproc/docs/reference/rpc/google.cloud.dataproc.v1#google.cloud.dataproc.v1.SubmitJobRequest)s
    with the same id, then the second request will be ignored and the
    first [Job][google.cloud.dataproc.v1.Job] created and stored in the backend
    is returned.

    It is recommended to always set this value to a
    [UUID](https://en.wikipedia.org/wiki/Universally_unique_identifier).

    The id must contain only letters (a-z, A-Z), numbers (0-9),
    underscores (_), and hyphens (-). The maximum length is 40 characters.
    """

    def __init__(self,
        *,
        project_id : typing.Text = ...,
        region : typing.Text = ...,
        job : typing.Optional[global___Job] = ...,
        request_id : typing.Text = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["job",b"job"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["job",b"job","project_id",b"project_id","region",b"region","request_id",b"request_id"]) -> None: ...
global___SubmitJobRequest = SubmitJobRequest

class JobMetadata(google.protobuf.message.Message):
    """Job Operation metadata."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    JOB_ID_FIELD_NUMBER: builtins.int
    STATUS_FIELD_NUMBER: builtins.int
    OPERATION_TYPE_FIELD_NUMBER: builtins.int
    START_TIME_FIELD_NUMBER: builtins.int
    job_id: typing.Text = ...
    """Output only. The job id."""

    @property
    def status(self) -> global___JobStatus:
        """Output only. Most recent job status."""
        pass
    operation_type: typing.Text = ...
    """Output only. Operation type."""

    @property
    def start_time(self) -> google.protobuf.timestamp_pb2.Timestamp:
        """Output only. Job submission time."""
        pass
    def __init__(self,
        *,
        job_id : typing.Text = ...,
        status : typing.Optional[global___JobStatus] = ...,
        operation_type : typing.Text = ...,
        start_time : typing.Optional[google.protobuf.timestamp_pb2.Timestamp] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["start_time",b"start_time","status",b"status"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["job_id",b"job_id","operation_type",b"operation_type","start_time",b"start_time","status",b"status"]) -> None: ...
global___JobMetadata = JobMetadata

class GetJobRequest(google.protobuf.message.Message):
    """A request to get the resource representation for a job in a project."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    PROJECT_ID_FIELD_NUMBER: builtins.int
    REGION_FIELD_NUMBER: builtins.int
    JOB_ID_FIELD_NUMBER: builtins.int
    project_id: typing.Text = ...
    """Required. The ID of the Google Cloud Platform project that the job
    belongs to.
    """

    region: typing.Text = ...
    """Required. The Dataproc region in which to handle the request."""

    job_id: typing.Text = ...
    """Required. The job ID."""

    def __init__(self,
        *,
        project_id : typing.Text = ...,
        region : typing.Text = ...,
        job_id : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["job_id",b"job_id","project_id",b"project_id","region",b"region"]) -> None: ...
global___GetJobRequest = GetJobRequest

class ListJobsRequest(google.protobuf.message.Message):
    """A request to list jobs in a project."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class _JobStateMatcher:
        ValueType = typing.NewType('ValueType', builtins.int)
        V: typing_extensions.TypeAlias = ValueType
    class _JobStateMatcherEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_JobStateMatcher.ValueType], builtins.type):
        DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor = ...
        ALL: ListJobsRequest.JobStateMatcher.ValueType = ...  # 0
        """Match all jobs, regardless of state."""

        ACTIVE: ListJobsRequest.JobStateMatcher.ValueType = ...  # 1
        """Only match jobs in non-terminal states: PENDING, RUNNING, or
        CANCEL_PENDING.
        """

        NON_ACTIVE: ListJobsRequest.JobStateMatcher.ValueType = ...  # 2
        """Only match jobs in terminal states: CANCELLED, DONE, or ERROR."""

    class JobStateMatcher(_JobStateMatcher, metaclass=_JobStateMatcherEnumTypeWrapper):
        """A matcher that specifies categories of job states."""
        pass

    ALL: ListJobsRequest.JobStateMatcher.ValueType = ...  # 0
    """Match all jobs, regardless of state."""

    ACTIVE: ListJobsRequest.JobStateMatcher.ValueType = ...  # 1
    """Only match jobs in non-terminal states: PENDING, RUNNING, or
    CANCEL_PENDING.
    """

    NON_ACTIVE: ListJobsRequest.JobStateMatcher.ValueType = ...  # 2
    """Only match jobs in terminal states: CANCELLED, DONE, or ERROR."""


    PROJECT_ID_FIELD_NUMBER: builtins.int
    REGION_FIELD_NUMBER: builtins.int
    PAGE_SIZE_FIELD_NUMBER: builtins.int
    PAGE_TOKEN_FIELD_NUMBER: builtins.int
    CLUSTER_NAME_FIELD_NUMBER: builtins.int
    JOB_STATE_MATCHER_FIELD_NUMBER: builtins.int
    FILTER_FIELD_NUMBER: builtins.int
    project_id: typing.Text = ...
    """Required. The ID of the Google Cloud Platform project that the job
    belongs to.
    """

    region: typing.Text = ...
    """Required. The Dataproc region in which to handle the request."""

    page_size: builtins.int = ...
    """Optional. The number of results to return in each response."""

    page_token: typing.Text = ...
    """Optional. The page token, returned by a previous call, to request the
    next page of results.
    """

    cluster_name: typing.Text = ...
    """Optional. If set, the returned jobs list includes only jobs that were
    submitted to the named cluster.
    """

    job_state_matcher: global___ListJobsRequest.JobStateMatcher.ValueType = ...
    """Optional. Specifies enumerated categories of jobs to list.
    (default = match ALL jobs).

    If `filter` is provided, `jobStateMatcher` will be ignored.
    """

    filter: typing.Text = ...
    """Optional. A filter constraining the jobs to list. Filters are
    case-sensitive and have the following syntax:

    [field = value] AND [field [= value]] ...

    where **field** is `status.state` or `labels.[KEY]`, and `[KEY]` is a label
    key. **value** can be `*` to match all values.
    `status.state` can be either `ACTIVE` or `NON_ACTIVE`.
    Only the logical `AND` operator is supported; space-separated items are
    treated as having an implicit `AND` operator.

    Example filter:

    status.state = ACTIVE AND labels.env = staging AND labels.starred = *
    """

    def __init__(self,
        *,
        project_id : typing.Text = ...,
        region : typing.Text = ...,
        page_size : builtins.int = ...,
        page_token : typing.Text = ...,
        cluster_name : typing.Text = ...,
        job_state_matcher : global___ListJobsRequest.JobStateMatcher.ValueType = ...,
        filter : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["cluster_name",b"cluster_name","filter",b"filter","job_state_matcher",b"job_state_matcher","page_size",b"page_size","page_token",b"page_token","project_id",b"project_id","region",b"region"]) -> None: ...
global___ListJobsRequest = ListJobsRequest

class UpdateJobRequest(google.protobuf.message.Message):
    """A request to update a job."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    PROJECT_ID_FIELD_NUMBER: builtins.int
    REGION_FIELD_NUMBER: builtins.int
    JOB_ID_FIELD_NUMBER: builtins.int
    JOB_FIELD_NUMBER: builtins.int
    UPDATE_MASK_FIELD_NUMBER: builtins.int
    project_id: typing.Text = ...
    """Required. The ID of the Google Cloud Platform project that the job
    belongs to.
    """

    region: typing.Text = ...
    """Required. The Dataproc region in which to handle the request."""

    job_id: typing.Text = ...
    """Required. The job ID."""

    @property
    def job(self) -> global___Job:
        """Required. The changes to the job."""
        pass
    @property
    def update_mask(self) -> google.protobuf.field_mask_pb2.FieldMask:
        """Required. Specifies the path, relative to <code>Job</code>, of
        the field to update. For example, to update the labels of a Job the
        <code>update_mask</code> parameter would be specified as
        <code>labels</code>, and the `PATCH` request body would specify the new
        value. <strong>Note:</strong> Currently, <code>labels</code> is the only
        field that can be updated.
        """
        pass
    def __init__(self,
        *,
        project_id : typing.Text = ...,
        region : typing.Text = ...,
        job_id : typing.Text = ...,
        job : typing.Optional[global___Job] = ...,
        update_mask : typing.Optional[google.protobuf.field_mask_pb2.FieldMask] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["job",b"job","update_mask",b"update_mask"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["job",b"job","job_id",b"job_id","project_id",b"project_id","region",b"region","update_mask",b"update_mask"]) -> None: ...
global___UpdateJobRequest = UpdateJobRequest

class ListJobsResponse(google.protobuf.message.Message):
    """A list of jobs in a project."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    JOBS_FIELD_NUMBER: builtins.int
    NEXT_PAGE_TOKEN_FIELD_NUMBER: builtins.int
    @property
    def jobs(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___Job]:
        """Output only. Jobs list."""
        pass
    next_page_token: typing.Text = ...
    """Optional. This token is included in the response if there are more results
    to fetch. To fetch additional results, provide this value as the
    `page_token` in a subsequent <code>ListJobsRequest</code>.
    """

    def __init__(self,
        *,
        jobs : typing.Optional[typing.Iterable[global___Job]] = ...,
        next_page_token : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["jobs",b"jobs","next_page_token",b"next_page_token"]) -> None: ...
global___ListJobsResponse = ListJobsResponse

class CancelJobRequest(google.protobuf.message.Message):
    """A request to cancel a job."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    PROJECT_ID_FIELD_NUMBER: builtins.int
    REGION_FIELD_NUMBER: builtins.int
    JOB_ID_FIELD_NUMBER: builtins.int
    project_id: typing.Text = ...
    """Required. The ID of the Google Cloud Platform project that the job
    belongs to.
    """

    region: typing.Text = ...
    """Required. The Dataproc region in which to handle the request."""

    job_id: typing.Text = ...
    """Required. The job ID."""

    def __init__(self,
        *,
        project_id : typing.Text = ...,
        region : typing.Text = ...,
        job_id : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["job_id",b"job_id","project_id",b"project_id","region",b"region"]) -> None: ...
global___CancelJobRequest = CancelJobRequest

class DeleteJobRequest(google.protobuf.message.Message):
    """A request to delete a job."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    PROJECT_ID_FIELD_NUMBER: builtins.int
    REGION_FIELD_NUMBER: builtins.int
    JOB_ID_FIELD_NUMBER: builtins.int
    project_id: typing.Text = ...
    """Required. The ID of the Google Cloud Platform project that the job
    belongs to.
    """

    region: typing.Text = ...
    """Required. The Dataproc region in which to handle the request."""

    job_id: typing.Text = ...
    """Required. The job ID."""

    def __init__(self,
        *,
        project_id : typing.Text = ...,
        region : typing.Text = ...,
        job_id : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["job_id",b"job_id","project_id",b"project_id","region",b"region"]) -> None: ...
global___DeleteJobRequest = DeleteJobRequest
