"""
@generated by mypy-protobuf.  Do not edit manually!
isort:skip_file
"""
import builtins
import google.cloud.dialogflow.v2.audio_config_pb2
import google.cloud.dialogflow.v2.context_pb2
import google.cloud.dialogflow.v2.intent_pb2
import google.cloud.dialogflow.v2.session_entity_type_pb2
import google.protobuf.descriptor
import google.protobuf.duration_pb2
import google.protobuf.field_mask_pb2
import google.protobuf.internal.containers
import google.protobuf.internal.enum_type_wrapper
import google.protobuf.message
import google.protobuf.struct_pb2
import google.rpc.status_pb2
import google.type.latlng_pb2
import typing
import typing_extensions

DESCRIPTOR: google.protobuf.descriptor.FileDescriptor = ...

class DetectIntentRequest(google.protobuf.message.Message):
    """The request to detect user's intent."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    SESSION_FIELD_NUMBER: builtins.int
    QUERY_PARAMS_FIELD_NUMBER: builtins.int
    QUERY_INPUT_FIELD_NUMBER: builtins.int
    OUTPUT_AUDIO_CONFIG_FIELD_NUMBER: builtins.int
    OUTPUT_AUDIO_CONFIG_MASK_FIELD_NUMBER: builtins.int
    INPUT_AUDIO_FIELD_NUMBER: builtins.int
    session: typing.Text = ...
    """Required. The name of the session this query is sent to. Format:
    `projects/<Project ID>/agent/sessions/<Session ID>`, or
    `projects/<Project ID>/agent/environments/<Environment ID>/users/<User
    ID>/sessions/<Session ID>`. If `Environment ID` is not specified, we assume
    default 'draft' environment (`Environment ID` might be referred to as
    environment name at some places). If `User ID` is not specified, we are
    using "-". It's up to the API caller to choose an appropriate `Session ID`
    and `User Id`. They can be a random number or some type of user and session
    identifiers (preferably hashed). The length of the `Session ID` and
    `User ID` must not exceed 36 characters.

    For more information, see the [API interactions
    guide](https://cloud.google.com/dialogflow/docs/api-overview).

    Note: Always use agent versions for production traffic.
    See [Versions and
    environments](https://cloud.google.com/dialogflow/es/docs/agents-versions).
    """

    @property
    def query_params(self) -> global___QueryParameters:
        """The parameters of this query."""
        pass
    @property
    def query_input(self) -> global___QueryInput:
        """Required. The input specification. It can be set to:

        1.  an audio config
            which instructs the speech recognizer how to process the speech audio,

        2.  a conversational query in the form of text, or

        3.  an event that specifies which intent to trigger.
        """
        pass
    @property
    def output_audio_config(self) -> google.cloud.dialogflow.v2.audio_config_pb2.OutputAudioConfig:
        """Instructs the speech synthesizer how to generate the output
        audio. If this field is not set and agent-level speech synthesizer is not
        configured, no output audio is generated.
        """
        pass
    @property
    def output_audio_config_mask(self) -> google.protobuf.field_mask_pb2.FieldMask:
        """Mask for [output_audio_config][google.cloud.dialogflow.v2.DetectIntentRequest.output_audio_config] indicating which settings in this
        request-level config should override speech synthesizer settings defined at
        agent-level.

        If unspecified or empty, [output_audio_config][google.cloud.dialogflow.v2.DetectIntentRequest.output_audio_config] replaces the agent-level
        config in its entirety.
        """
        pass
    input_audio: builtins.bytes = ...
    """The natural language speech audio to be processed. This field
    should be populated iff `query_input` is set to an input audio config.
    A single request can contain up to 1 minute of speech audio data.
    """

    def __init__(self,
        *,
        session : typing.Text = ...,
        query_params : typing.Optional[global___QueryParameters] = ...,
        query_input : typing.Optional[global___QueryInput] = ...,
        output_audio_config : typing.Optional[google.cloud.dialogflow.v2.audio_config_pb2.OutputAudioConfig] = ...,
        output_audio_config_mask : typing.Optional[google.protobuf.field_mask_pb2.FieldMask] = ...,
        input_audio : builtins.bytes = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["output_audio_config",b"output_audio_config","output_audio_config_mask",b"output_audio_config_mask","query_input",b"query_input","query_params",b"query_params"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["input_audio",b"input_audio","output_audio_config",b"output_audio_config","output_audio_config_mask",b"output_audio_config_mask","query_input",b"query_input","query_params",b"query_params","session",b"session"]) -> None: ...
global___DetectIntentRequest = DetectIntentRequest

class DetectIntentResponse(google.protobuf.message.Message):
    """The message returned from the DetectIntent method."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    RESPONSE_ID_FIELD_NUMBER: builtins.int
    QUERY_RESULT_FIELD_NUMBER: builtins.int
    WEBHOOK_STATUS_FIELD_NUMBER: builtins.int
    OUTPUT_AUDIO_FIELD_NUMBER: builtins.int
    OUTPUT_AUDIO_CONFIG_FIELD_NUMBER: builtins.int
    response_id: typing.Text = ...
    """The unique identifier of the response. It can be used to
    locate a response in the training example set or for reporting issues.
    """

    @property
    def query_result(self) -> global___QueryResult:
        """The selected results of the conversational query or event processing.
        See `alternative_query_results` for additional potential results.
        """
        pass
    @property
    def webhook_status(self) -> google.rpc.status_pb2.Status:
        """Specifies the status of the webhook request."""
        pass
    output_audio: builtins.bytes = ...
    """The audio data bytes encoded as specified in the request.
    Note: The output audio is generated based on the values of default platform
    text responses found in the `query_result.fulfillment_messages` field. If
    multiple default text responses exist, they will be concatenated when
    generating audio. If no default platform text responses exist, the
    generated audio content will be empty.

    In some scenarios, multiple output audio fields may be present in the
    response structure. In these cases, only the top-most-level audio output
    has content.
    """

    @property
    def output_audio_config(self) -> google.cloud.dialogflow.v2.audio_config_pb2.OutputAudioConfig:
        """The config used by the speech synthesizer to generate the output audio."""
        pass
    def __init__(self,
        *,
        response_id : typing.Text = ...,
        query_result : typing.Optional[global___QueryResult] = ...,
        webhook_status : typing.Optional[google.rpc.status_pb2.Status] = ...,
        output_audio : builtins.bytes = ...,
        output_audio_config : typing.Optional[google.cloud.dialogflow.v2.audio_config_pb2.OutputAudioConfig] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["output_audio_config",b"output_audio_config","query_result",b"query_result","webhook_status",b"webhook_status"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["output_audio",b"output_audio","output_audio_config",b"output_audio_config","query_result",b"query_result","response_id",b"response_id","webhook_status",b"webhook_status"]) -> None: ...
global___DetectIntentResponse = DetectIntentResponse

class QueryParameters(google.protobuf.message.Message):
    """Represents the parameters of the conversational query."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class WebhookHeadersEntry(google.protobuf.message.Message):
        DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
        KEY_FIELD_NUMBER: builtins.int
        VALUE_FIELD_NUMBER: builtins.int
        key: typing.Text = ...
        value: typing.Text = ...
        def __init__(self,
            *,
            key : typing.Text = ...,
            value : typing.Text = ...,
            ) -> None: ...
        def ClearField(self, field_name: typing_extensions.Literal["key",b"key","value",b"value"]) -> None: ...

    TIME_ZONE_FIELD_NUMBER: builtins.int
    GEO_LOCATION_FIELD_NUMBER: builtins.int
    CONTEXTS_FIELD_NUMBER: builtins.int
    RESET_CONTEXTS_FIELD_NUMBER: builtins.int
    SESSION_ENTITY_TYPES_FIELD_NUMBER: builtins.int
    PAYLOAD_FIELD_NUMBER: builtins.int
    SENTIMENT_ANALYSIS_REQUEST_CONFIG_FIELD_NUMBER: builtins.int
    WEBHOOK_HEADERS_FIELD_NUMBER: builtins.int
    time_zone: typing.Text = ...
    """The time zone of this conversational query from the
    [time zone database](https://www.iana.org/time-zones), e.g.,
    America/New_York, Europe/Paris. If not provided, the time zone specified in
    agent settings is used.
    """

    @property
    def geo_location(self) -> google.type.latlng_pb2.LatLng:
        """The geo location of this conversational query."""
        pass
    @property
    def contexts(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[google.cloud.dialogflow.v2.context_pb2.Context]:
        """The collection of contexts to be activated before this query is
        executed.
        """
        pass
    reset_contexts: builtins.bool = ...
    """Specifies whether to delete all contexts in the current session
    before the new ones are activated.
    """

    @property
    def session_entity_types(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[google.cloud.dialogflow.v2.session_entity_type_pb2.SessionEntityType]:
        """Additional session entity types to replace or extend developer
        entity types with. The entity synonyms apply to all languages and persist
        for the session of this query.
        """
        pass
    @property
    def payload(self) -> google.protobuf.struct_pb2.Struct:
        """This field can be used to pass custom data to your webhook.
        Arbitrary JSON objects are supported.
        If supplied, the value is used to populate the
        `WebhookRequest.original_detect_intent_request.payload`
        field sent to your webhook.
        """
        pass
    @property
    def sentiment_analysis_request_config(self) -> global___SentimentAnalysisRequestConfig:
        """Configures the type of sentiment analysis to perform. If not
        provided, sentiment analysis is not performed.
        """
        pass
    @property
    def webhook_headers(self) -> google.protobuf.internal.containers.ScalarMap[typing.Text, typing.Text]:
        """This field can be used to pass HTTP headers for a webhook
        call. These headers will be sent to webhook along with the headers that
        have been configured through the Dialogflow web console. The headers
        defined within this field will overwrite the headers configured through the
        Dialogflow console if there is a conflict. Header names are
        case-insensitive. Google's specified headers are not allowed. Including:
        "Host", "Content-Length", "Connection", "From", "User-Agent",
        "Accept-Encoding", "If-Modified-Since", "If-None-Match", "X-Forwarded-For",
        etc.
        """
        pass
    def __init__(self,
        *,
        time_zone : typing.Text = ...,
        geo_location : typing.Optional[google.type.latlng_pb2.LatLng] = ...,
        contexts : typing.Optional[typing.Iterable[google.cloud.dialogflow.v2.context_pb2.Context]] = ...,
        reset_contexts : builtins.bool = ...,
        session_entity_types : typing.Optional[typing.Iterable[google.cloud.dialogflow.v2.session_entity_type_pb2.SessionEntityType]] = ...,
        payload : typing.Optional[google.protobuf.struct_pb2.Struct] = ...,
        sentiment_analysis_request_config : typing.Optional[global___SentimentAnalysisRequestConfig] = ...,
        webhook_headers : typing.Optional[typing.Mapping[typing.Text, typing.Text]] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["geo_location",b"geo_location","payload",b"payload","sentiment_analysis_request_config",b"sentiment_analysis_request_config"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["contexts",b"contexts","geo_location",b"geo_location","payload",b"payload","reset_contexts",b"reset_contexts","sentiment_analysis_request_config",b"sentiment_analysis_request_config","session_entity_types",b"session_entity_types","time_zone",b"time_zone","webhook_headers",b"webhook_headers"]) -> None: ...
global___QueryParameters = QueryParameters

class QueryInput(google.protobuf.message.Message):
    """Represents the query input. It can contain either:

    1.  An audio config which
        instructs the speech recognizer how to process the speech audio.

    2.  A conversational query in the form of text,.

    3.  An event that specifies which intent to trigger.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    AUDIO_CONFIG_FIELD_NUMBER: builtins.int
    TEXT_FIELD_NUMBER: builtins.int
    EVENT_FIELD_NUMBER: builtins.int
    @property
    def audio_config(self) -> google.cloud.dialogflow.v2.audio_config_pb2.InputAudioConfig:
        """Instructs the speech recognizer how to process the speech audio."""
        pass
    @property
    def text(self) -> global___TextInput:
        """The natural language text to be processed."""
        pass
    @property
    def event(self) -> global___EventInput:
        """The event to be processed."""
        pass
    def __init__(self,
        *,
        audio_config : typing.Optional[google.cloud.dialogflow.v2.audio_config_pb2.InputAudioConfig] = ...,
        text : typing.Optional[global___TextInput] = ...,
        event : typing.Optional[global___EventInput] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["audio_config",b"audio_config","event",b"event","input",b"input","text",b"text"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["audio_config",b"audio_config","event",b"event","input",b"input","text",b"text"]) -> None: ...
    def WhichOneof(self, oneof_group: typing_extensions.Literal["input",b"input"]) -> typing.Optional[typing_extensions.Literal["audio_config","text","event"]]: ...
global___QueryInput = QueryInput

class QueryResult(google.protobuf.message.Message):
    """Represents the result of conversational query or event processing."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    QUERY_TEXT_FIELD_NUMBER: builtins.int
    LANGUAGE_CODE_FIELD_NUMBER: builtins.int
    SPEECH_RECOGNITION_CONFIDENCE_FIELD_NUMBER: builtins.int
    ACTION_FIELD_NUMBER: builtins.int
    PARAMETERS_FIELD_NUMBER: builtins.int
    ALL_REQUIRED_PARAMS_PRESENT_FIELD_NUMBER: builtins.int
    CANCELS_SLOT_FILLING_FIELD_NUMBER: builtins.int
    FULFILLMENT_TEXT_FIELD_NUMBER: builtins.int
    FULFILLMENT_MESSAGES_FIELD_NUMBER: builtins.int
    WEBHOOK_SOURCE_FIELD_NUMBER: builtins.int
    WEBHOOK_PAYLOAD_FIELD_NUMBER: builtins.int
    OUTPUT_CONTEXTS_FIELD_NUMBER: builtins.int
    INTENT_FIELD_NUMBER: builtins.int
    INTENT_DETECTION_CONFIDENCE_FIELD_NUMBER: builtins.int
    DIAGNOSTIC_INFO_FIELD_NUMBER: builtins.int
    SENTIMENT_ANALYSIS_RESULT_FIELD_NUMBER: builtins.int
    query_text: typing.Text = ...
    """The original conversational query text:

    - If natural language text was provided as input, `query_text` contains
      a copy of the input.
    - If natural language speech audio was provided as input, `query_text`
      contains the speech recognition result. If speech recognizer produced
      multiple alternatives, a particular one is picked.
    - If automatic spell correction is enabled, `query_text` will contain the
      corrected user input.
    """

    language_code: typing.Text = ...
    """The language that was triggered during intent detection.
    See [Language
    Support](https://cloud.google.com/dialogflow/docs/reference/language)
    for a list of the currently supported language codes.
    """

    speech_recognition_confidence: builtins.float = ...
    """The Speech recognition confidence between 0.0 and 1.0. A higher number
    indicates an estimated greater likelihood that the recognized words are
    correct. The default of 0.0 is a sentinel value indicating that confidence
    was not set.

    This field is not guaranteed to be accurate or set. In particular this
    field isn't set for StreamingDetectIntent since the streaming endpoint has
    separate confidence estimates per portion of the audio in
    StreamingRecognitionResult.
    """

    action: typing.Text = ...
    """The action name from the matched intent."""

    @property
    def parameters(self) -> google.protobuf.struct_pb2.Struct:
        """The collection of extracted parameters.

        Depending on your protocol or client library language, this is a
        map, associative array, symbol table, dictionary, or JSON object
        composed of a collection of (MapKey, MapValue) pairs:

        -   MapKey type: string
        -   MapKey value: parameter name
        -   MapValue type:
            -   If parameter's entity type is a composite entity: map
            -   Else: depending on parameter value type, could be one of string,
                number, boolean, null, list or map
        -   MapValue value:
            -   If parameter's entity type is a composite entity:
                map from composite entity property names to property values
            -   Else: parameter value
        """
        pass
    all_required_params_present: builtins.bool = ...
    """This field is set to:

    - `false` if the matched intent has required parameters and not all of
       the required parameter values have been collected.
    - `true` if all required parameter values have been collected, or if the
       matched intent doesn't contain any required parameters.
    """

    cancels_slot_filling: builtins.bool = ...
    """Indicates whether the conversational query triggers a cancellation for slot
    filling.
    """

    fulfillment_text: typing.Text = ...
    """The text to be pronounced to the user or shown on the screen.
    Note: This is a legacy field, `fulfillment_messages` should be preferred.
    """

    @property
    def fulfillment_messages(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[google.cloud.dialogflow.v2.intent_pb2.Intent.Message]:
        """The collection of rich messages to present to the user."""
        pass
    webhook_source: typing.Text = ...
    """If the query was fulfilled by a webhook call, this field is set to the
    value of the `source` field returned in the webhook response.
    """

    @property
    def webhook_payload(self) -> google.protobuf.struct_pb2.Struct:
        """If the query was fulfilled by a webhook call, this field is set to the
        value of the `payload` field returned in the webhook response.
        """
        pass
    @property
    def output_contexts(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[google.cloud.dialogflow.v2.context_pb2.Context]:
        """The collection of output contexts. If applicable,
        `output_contexts.parameters` contains entries with name
        `<parameter name>.original` containing the original parameter values
        before the query.
        """
        pass
    @property
    def intent(self) -> google.cloud.dialogflow.v2.intent_pb2.Intent:
        """The intent that matched the conversational query. Some, not
        all fields are filled in this message, including but not limited to:
        `name`, `display_name`, `end_interaction` and `is_fallback`.
        """
        pass
    intent_detection_confidence: builtins.float = ...
    """The intent detection confidence. Values range from 0.0
    (completely uncertain) to 1.0 (completely certain).
    This value is for informational purpose only and is only used to
    help match the best intent within the classification threshold.
    This value may change for the same end-user expression at any time due to a
    model retraining or change in implementation.
    If there are `multiple knowledge_answers` messages, this value is set to
    the greatest `knowledgeAnswers.match_confidence` value in the list.
    """

    @property
    def diagnostic_info(self) -> google.protobuf.struct_pb2.Struct:
        """Free-form diagnostic information for the associated detect intent request.
        The fields of this data can change without notice, so you should not write
        code that depends on its structure.
        The data may contain:

        - webhook call latency
        - webhook errors
        """
        pass
    @property
    def sentiment_analysis_result(self) -> global___SentimentAnalysisResult:
        """The sentiment analysis result, which depends on the
        `sentiment_analysis_request_config` specified in the request.
        """
        pass
    def __init__(self,
        *,
        query_text : typing.Text = ...,
        language_code : typing.Text = ...,
        speech_recognition_confidence : builtins.float = ...,
        action : typing.Text = ...,
        parameters : typing.Optional[google.protobuf.struct_pb2.Struct] = ...,
        all_required_params_present : builtins.bool = ...,
        cancels_slot_filling : builtins.bool = ...,
        fulfillment_text : typing.Text = ...,
        fulfillment_messages : typing.Optional[typing.Iterable[google.cloud.dialogflow.v2.intent_pb2.Intent.Message]] = ...,
        webhook_source : typing.Text = ...,
        webhook_payload : typing.Optional[google.protobuf.struct_pb2.Struct] = ...,
        output_contexts : typing.Optional[typing.Iterable[google.cloud.dialogflow.v2.context_pb2.Context]] = ...,
        intent : typing.Optional[google.cloud.dialogflow.v2.intent_pb2.Intent] = ...,
        intent_detection_confidence : builtins.float = ...,
        diagnostic_info : typing.Optional[google.protobuf.struct_pb2.Struct] = ...,
        sentiment_analysis_result : typing.Optional[global___SentimentAnalysisResult] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["diagnostic_info",b"diagnostic_info","intent",b"intent","parameters",b"parameters","sentiment_analysis_result",b"sentiment_analysis_result","webhook_payload",b"webhook_payload"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["action",b"action","all_required_params_present",b"all_required_params_present","cancels_slot_filling",b"cancels_slot_filling","diagnostic_info",b"diagnostic_info","fulfillment_messages",b"fulfillment_messages","fulfillment_text",b"fulfillment_text","intent",b"intent","intent_detection_confidence",b"intent_detection_confidence","language_code",b"language_code","output_contexts",b"output_contexts","parameters",b"parameters","query_text",b"query_text","sentiment_analysis_result",b"sentiment_analysis_result","speech_recognition_confidence",b"speech_recognition_confidence","webhook_payload",b"webhook_payload","webhook_source",b"webhook_source"]) -> None: ...
global___QueryResult = QueryResult

class StreamingDetectIntentRequest(google.protobuf.message.Message):
    """The top-level message sent by the client to the
    [Sessions.StreamingDetectIntent][google.cloud.dialogflow.v2.Sessions.StreamingDetectIntent] method.

    Multiple request messages should be sent in order:

    1.  The first message must contain
    [session][google.cloud.dialogflow.v2.StreamingDetectIntentRequest.session],
        [query_input][google.cloud.dialogflow.v2.StreamingDetectIntentRequest.query_input] plus optionally
        [query_params][google.cloud.dialogflow.v2.StreamingDetectIntentRequest.query_params]. If the client
        wants to receive an audio response, it should also contain
        [output_audio_config][google.cloud.dialogflow.v2.StreamingDetectIntentRequest.output_audio_config].
        The message must not contain
        [input_audio][google.cloud.dialogflow.v2.StreamingDetectIntentRequest.input_audio].
    2.  If [query_input][google.cloud.dialogflow.v2.StreamingDetectIntentRequest.query_input] was set to
        [query_input.audio_config][google.cloud.dialogflow.v2.InputAudioConfig], all subsequent
        messages must contain
        [input_audio][google.cloud.dialogflow.v2.StreamingDetectIntentRequest.input_audio] to continue with
        Speech recognition.
        If you decide to rather detect an intent from text input after you
        already started Speech recognition, please send a message with
        [query_input.text][google.cloud.dialogflow.v2.QueryInput.text].

        However, note that:

        * Dialogflow will bill you for the audio duration so far.
        * Dialogflow discards all Speech recognition results in favor of the
          input text.
        * Dialogflow will use the language code from the first message.

    After you sent all input, you must half-close or abort the request stream.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    SESSION_FIELD_NUMBER: builtins.int
    QUERY_PARAMS_FIELD_NUMBER: builtins.int
    QUERY_INPUT_FIELD_NUMBER: builtins.int
    SINGLE_UTTERANCE_FIELD_NUMBER: builtins.int
    OUTPUT_AUDIO_CONFIG_FIELD_NUMBER: builtins.int
    OUTPUT_AUDIO_CONFIG_MASK_FIELD_NUMBER: builtins.int
    INPUT_AUDIO_FIELD_NUMBER: builtins.int
    session: typing.Text = ...
    """Required. The name of the session the query is sent to.
    Format of the session name:
    `projects/<Project ID>/agent/sessions/<Session ID>`, or
    `projects/<Project ID>/agent/environments/<Environment ID>/users/<User
    ID>/sessions/<Session ID>`. If `Environment ID` is not specified, we assume
    default 'draft' environment. If `User ID` is not specified, we are using
    "-". It's up to the API caller to choose an appropriate `Session ID` and
    `User Id`. They can be a random number or some type of user and session
    identifiers (preferably hashed). The length of the `Session ID` and
    `User ID` must not exceed 36 characters.

    For more information, see the [API interactions
    guide](https://cloud.google.com/dialogflow/docs/api-overview).

    Note: Always use agent versions for production traffic.
    See [Versions and
    environments](https://cloud.google.com/dialogflow/es/docs/agents-versions).
    """

    @property
    def query_params(self) -> global___QueryParameters:
        """The parameters of this query."""
        pass
    @property
    def query_input(self) -> global___QueryInput:
        """Required. The input specification. It can be set to:

        1.  an audio config which instructs the speech recognizer how to process
            the speech audio,

        2.  a conversational query in the form of text, or

        3.  an event that specifies which intent to trigger.
        """
        pass
    single_utterance: builtins.bool = ...
    """Please use [InputAudioConfig.single_utterance][google.cloud.dialogflow.v2.InputAudioConfig.single_utterance] instead.
    If `false` (default), recognition does not cease until
    the client closes the stream. If `true`, the recognizer will detect a
    single spoken utterance in input audio. Recognition ceases when it detects
    the audio's voice has stopped or paused. In this case, once a detected
    intent is received, the client should close the stream and start a new
    request with a new stream as needed.
    This setting is ignored when `query_input` is a piece of text or an event.
    """

    @property
    def output_audio_config(self) -> google.cloud.dialogflow.v2.audio_config_pb2.OutputAudioConfig:
        """Instructs the speech synthesizer how to generate the output
        audio. If this field is not set and agent-level speech synthesizer is not
        configured, no output audio is generated.
        """
        pass
    @property
    def output_audio_config_mask(self) -> google.protobuf.field_mask_pb2.FieldMask:
        """Mask for [output_audio_config][google.cloud.dialogflow.v2.StreamingDetectIntentRequest.output_audio_config] indicating which settings in this
        request-level config should override speech synthesizer settings defined at
        agent-level.

        If unspecified or empty, [output_audio_config][google.cloud.dialogflow.v2.StreamingDetectIntentRequest.output_audio_config] replaces the agent-level
        config in its entirety.
        """
        pass
    input_audio: builtins.bytes = ...
    """The input audio content to be recognized. Must be sent if
    `query_input` was set to a streaming input audio config. The complete audio
    over all streaming messages must not exceed 1 minute.
    """

    def __init__(self,
        *,
        session : typing.Text = ...,
        query_params : typing.Optional[global___QueryParameters] = ...,
        query_input : typing.Optional[global___QueryInput] = ...,
        single_utterance : builtins.bool = ...,
        output_audio_config : typing.Optional[google.cloud.dialogflow.v2.audio_config_pb2.OutputAudioConfig] = ...,
        output_audio_config_mask : typing.Optional[google.protobuf.field_mask_pb2.FieldMask] = ...,
        input_audio : builtins.bytes = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["output_audio_config",b"output_audio_config","output_audio_config_mask",b"output_audio_config_mask","query_input",b"query_input","query_params",b"query_params"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["input_audio",b"input_audio","output_audio_config",b"output_audio_config","output_audio_config_mask",b"output_audio_config_mask","query_input",b"query_input","query_params",b"query_params","session",b"session","single_utterance",b"single_utterance"]) -> None: ...
global___StreamingDetectIntentRequest = StreamingDetectIntentRequest

class StreamingDetectIntentResponse(google.protobuf.message.Message):
    """The top-level message returned from the
    `StreamingDetectIntent` method.

    Multiple response messages can be returned in order:

    1.  If the `StreamingDetectIntentRequest.input_audio` field was
        set, the `recognition_result` field is populated for one
        or more messages.
        See the [StreamingRecognitionResult][google.cloud.dialogflow.v2.StreamingRecognitionResult] message for details
        about the result message sequence.

    2.  The next message contains `response_id`, `query_result`
        and optionally `webhook_status` if a WebHook was called.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    RESPONSE_ID_FIELD_NUMBER: builtins.int
    RECOGNITION_RESULT_FIELD_NUMBER: builtins.int
    QUERY_RESULT_FIELD_NUMBER: builtins.int
    WEBHOOK_STATUS_FIELD_NUMBER: builtins.int
    OUTPUT_AUDIO_FIELD_NUMBER: builtins.int
    OUTPUT_AUDIO_CONFIG_FIELD_NUMBER: builtins.int
    response_id: typing.Text = ...
    """The unique identifier of the response. It can be used to
    locate a response in the training example set or for reporting issues.
    """

    @property
    def recognition_result(self) -> global___StreamingRecognitionResult:
        """The result of speech recognition."""
        pass
    @property
    def query_result(self) -> global___QueryResult:
        """The result of the conversational query or event processing."""
        pass
    @property
    def webhook_status(self) -> google.rpc.status_pb2.Status:
        """Specifies the status of the webhook request."""
        pass
    output_audio: builtins.bytes = ...
    """The audio data bytes encoded as specified in the request.
    Note: The output audio is generated based on the values of default platform
    text responses found in the `query_result.fulfillment_messages` field. If
    multiple default text responses exist, they will be concatenated when
    generating audio. If no default platform text responses exist, the
    generated audio content will be empty.

    In some scenarios, multiple output audio fields may be present in the
    response structure. In these cases, only the top-most-level audio output
    has content.
    """

    @property
    def output_audio_config(self) -> google.cloud.dialogflow.v2.audio_config_pb2.OutputAudioConfig:
        """The config used by the speech synthesizer to generate the output audio."""
        pass
    def __init__(self,
        *,
        response_id : typing.Text = ...,
        recognition_result : typing.Optional[global___StreamingRecognitionResult] = ...,
        query_result : typing.Optional[global___QueryResult] = ...,
        webhook_status : typing.Optional[google.rpc.status_pb2.Status] = ...,
        output_audio : builtins.bytes = ...,
        output_audio_config : typing.Optional[google.cloud.dialogflow.v2.audio_config_pb2.OutputAudioConfig] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["output_audio_config",b"output_audio_config","query_result",b"query_result","recognition_result",b"recognition_result","webhook_status",b"webhook_status"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["output_audio",b"output_audio","output_audio_config",b"output_audio_config","query_result",b"query_result","recognition_result",b"recognition_result","response_id",b"response_id","webhook_status",b"webhook_status"]) -> None: ...
global___StreamingDetectIntentResponse = StreamingDetectIntentResponse

class StreamingRecognitionResult(google.protobuf.message.Message):
    """Contains a speech recognition result corresponding to a portion of the audio
    that is currently being processed or an indication that this is the end
    of the single requested utterance.

    While end-user audio is being processed, Dialogflow sends a series of
    results. Each result may contain a `transcript` value. A transcript
    represents a portion of the utterance. While the recognizer is processing
    audio, transcript values may be interim values or finalized values.
    Once a transcript is finalized, the `is_final` value is set to true and
    processing continues for the next transcript.

    If `StreamingDetectIntentRequest.query_input.audio_config.single_utterance`
    was true, and the recognizer has completed processing audio,
    the `message_type` value is set to `END_OF_SINGLE_UTTERANCE and the
    following (last) result contains the last finalized transcript.

    The complete end-user utterance is determined by concatenating the
    finalized transcript values received for the series of results.

    In the following example, single utterance is enabled. In the case where
    single utterance is not enabled, result 7 would not occur.

    ```
    Num | transcript              | message_type            | is_final
    --- | ----------------------- | ----------------------- | --------
    1   | "tube"                  | TRANSCRIPT              | false
    2   | "to be a"               | TRANSCRIPT              | false
    3   | "to be"                 | TRANSCRIPT              | false
    4   | "to be or not to be"    | TRANSCRIPT              | true
    5   | "that's"                | TRANSCRIPT              | false
    6   | "that is                | TRANSCRIPT              | false
    7   | unset                   | END_OF_SINGLE_UTTERANCE | unset
    8   | " that is the question" | TRANSCRIPT              | true
    ```

    Concatenating the finalized transcripts with `is_final` set to true,
    the complete utterance becomes "to be or not to be that is the question".
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class _MessageType:
        ValueType = typing.NewType('ValueType', builtins.int)
        V: typing_extensions.TypeAlias = ValueType
    class _MessageTypeEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_MessageType.ValueType], builtins.type):
        DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor = ...
        MESSAGE_TYPE_UNSPECIFIED: StreamingRecognitionResult.MessageType.ValueType = ...  # 0
        """Not specified. Should never be used."""

        TRANSCRIPT: StreamingRecognitionResult.MessageType.ValueType = ...  # 1
        """Message contains a (possibly partial) transcript."""

        END_OF_SINGLE_UTTERANCE: StreamingRecognitionResult.MessageType.ValueType = ...  # 2
        """Event indicates that the server has detected the end of the user's speech
        utterance and expects no additional inputs.
        Therefore, the server will not process additional audio (although it may subsequently return additional results). The
        client should stop sending additional audio data, half-close the gRPC
        connection, and wait for any additional results until the server closes
        the gRPC connection. This message is only sent if `single_utterance` was
        set to `true`, and is not used otherwise.
        """

    class MessageType(_MessageType, metaclass=_MessageTypeEnumTypeWrapper):
        """Type of the response message."""
        pass

    MESSAGE_TYPE_UNSPECIFIED: StreamingRecognitionResult.MessageType.ValueType = ...  # 0
    """Not specified. Should never be used."""

    TRANSCRIPT: StreamingRecognitionResult.MessageType.ValueType = ...  # 1
    """Message contains a (possibly partial) transcript."""

    END_OF_SINGLE_UTTERANCE: StreamingRecognitionResult.MessageType.ValueType = ...  # 2
    """Event indicates that the server has detected the end of the user's speech
    utterance and expects no additional inputs.
    Therefore, the server will not process additional audio (although it may subsequently return additional results). The
    client should stop sending additional audio data, half-close the gRPC
    connection, and wait for any additional results until the server closes
    the gRPC connection. This message is only sent if `single_utterance` was
    set to `true`, and is not used otherwise.
    """


    MESSAGE_TYPE_FIELD_NUMBER: builtins.int
    TRANSCRIPT_FIELD_NUMBER: builtins.int
    IS_FINAL_FIELD_NUMBER: builtins.int
    CONFIDENCE_FIELD_NUMBER: builtins.int
    SPEECH_WORD_INFO_FIELD_NUMBER: builtins.int
    SPEECH_END_OFFSET_FIELD_NUMBER: builtins.int
    LANGUAGE_CODE_FIELD_NUMBER: builtins.int
    message_type: global___StreamingRecognitionResult.MessageType.ValueType = ...
    """Type of the result message."""

    transcript: typing.Text = ...
    """Transcript text representing the words that the user spoke.
    Populated if and only if `message_type` = `TRANSCRIPT`.
    """

    is_final: builtins.bool = ...
    """If `false`, the `StreamingRecognitionResult` represents an
    interim result that may change. If `true`, the recognizer will not return
    any further hypotheses about this piece of the audio. May only be populated
    for `message_type` = `TRANSCRIPT`.
    """

    confidence: builtins.float = ...
    """The Speech confidence between 0.0 and 1.0 for the current portion of audio.
    A higher number indicates an estimated greater likelihood that the
    recognized words are correct. The default of 0.0 is a sentinel value
    indicating that confidence was not set.

    This field is typically only provided if `is_final` is true and you should
    not rely on it being accurate or even set.
    """

    @property
    def speech_word_info(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[google.cloud.dialogflow.v2.audio_config_pb2.SpeechWordInfo]:
        """Word-specific information for the words recognized by Speech in
        [transcript][google.cloud.dialogflow.v2.StreamingRecognitionResult.transcript]. Populated if and only if `message_type` = `TRANSCRIPT` and
        [InputAudioConfig.enable_word_info] is set.
        """
        pass
    @property
    def speech_end_offset(self) -> google.protobuf.duration_pb2.Duration:
        """Time offset of the end of this Speech recognition result relative to the
        beginning of the audio. Only populated for `message_type` = `TRANSCRIPT`.
        """
        pass
    language_code: typing.Text = ...
    """Detected language code for the transcript."""

    def __init__(self,
        *,
        message_type : global___StreamingRecognitionResult.MessageType.ValueType = ...,
        transcript : typing.Text = ...,
        is_final : builtins.bool = ...,
        confidence : builtins.float = ...,
        speech_word_info : typing.Optional[typing.Iterable[google.cloud.dialogflow.v2.audio_config_pb2.SpeechWordInfo]] = ...,
        speech_end_offset : typing.Optional[google.protobuf.duration_pb2.Duration] = ...,
        language_code : typing.Text = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["speech_end_offset",b"speech_end_offset"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["confidence",b"confidence","is_final",b"is_final","language_code",b"language_code","message_type",b"message_type","speech_end_offset",b"speech_end_offset","speech_word_info",b"speech_word_info","transcript",b"transcript"]) -> None: ...
global___StreamingRecognitionResult = StreamingRecognitionResult

class TextInput(google.protobuf.message.Message):
    """Represents the natural language text to be processed."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    TEXT_FIELD_NUMBER: builtins.int
    LANGUAGE_CODE_FIELD_NUMBER: builtins.int
    text: typing.Text = ...
    """Required. The UTF-8 encoded natural language text to be processed.
    Text length must not exceed 256 characters.
    """

    language_code: typing.Text = ...
    """Required. The language of this conversational query. See [Language
    Support](https://cloud.google.com/dialogflow/docs/reference/language)
    for a list of the currently supported language codes. Note that queries in
    the same session do not necessarily need to specify the same language.
    """

    def __init__(self,
        *,
        text : typing.Text = ...,
        language_code : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["language_code",b"language_code","text",b"text"]) -> None: ...
global___TextInput = TextInput

class EventInput(google.protobuf.message.Message):
    """Events allow for matching intents by event name instead of the natural
    language input. For instance, input `<event: { name: "welcome_event",
    parameters: { name: "Sam" } }>` can trigger a personalized welcome response.
    The parameter `name` may be used by the agent in the response:
    `"Hello #welcome_event.name! What can I do for you today?"`.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    NAME_FIELD_NUMBER: builtins.int
    PARAMETERS_FIELD_NUMBER: builtins.int
    LANGUAGE_CODE_FIELD_NUMBER: builtins.int
    name: typing.Text = ...
    """Required. The unique identifier of the event."""

    @property
    def parameters(self) -> google.protobuf.struct_pb2.Struct:
        """The collection of parameters associated with the event.

        Depending on your protocol or client library language, this is a
        map, associative array, symbol table, dictionary, or JSON object
        composed of a collection of (MapKey, MapValue) pairs:

        -   MapKey type: string
        -   MapKey value: parameter name
        -   MapValue type:
            -   If parameter's entity type is a composite entity: map
            -   Else: depending on parameter value type, could be one of string,
                number, boolean, null, list or map
        -   MapValue value:
            -   If parameter's entity type is a composite entity:
                map from composite entity property names to property values
            -   Else: parameter value
        """
        pass
    language_code: typing.Text = ...
    """Required. The language of this query. See [Language
    Support](https://cloud.google.com/dialogflow/docs/reference/language)
    for a list of the currently supported language codes. Note that queries in
    the same session do not necessarily need to specify the same language.
    """

    def __init__(self,
        *,
        name : typing.Text = ...,
        parameters : typing.Optional[google.protobuf.struct_pb2.Struct] = ...,
        language_code : typing.Text = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["parameters",b"parameters"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["language_code",b"language_code","name",b"name","parameters",b"parameters"]) -> None: ...
global___EventInput = EventInput

class SentimentAnalysisRequestConfig(google.protobuf.message.Message):
    """Configures the types of sentiment analysis to perform."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    ANALYZE_QUERY_TEXT_SENTIMENT_FIELD_NUMBER: builtins.int
    analyze_query_text_sentiment: builtins.bool = ...
    """Instructs the service to perform sentiment analysis on
    `query_text`. If not provided, sentiment analysis is not performed on
    `query_text`.
    """

    def __init__(self,
        *,
        analyze_query_text_sentiment : builtins.bool = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["analyze_query_text_sentiment",b"analyze_query_text_sentiment"]) -> None: ...
global___SentimentAnalysisRequestConfig = SentimentAnalysisRequestConfig

class SentimentAnalysisResult(google.protobuf.message.Message):
    """The result of sentiment analysis. Sentiment analysis inspects user input
    and identifies the prevailing subjective opinion, especially to determine a
    user's attitude as positive, negative, or neutral.
    For [Participants.DetectIntent][], it needs to be configured in
    [DetectIntentRequest.query_params][google.cloud.dialogflow.v2.DetectIntentRequest.query_params]. For
    [Participants.StreamingDetectIntent][], it needs to be configured in
    [StreamingDetectIntentRequest.query_params][google.cloud.dialogflow.v2.StreamingDetectIntentRequest.query_params].
    And for [Participants.AnalyzeContent][google.cloud.dialogflow.v2.Participants.AnalyzeContent] and
    [Participants.StreamingAnalyzeContent][google.cloud.dialogflow.v2.Participants.StreamingAnalyzeContent], it needs to be configured in
    [ConversationProfile.human_agent_assistant_config][google.cloud.dialogflow.v2.ConversationProfile.human_agent_assistant_config]
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    QUERY_TEXT_SENTIMENT_FIELD_NUMBER: builtins.int
    @property
    def query_text_sentiment(self) -> global___Sentiment:
        """The sentiment analysis result for `query_text`."""
        pass
    def __init__(self,
        *,
        query_text_sentiment : typing.Optional[global___Sentiment] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["query_text_sentiment",b"query_text_sentiment"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["query_text_sentiment",b"query_text_sentiment"]) -> None: ...
global___SentimentAnalysisResult = SentimentAnalysisResult

class Sentiment(google.protobuf.message.Message):
    """The sentiment, such as positive/negative feeling or association, for a unit
    of analysis, such as the query text.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    SCORE_FIELD_NUMBER: builtins.int
    MAGNITUDE_FIELD_NUMBER: builtins.int
    score: builtins.float = ...
    """Sentiment score between -1.0 (negative sentiment) and 1.0 (positive
    sentiment).
    """

    magnitude: builtins.float = ...
    """A non-negative number in the [0, +inf) range, which represents the absolute
    magnitude of sentiment, regardless of score (positive or negative).
    """

    def __init__(self,
        *,
        score : builtins.float = ...,
        magnitude : builtins.float = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["magnitude",b"magnitude","score",b"score"]) -> None: ...
global___Sentiment = Sentiment
