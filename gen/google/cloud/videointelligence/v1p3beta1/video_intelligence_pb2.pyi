"""
@generated by mypy-protobuf.  Do not edit manually!
isort:skip_file
"""
import builtins
import google.protobuf.descriptor
import google.protobuf.duration_pb2
import google.protobuf.internal.containers
import google.protobuf.internal.enum_type_wrapper
import google.protobuf.message
import google.protobuf.timestamp_pb2
import google.rpc.status_pb2
import typing
import typing_extensions

DESCRIPTOR: google.protobuf.descriptor.FileDescriptor = ...

class _LabelDetectionMode:
    ValueType = typing.NewType('ValueType', builtins.int)
    V: typing_extensions.TypeAlias = ValueType
class _LabelDetectionModeEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_LabelDetectionMode.ValueType], builtins.type):
    DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor = ...
    LABEL_DETECTION_MODE_UNSPECIFIED: LabelDetectionMode.ValueType = ...  # 0
    """Unspecified."""

    SHOT_MODE: LabelDetectionMode.ValueType = ...  # 1
    """Detect shot-level labels."""

    FRAME_MODE: LabelDetectionMode.ValueType = ...  # 2
    """Detect frame-level labels."""

    SHOT_AND_FRAME_MODE: LabelDetectionMode.ValueType = ...  # 3
    """Detect both shot-level and frame-level labels."""

class LabelDetectionMode(_LabelDetectionMode, metaclass=_LabelDetectionModeEnumTypeWrapper):
    """Label detection mode."""
    pass

LABEL_DETECTION_MODE_UNSPECIFIED: LabelDetectionMode.ValueType = ...  # 0
"""Unspecified."""

SHOT_MODE: LabelDetectionMode.ValueType = ...  # 1
"""Detect shot-level labels."""

FRAME_MODE: LabelDetectionMode.ValueType = ...  # 2
"""Detect frame-level labels."""

SHOT_AND_FRAME_MODE: LabelDetectionMode.ValueType = ...  # 3
"""Detect both shot-level and frame-level labels."""

global___LabelDetectionMode = LabelDetectionMode


class _Likelihood:
    ValueType = typing.NewType('ValueType', builtins.int)
    V: typing_extensions.TypeAlias = ValueType
class _LikelihoodEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_Likelihood.ValueType], builtins.type):
    DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor = ...
    LIKELIHOOD_UNSPECIFIED: Likelihood.ValueType = ...  # 0
    """Unspecified likelihood."""

    VERY_UNLIKELY: Likelihood.ValueType = ...  # 1
    """Very unlikely."""

    UNLIKELY: Likelihood.ValueType = ...  # 2
    """Unlikely."""

    POSSIBLE: Likelihood.ValueType = ...  # 3
    """Possible."""

    LIKELY: Likelihood.ValueType = ...  # 4
    """Likely."""

    VERY_LIKELY: Likelihood.ValueType = ...  # 5
    """Very likely."""

class Likelihood(_Likelihood, metaclass=_LikelihoodEnumTypeWrapper):
    """Bucketized representation of likelihood."""
    pass

LIKELIHOOD_UNSPECIFIED: Likelihood.ValueType = ...  # 0
"""Unspecified likelihood."""

VERY_UNLIKELY: Likelihood.ValueType = ...  # 1
"""Very unlikely."""

UNLIKELY: Likelihood.ValueType = ...  # 2
"""Unlikely."""

POSSIBLE: Likelihood.ValueType = ...  # 3
"""Possible."""

LIKELY: Likelihood.ValueType = ...  # 4
"""Likely."""

VERY_LIKELY: Likelihood.ValueType = ...  # 5
"""Very likely."""

global___Likelihood = Likelihood


class _StreamingFeature:
    ValueType = typing.NewType('ValueType', builtins.int)
    V: typing_extensions.TypeAlias = ValueType
class _StreamingFeatureEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_StreamingFeature.ValueType], builtins.type):
    DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor = ...
    STREAMING_FEATURE_UNSPECIFIED: StreamingFeature.ValueType = ...  # 0
    """Unspecified."""

    STREAMING_LABEL_DETECTION: StreamingFeature.ValueType = ...  # 1
    """Label detection. Detect objects, such as dog or flower."""

    STREAMING_SHOT_CHANGE_DETECTION: StreamingFeature.ValueType = ...  # 2
    """Shot change detection."""

    STREAMING_EXPLICIT_CONTENT_DETECTION: StreamingFeature.ValueType = ...  # 3
    """Explicit content detection."""

    STREAMING_OBJECT_TRACKING: StreamingFeature.ValueType = ...  # 4
    """Object detection and tracking."""

    STREAMING_AUTOML_ACTION_RECOGNITION: StreamingFeature.ValueType = ...  # 23
    """Action recognition based on AutoML model."""

    STREAMING_AUTOML_CLASSIFICATION: StreamingFeature.ValueType = ...  # 21
    """Video classification based on AutoML model."""

    STREAMING_AUTOML_OBJECT_TRACKING: StreamingFeature.ValueType = ...  # 22
    """Object detection and tracking based on AutoML model."""

class StreamingFeature(_StreamingFeature, metaclass=_StreamingFeatureEnumTypeWrapper):
    """Streaming video annotation feature."""
    pass

STREAMING_FEATURE_UNSPECIFIED: StreamingFeature.ValueType = ...  # 0
"""Unspecified."""

STREAMING_LABEL_DETECTION: StreamingFeature.ValueType = ...  # 1
"""Label detection. Detect objects, such as dog or flower."""

STREAMING_SHOT_CHANGE_DETECTION: StreamingFeature.ValueType = ...  # 2
"""Shot change detection."""

STREAMING_EXPLICIT_CONTENT_DETECTION: StreamingFeature.ValueType = ...  # 3
"""Explicit content detection."""

STREAMING_OBJECT_TRACKING: StreamingFeature.ValueType = ...  # 4
"""Object detection and tracking."""

STREAMING_AUTOML_ACTION_RECOGNITION: StreamingFeature.ValueType = ...  # 23
"""Action recognition based on AutoML model."""

STREAMING_AUTOML_CLASSIFICATION: StreamingFeature.ValueType = ...  # 21
"""Video classification based on AutoML model."""

STREAMING_AUTOML_OBJECT_TRACKING: StreamingFeature.ValueType = ...  # 22
"""Object detection and tracking based on AutoML model."""

global___StreamingFeature = StreamingFeature


class _Feature:
    ValueType = typing.NewType('ValueType', builtins.int)
    V: typing_extensions.TypeAlias = ValueType
class _FeatureEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_Feature.ValueType], builtins.type):
    DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor = ...
    FEATURE_UNSPECIFIED: Feature.ValueType = ...  # 0
    """Unspecified."""

    LABEL_DETECTION: Feature.ValueType = ...  # 1
    """Label detection. Detect objects, such as dog or flower."""

    SHOT_CHANGE_DETECTION: Feature.ValueType = ...  # 2
    """Shot change detection."""

    EXPLICIT_CONTENT_DETECTION: Feature.ValueType = ...  # 3
    """Explicit content detection."""

    FACE_DETECTION: Feature.ValueType = ...  # 4
    """Human face detection."""

    SPEECH_TRANSCRIPTION: Feature.ValueType = ...  # 6
    """Speech transcription."""

    TEXT_DETECTION: Feature.ValueType = ...  # 7
    """OCR text detection and tracking."""

    OBJECT_TRACKING: Feature.ValueType = ...  # 9
    """Object detection and tracking."""

    LOGO_RECOGNITION: Feature.ValueType = ...  # 12
    """Logo detection, tracking, and recognition."""

    CELEBRITY_RECOGNITION: Feature.ValueType = ...  # 13
    """Celebrity recognition."""

    PERSON_DETECTION: Feature.ValueType = ...  # 14
    """Person detection."""

class Feature(_Feature, metaclass=_FeatureEnumTypeWrapper):
    """Video annotation feature."""
    pass

FEATURE_UNSPECIFIED: Feature.ValueType = ...  # 0
"""Unspecified."""

LABEL_DETECTION: Feature.ValueType = ...  # 1
"""Label detection. Detect objects, such as dog or flower."""

SHOT_CHANGE_DETECTION: Feature.ValueType = ...  # 2
"""Shot change detection."""

EXPLICIT_CONTENT_DETECTION: Feature.ValueType = ...  # 3
"""Explicit content detection."""

FACE_DETECTION: Feature.ValueType = ...  # 4
"""Human face detection."""

SPEECH_TRANSCRIPTION: Feature.ValueType = ...  # 6
"""Speech transcription."""

TEXT_DETECTION: Feature.ValueType = ...  # 7
"""OCR text detection and tracking."""

OBJECT_TRACKING: Feature.ValueType = ...  # 9
"""Object detection and tracking."""

LOGO_RECOGNITION: Feature.ValueType = ...  # 12
"""Logo detection, tracking, and recognition."""

CELEBRITY_RECOGNITION: Feature.ValueType = ...  # 13
"""Celebrity recognition."""

PERSON_DETECTION: Feature.ValueType = ...  # 14
"""Person detection."""

global___Feature = Feature


class AnnotateVideoRequest(google.protobuf.message.Message):
    """Video annotation request."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    INPUT_URI_FIELD_NUMBER: builtins.int
    INPUT_CONTENT_FIELD_NUMBER: builtins.int
    FEATURES_FIELD_NUMBER: builtins.int
    VIDEO_CONTEXT_FIELD_NUMBER: builtins.int
    OUTPUT_URI_FIELD_NUMBER: builtins.int
    LOCATION_ID_FIELD_NUMBER: builtins.int
    input_uri: typing.Text = ...
    """Input video location. Currently, only
    [Cloud Storage](https://cloud.google.com/storage/) URIs are
    supported. URIs must be specified in the following format:
    `gs://bucket-id/object-id` (other URI formats return
    [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT]). For
    more information, see [Request
    URIs](https://cloud.google.com/storage/docs/request-endpoints). To identify
    multiple videos, a video URI may include wildcards in the `object-id`.
    Supported wildcards: '*' to match 0 or more characters;
    '?' to match 1 character. If unset, the input video should be embedded
    in the request as `input_content`. If set, `input_content` must be unset.
    """

    input_content: builtins.bytes = ...
    """The video data bytes.
    If unset, the input video(s) should be specified via the `input_uri`.
    If set, `input_uri` must be unset.
    """

    @property
    def features(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[global___Feature.ValueType]:
        """Required. Requested video annotation features."""
        pass
    @property
    def video_context(self) -> global___VideoContext:
        """Additional video context and/or feature-specific parameters."""
        pass
    output_uri: typing.Text = ...
    """Optional. Location where the output (in JSON format) should be stored.
    Currently, only [Cloud Storage](https://cloud.google.com/storage/)
    URIs are supported. These must be specified in the following format:
    `gs://bucket-id/object-id` (other URI formats return
    [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT]). For
    more information, see [Request
    URIs](https://cloud.google.com/storage/docs/request-endpoints).
    """

    location_id: typing.Text = ...
    """Optional. Cloud region where annotation should take place. Supported cloud
    regions are: `us-east1`, `us-west1`, `europe-west1`, `asia-east1`. If no
    region is specified, the region will be determined based on video file
    location.
    """

    def __init__(self,
        *,
        input_uri : typing.Text = ...,
        input_content : builtins.bytes = ...,
        features : typing.Optional[typing.Iterable[global___Feature.ValueType]] = ...,
        video_context : typing.Optional[global___VideoContext] = ...,
        output_uri : typing.Text = ...,
        location_id : typing.Text = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["video_context",b"video_context"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["features",b"features","input_content",b"input_content","input_uri",b"input_uri","location_id",b"location_id","output_uri",b"output_uri","video_context",b"video_context"]) -> None: ...
global___AnnotateVideoRequest = AnnotateVideoRequest

class VideoContext(google.protobuf.message.Message):
    """Video context and/or feature-specific parameters."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    SEGMENTS_FIELD_NUMBER: builtins.int
    LABEL_DETECTION_CONFIG_FIELD_NUMBER: builtins.int
    SHOT_CHANGE_DETECTION_CONFIG_FIELD_NUMBER: builtins.int
    EXPLICIT_CONTENT_DETECTION_CONFIG_FIELD_NUMBER: builtins.int
    FACE_DETECTION_CONFIG_FIELD_NUMBER: builtins.int
    SPEECH_TRANSCRIPTION_CONFIG_FIELD_NUMBER: builtins.int
    TEXT_DETECTION_CONFIG_FIELD_NUMBER: builtins.int
    PERSON_DETECTION_CONFIG_FIELD_NUMBER: builtins.int
    OBJECT_TRACKING_CONFIG_FIELD_NUMBER: builtins.int
    @property
    def segments(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___VideoSegment]:
        """Video segments to annotate. The segments may overlap and are not required
        to be contiguous or span the whole video. If unspecified, each video is
        treated as a single segment.
        """
        pass
    @property
    def label_detection_config(self) -> global___LabelDetectionConfig:
        """Config for LABEL_DETECTION."""
        pass
    @property
    def shot_change_detection_config(self) -> global___ShotChangeDetectionConfig:
        """Config for SHOT_CHANGE_DETECTION."""
        pass
    @property
    def explicit_content_detection_config(self) -> global___ExplicitContentDetectionConfig:
        """Config for EXPLICIT_CONTENT_DETECTION."""
        pass
    @property
    def face_detection_config(self) -> global___FaceDetectionConfig:
        """Config for FACE_DETECTION."""
        pass
    @property
    def speech_transcription_config(self) -> global___SpeechTranscriptionConfig:
        """Config for SPEECH_TRANSCRIPTION."""
        pass
    @property
    def text_detection_config(self) -> global___TextDetectionConfig:
        """Config for TEXT_DETECTION."""
        pass
    @property
    def person_detection_config(self) -> global___PersonDetectionConfig:
        """Config for PERSON_DETECTION."""
        pass
    @property
    def object_tracking_config(self) -> global___ObjectTrackingConfig:
        """Config for OBJECT_TRACKING."""
        pass
    def __init__(self,
        *,
        segments : typing.Optional[typing.Iterable[global___VideoSegment]] = ...,
        label_detection_config : typing.Optional[global___LabelDetectionConfig] = ...,
        shot_change_detection_config : typing.Optional[global___ShotChangeDetectionConfig] = ...,
        explicit_content_detection_config : typing.Optional[global___ExplicitContentDetectionConfig] = ...,
        face_detection_config : typing.Optional[global___FaceDetectionConfig] = ...,
        speech_transcription_config : typing.Optional[global___SpeechTranscriptionConfig] = ...,
        text_detection_config : typing.Optional[global___TextDetectionConfig] = ...,
        person_detection_config : typing.Optional[global___PersonDetectionConfig] = ...,
        object_tracking_config : typing.Optional[global___ObjectTrackingConfig] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["explicit_content_detection_config",b"explicit_content_detection_config","face_detection_config",b"face_detection_config","label_detection_config",b"label_detection_config","object_tracking_config",b"object_tracking_config","person_detection_config",b"person_detection_config","shot_change_detection_config",b"shot_change_detection_config","speech_transcription_config",b"speech_transcription_config","text_detection_config",b"text_detection_config"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["explicit_content_detection_config",b"explicit_content_detection_config","face_detection_config",b"face_detection_config","label_detection_config",b"label_detection_config","object_tracking_config",b"object_tracking_config","person_detection_config",b"person_detection_config","segments",b"segments","shot_change_detection_config",b"shot_change_detection_config","speech_transcription_config",b"speech_transcription_config","text_detection_config",b"text_detection_config"]) -> None: ...
global___VideoContext = VideoContext

class LabelDetectionConfig(google.protobuf.message.Message):
    """Config for LABEL_DETECTION."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    LABEL_DETECTION_MODE_FIELD_NUMBER: builtins.int
    STATIONARY_CAMERA_FIELD_NUMBER: builtins.int
    MODEL_FIELD_NUMBER: builtins.int
    FRAME_CONFIDENCE_THRESHOLD_FIELD_NUMBER: builtins.int
    VIDEO_CONFIDENCE_THRESHOLD_FIELD_NUMBER: builtins.int
    label_detection_mode: global___LabelDetectionMode.ValueType = ...
    """What labels should be detected with LABEL_DETECTION, in addition to
    video-level labels or segment-level labels.
    If unspecified, defaults to `SHOT_MODE`.
    """

    stationary_camera: builtins.bool = ...
    """Whether the video has been shot from a stationary (i.e., non-moving)
    camera. When set to true, might improve detection accuracy for moving
    objects. Should be used with `SHOT_AND_FRAME_MODE` enabled.
    """

    model: typing.Text = ...
    """Model to use for label detection.
    Supported values: "builtin/stable" (the default if unset) and
    "builtin/latest".
    """

    frame_confidence_threshold: builtins.float = ...
    """The confidence threshold we perform filtering on the labels from
    frame-level detection. If not set, it is set to 0.4 by default. The valid
    range for this threshold is [0.1, 0.9]. Any value set outside of this
    range will be clipped.
    Note: For best results, follow the default threshold. We will update
    the default threshold everytime when we release a new model.
    """

    video_confidence_threshold: builtins.float = ...
    """The confidence threshold we perform filtering on the labels from
    video-level and shot-level detections. If not set, it's set to 0.3 by
    default. The valid range for this threshold is [0.1, 0.9]. Any value set
    outside of this range will be clipped.
    Note: For best results, follow the default threshold. We will update
    the default threshold everytime when we release a new model.
    """

    def __init__(self,
        *,
        label_detection_mode : global___LabelDetectionMode.ValueType = ...,
        stationary_camera : builtins.bool = ...,
        model : typing.Text = ...,
        frame_confidence_threshold : builtins.float = ...,
        video_confidence_threshold : builtins.float = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["frame_confidence_threshold",b"frame_confidence_threshold","label_detection_mode",b"label_detection_mode","model",b"model","stationary_camera",b"stationary_camera","video_confidence_threshold",b"video_confidence_threshold"]) -> None: ...
global___LabelDetectionConfig = LabelDetectionConfig

class ShotChangeDetectionConfig(google.protobuf.message.Message):
    """Config for SHOT_CHANGE_DETECTION."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    MODEL_FIELD_NUMBER: builtins.int
    model: typing.Text = ...
    """Model to use for shot change detection.
    Supported values: "builtin/stable" (the default if unset) and
    "builtin/latest".
    """

    def __init__(self,
        *,
        model : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["model",b"model"]) -> None: ...
global___ShotChangeDetectionConfig = ShotChangeDetectionConfig

class ObjectTrackingConfig(google.protobuf.message.Message):
    """Config for OBJECT_TRACKING."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    MODEL_FIELD_NUMBER: builtins.int
    model: typing.Text = ...
    """Model to use for object tracking.
    Supported values: "builtin/stable" (the default if unset) and
    "builtin/latest".
    """

    def __init__(self,
        *,
        model : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["model",b"model"]) -> None: ...
global___ObjectTrackingConfig = ObjectTrackingConfig

class ExplicitContentDetectionConfig(google.protobuf.message.Message):
    """Config for EXPLICIT_CONTENT_DETECTION."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    MODEL_FIELD_NUMBER: builtins.int
    model: typing.Text = ...
    """Model to use for explicit content detection.
    Supported values: "builtin/stable" (the default if unset) and
    "builtin/latest".
    """

    def __init__(self,
        *,
        model : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["model",b"model"]) -> None: ...
global___ExplicitContentDetectionConfig = ExplicitContentDetectionConfig

class FaceDetectionConfig(google.protobuf.message.Message):
    """Config for FACE_DETECTION."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    MODEL_FIELD_NUMBER: builtins.int
    INCLUDE_BOUNDING_BOXES_FIELD_NUMBER: builtins.int
    INCLUDE_ATTRIBUTES_FIELD_NUMBER: builtins.int
    model: typing.Text = ...
    """Model to use for face detection.
    Supported values: "builtin/stable" (the default if unset) and
    "builtin/latest".
    """

    include_bounding_boxes: builtins.bool = ...
    """Whether bounding boxes are included in the face annotation output."""

    include_attributes: builtins.bool = ...
    """Whether to enable face attributes detection, such as glasses, dark_glasses,
    mouth_open etc. Ignored if 'include_bounding_boxes' is set to false.
    """

    def __init__(self,
        *,
        model : typing.Text = ...,
        include_bounding_boxes : builtins.bool = ...,
        include_attributes : builtins.bool = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["include_attributes",b"include_attributes","include_bounding_boxes",b"include_bounding_boxes","model",b"model"]) -> None: ...
global___FaceDetectionConfig = FaceDetectionConfig

class PersonDetectionConfig(google.protobuf.message.Message):
    """Config for PERSON_DETECTION."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    INCLUDE_BOUNDING_BOXES_FIELD_NUMBER: builtins.int
    INCLUDE_POSE_LANDMARKS_FIELD_NUMBER: builtins.int
    INCLUDE_ATTRIBUTES_FIELD_NUMBER: builtins.int
    include_bounding_boxes: builtins.bool = ...
    """Whether bounding boxes are included in the person detection annotation
    output.
    """

    include_pose_landmarks: builtins.bool = ...
    """Whether to enable pose landmarks detection. Ignored if
    'include_bounding_boxes' is set to false.
    """

    include_attributes: builtins.bool = ...
    """Whether to enable person attributes detection, such as cloth color (black,
    blue, etc), type (coat, dress, etc), pattern (plain, floral, etc), hair,
    etc.
    Ignored if 'include_bounding_boxes' is set to false.
    """

    def __init__(self,
        *,
        include_bounding_boxes : builtins.bool = ...,
        include_pose_landmarks : builtins.bool = ...,
        include_attributes : builtins.bool = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["include_attributes",b"include_attributes","include_bounding_boxes",b"include_bounding_boxes","include_pose_landmarks",b"include_pose_landmarks"]) -> None: ...
global___PersonDetectionConfig = PersonDetectionConfig

class TextDetectionConfig(google.protobuf.message.Message):
    """Config for TEXT_DETECTION."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    LANGUAGE_HINTS_FIELD_NUMBER: builtins.int
    MODEL_FIELD_NUMBER: builtins.int
    @property
    def language_hints(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """Language hint can be specified if the language to be detected is known a
        priori. It can increase the accuracy of the detection. Language hint must
        be language code in BCP-47 format.

        Automatic language detection is performed if no hint is provided.
        """
        pass
    model: typing.Text = ...
    """Model to use for text detection.
    Supported values: "builtin/stable" (the default if unset) and
    "builtin/latest".
    """

    def __init__(self,
        *,
        language_hints : typing.Optional[typing.Iterable[typing.Text]] = ...,
        model : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["language_hints",b"language_hints","model",b"model"]) -> None: ...
global___TextDetectionConfig = TextDetectionConfig

class VideoSegment(google.protobuf.message.Message):
    """Video segment."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    START_TIME_OFFSET_FIELD_NUMBER: builtins.int
    END_TIME_OFFSET_FIELD_NUMBER: builtins.int
    @property
    def start_time_offset(self) -> google.protobuf.duration_pb2.Duration:
        """Time-offset, relative to the beginning of the video,
        corresponding to the start of the segment (inclusive).
        """
        pass
    @property
    def end_time_offset(self) -> google.protobuf.duration_pb2.Duration:
        """Time-offset, relative to the beginning of the video,
        corresponding to the end of the segment (inclusive).
        """
        pass
    def __init__(self,
        *,
        start_time_offset : typing.Optional[google.protobuf.duration_pb2.Duration] = ...,
        end_time_offset : typing.Optional[google.protobuf.duration_pb2.Duration] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["end_time_offset",b"end_time_offset","start_time_offset",b"start_time_offset"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["end_time_offset",b"end_time_offset","start_time_offset",b"start_time_offset"]) -> None: ...
global___VideoSegment = VideoSegment

class LabelSegment(google.protobuf.message.Message):
    """Video segment level annotation results for label detection."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    SEGMENT_FIELD_NUMBER: builtins.int
    CONFIDENCE_FIELD_NUMBER: builtins.int
    @property
    def segment(self) -> global___VideoSegment:
        """Video segment where a label was detected."""
        pass
    confidence: builtins.float = ...
    """Confidence that the label is accurate. Range: [0, 1]."""

    def __init__(self,
        *,
        segment : typing.Optional[global___VideoSegment] = ...,
        confidence : builtins.float = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["segment",b"segment"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["confidence",b"confidence","segment",b"segment"]) -> None: ...
global___LabelSegment = LabelSegment

class LabelFrame(google.protobuf.message.Message):
    """Video frame level annotation results for label detection."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    TIME_OFFSET_FIELD_NUMBER: builtins.int
    CONFIDENCE_FIELD_NUMBER: builtins.int
    @property
    def time_offset(self) -> google.protobuf.duration_pb2.Duration:
        """Time-offset, relative to the beginning of the video, corresponding to the
        video frame for this location.
        """
        pass
    confidence: builtins.float = ...
    """Confidence that the label is accurate. Range: [0, 1]."""

    def __init__(self,
        *,
        time_offset : typing.Optional[google.protobuf.duration_pb2.Duration] = ...,
        confidence : builtins.float = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["time_offset",b"time_offset"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["confidence",b"confidence","time_offset",b"time_offset"]) -> None: ...
global___LabelFrame = LabelFrame

class Entity(google.protobuf.message.Message):
    """Detected entity from video analysis."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    ENTITY_ID_FIELD_NUMBER: builtins.int
    DESCRIPTION_FIELD_NUMBER: builtins.int
    LANGUAGE_CODE_FIELD_NUMBER: builtins.int
    entity_id: typing.Text = ...
    """Opaque entity ID. Some IDs may be available in
    [Google Knowledge Graph Search
    API](https://developers.google.com/knowledge-graph/).
    """

    description: typing.Text = ...
    """Textual description, e.g., `Fixed-gear bicycle`."""

    language_code: typing.Text = ...
    """Language code for `description` in BCP-47 format."""

    def __init__(self,
        *,
        entity_id : typing.Text = ...,
        description : typing.Text = ...,
        language_code : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["description",b"description","entity_id",b"entity_id","language_code",b"language_code"]) -> None: ...
global___Entity = Entity

class LabelAnnotation(google.protobuf.message.Message):
    """Label annotation."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    ENTITY_FIELD_NUMBER: builtins.int
    CATEGORY_ENTITIES_FIELD_NUMBER: builtins.int
    SEGMENTS_FIELD_NUMBER: builtins.int
    FRAMES_FIELD_NUMBER: builtins.int
    @property
    def entity(self) -> global___Entity:
        """Detected entity."""
        pass
    @property
    def category_entities(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___Entity]:
        """Common categories for the detected entity.
        For example, when the label is `Terrier`, the category is likely `dog`. And
        in some cases there might be more than one categories e.g., `Terrier` could
        also be a `pet`.
        """
        pass
    @property
    def segments(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___LabelSegment]:
        """All video segments where a label was detected."""
        pass
    @property
    def frames(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___LabelFrame]:
        """All video frames where a label was detected."""
        pass
    def __init__(self,
        *,
        entity : typing.Optional[global___Entity] = ...,
        category_entities : typing.Optional[typing.Iterable[global___Entity]] = ...,
        segments : typing.Optional[typing.Iterable[global___LabelSegment]] = ...,
        frames : typing.Optional[typing.Iterable[global___LabelFrame]] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["entity",b"entity"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["category_entities",b"category_entities","entity",b"entity","frames",b"frames","segments",b"segments"]) -> None: ...
global___LabelAnnotation = LabelAnnotation

class ExplicitContentFrame(google.protobuf.message.Message):
    """Video frame level annotation results for explicit content."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    TIME_OFFSET_FIELD_NUMBER: builtins.int
    PORNOGRAPHY_LIKELIHOOD_FIELD_NUMBER: builtins.int
    @property
    def time_offset(self) -> google.protobuf.duration_pb2.Duration:
        """Time-offset, relative to the beginning of the video, corresponding to the
        video frame for this location.
        """
        pass
    pornography_likelihood: global___Likelihood.ValueType = ...
    """Likelihood of the pornography content.."""

    def __init__(self,
        *,
        time_offset : typing.Optional[google.protobuf.duration_pb2.Duration] = ...,
        pornography_likelihood : global___Likelihood.ValueType = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["time_offset",b"time_offset"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["pornography_likelihood",b"pornography_likelihood","time_offset",b"time_offset"]) -> None: ...
global___ExplicitContentFrame = ExplicitContentFrame

class ExplicitContentAnnotation(google.protobuf.message.Message):
    """Explicit content annotation (based on per-frame visual signals only).
    If no explicit content has been detected in a frame, no annotations are
    present for that frame.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    FRAMES_FIELD_NUMBER: builtins.int
    @property
    def frames(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___ExplicitContentFrame]:
        """All video frames where explicit content was detected."""
        pass
    def __init__(self,
        *,
        frames : typing.Optional[typing.Iterable[global___ExplicitContentFrame]] = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["frames",b"frames"]) -> None: ...
global___ExplicitContentAnnotation = ExplicitContentAnnotation

class NormalizedBoundingBox(google.protobuf.message.Message):
    """Normalized bounding box.
    The normalized vertex coordinates are relative to the original image.
    Range: [0, 1].
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    LEFT_FIELD_NUMBER: builtins.int
    TOP_FIELD_NUMBER: builtins.int
    RIGHT_FIELD_NUMBER: builtins.int
    BOTTOM_FIELD_NUMBER: builtins.int
    left: builtins.float = ...
    """Left X coordinate."""

    top: builtins.float = ...
    """Top Y coordinate."""

    right: builtins.float = ...
    """Right X coordinate."""

    bottom: builtins.float = ...
    """Bottom Y coordinate."""

    def __init__(self,
        *,
        left : builtins.float = ...,
        top : builtins.float = ...,
        right : builtins.float = ...,
        bottom : builtins.float = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["bottom",b"bottom","left",b"left","right",b"right","top",b"top"]) -> None: ...
global___NormalizedBoundingBox = NormalizedBoundingBox

class TimestampedObject(google.protobuf.message.Message):
    """For tracking related features.
    An object at time_offset with attributes, and located with
    normalized_bounding_box.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    NORMALIZED_BOUNDING_BOX_FIELD_NUMBER: builtins.int
    TIME_OFFSET_FIELD_NUMBER: builtins.int
    ATTRIBUTES_FIELD_NUMBER: builtins.int
    LANDMARKS_FIELD_NUMBER: builtins.int
    @property
    def normalized_bounding_box(self) -> global___NormalizedBoundingBox:
        """Normalized Bounding box in a frame, where the object is located."""
        pass
    @property
    def time_offset(self) -> google.protobuf.duration_pb2.Duration:
        """Time-offset, relative to the beginning of the video,
        corresponding to the video frame for this object.
        """
        pass
    @property
    def attributes(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___DetectedAttribute]:
        """Optional. The attributes of the object in the bounding box."""
        pass
    @property
    def landmarks(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___DetectedLandmark]:
        """Optional. The detected landmarks."""
        pass
    def __init__(self,
        *,
        normalized_bounding_box : typing.Optional[global___NormalizedBoundingBox] = ...,
        time_offset : typing.Optional[google.protobuf.duration_pb2.Duration] = ...,
        attributes : typing.Optional[typing.Iterable[global___DetectedAttribute]] = ...,
        landmarks : typing.Optional[typing.Iterable[global___DetectedLandmark]] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["normalized_bounding_box",b"normalized_bounding_box","time_offset",b"time_offset"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["attributes",b"attributes","landmarks",b"landmarks","normalized_bounding_box",b"normalized_bounding_box","time_offset",b"time_offset"]) -> None: ...
global___TimestampedObject = TimestampedObject

class Track(google.protobuf.message.Message):
    """A track of an object instance."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    SEGMENT_FIELD_NUMBER: builtins.int
    TIMESTAMPED_OBJECTS_FIELD_NUMBER: builtins.int
    ATTRIBUTES_FIELD_NUMBER: builtins.int
    CONFIDENCE_FIELD_NUMBER: builtins.int
    @property
    def segment(self) -> global___VideoSegment:
        """Video segment of a track."""
        pass
    @property
    def timestamped_objects(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___TimestampedObject]:
        """The object with timestamp and attributes per frame in the track."""
        pass
    @property
    def attributes(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___DetectedAttribute]:
        """Optional. Attributes in the track level."""
        pass
    confidence: builtins.float = ...
    """Optional. The confidence score of the tracked object."""

    def __init__(self,
        *,
        segment : typing.Optional[global___VideoSegment] = ...,
        timestamped_objects : typing.Optional[typing.Iterable[global___TimestampedObject]] = ...,
        attributes : typing.Optional[typing.Iterable[global___DetectedAttribute]] = ...,
        confidence : builtins.float = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["segment",b"segment"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["attributes",b"attributes","confidence",b"confidence","segment",b"segment","timestamped_objects",b"timestamped_objects"]) -> None: ...
global___Track = Track

class DetectedAttribute(google.protobuf.message.Message):
    """A generic detected attribute represented by name in string format."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    NAME_FIELD_NUMBER: builtins.int
    CONFIDENCE_FIELD_NUMBER: builtins.int
    VALUE_FIELD_NUMBER: builtins.int
    name: typing.Text = ...
    """The name of the attribute, for example, glasses, dark_glasses, mouth_open.
    A full list of supported type names will be provided in the document.
    """

    confidence: builtins.float = ...
    """Detected attribute confidence. Range [0, 1]."""

    value: typing.Text = ...
    """Text value of the detection result. For example, the value for "HairColor"
    can be "black", "blonde", etc.
    """

    def __init__(self,
        *,
        name : typing.Text = ...,
        confidence : builtins.float = ...,
        value : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["confidence",b"confidence","name",b"name","value",b"value"]) -> None: ...
global___DetectedAttribute = DetectedAttribute

class Celebrity(google.protobuf.message.Message):
    """Celebrity definition."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    NAME_FIELD_NUMBER: builtins.int
    DISPLAY_NAME_FIELD_NUMBER: builtins.int
    DESCRIPTION_FIELD_NUMBER: builtins.int
    name: typing.Text = ...
    """The resource name of the celebrity. Have the format
    `video-intelligence/kg-mid` indicates a celebrity from preloaded gallery.
    kg-mid is the id in Google knowledge graph, which is unique for the
    celebrity.
    """

    display_name: typing.Text = ...
    """The celebrity name."""

    description: typing.Text = ...
    """Textual description of additional information about the celebrity, if
    applicable.
    """

    def __init__(self,
        *,
        name : typing.Text = ...,
        display_name : typing.Text = ...,
        description : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["description",b"description","display_name",b"display_name","name",b"name"]) -> None: ...
global___Celebrity = Celebrity

class CelebrityTrack(google.protobuf.message.Message):
    """The annotation result of a celebrity face track. RecognizedCelebrity field
    could be empty if the face track does not have any matched celebrities.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class RecognizedCelebrity(google.protobuf.message.Message):
        """The recognized celebrity with confidence score."""
        DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
        CELEBRITY_FIELD_NUMBER: builtins.int
        CONFIDENCE_FIELD_NUMBER: builtins.int
        @property
        def celebrity(self) -> global___Celebrity:
            """The recognized celebrity."""
            pass
        confidence: builtins.float = ...
        """Recognition confidence. Range [0, 1]."""

        def __init__(self,
            *,
            celebrity : typing.Optional[global___Celebrity] = ...,
            confidence : builtins.float = ...,
            ) -> None: ...
        def HasField(self, field_name: typing_extensions.Literal["celebrity",b"celebrity"]) -> builtins.bool: ...
        def ClearField(self, field_name: typing_extensions.Literal["celebrity",b"celebrity","confidence",b"confidence"]) -> None: ...

    CELEBRITIES_FIELD_NUMBER: builtins.int
    FACE_TRACK_FIELD_NUMBER: builtins.int
    @property
    def celebrities(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___CelebrityTrack.RecognizedCelebrity]:
        """Top N match of the celebrities for the face in this track."""
        pass
    @property
    def face_track(self) -> global___Track:
        """A track of a person's face."""
        pass
    def __init__(self,
        *,
        celebrities : typing.Optional[typing.Iterable[global___CelebrityTrack.RecognizedCelebrity]] = ...,
        face_track : typing.Optional[global___Track] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["face_track",b"face_track"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["celebrities",b"celebrities","face_track",b"face_track"]) -> None: ...
global___CelebrityTrack = CelebrityTrack

class CelebrityRecognitionAnnotation(google.protobuf.message.Message):
    """Celebrity recognition annotation per video."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    CELEBRITY_TRACKS_FIELD_NUMBER: builtins.int
    @property
    def celebrity_tracks(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___CelebrityTrack]:
        """The tracks detected from the input video, including recognized celebrities
        and other detected faces in the video.
        """
        pass
    def __init__(self,
        *,
        celebrity_tracks : typing.Optional[typing.Iterable[global___CelebrityTrack]] = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["celebrity_tracks",b"celebrity_tracks"]) -> None: ...
global___CelebrityRecognitionAnnotation = CelebrityRecognitionAnnotation

class DetectedLandmark(google.protobuf.message.Message):
    """A generic detected landmark represented by name in string format and a 2D
    location.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    NAME_FIELD_NUMBER: builtins.int
    POINT_FIELD_NUMBER: builtins.int
    CONFIDENCE_FIELD_NUMBER: builtins.int
    name: typing.Text = ...
    """The name of this landmark, for example, left_hand, right_shoulder."""

    @property
    def point(self) -> global___NormalizedVertex:
        """The 2D point of the detected landmark using the normalized image
        coordindate system. The normalized coordinates have the range from 0 to 1.
        """
        pass
    confidence: builtins.float = ...
    """The confidence score of the detected landmark. Range [0, 1]."""

    def __init__(self,
        *,
        name : typing.Text = ...,
        point : typing.Optional[global___NormalizedVertex] = ...,
        confidence : builtins.float = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["point",b"point"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["confidence",b"confidence","name",b"name","point",b"point"]) -> None: ...
global___DetectedLandmark = DetectedLandmark

class FaceDetectionAnnotation(google.protobuf.message.Message):
    """Face detection annotation."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    TRACKS_FIELD_NUMBER: builtins.int
    THUMBNAIL_FIELD_NUMBER: builtins.int
    @property
    def tracks(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___Track]:
        """The face tracks with attributes."""
        pass
    thumbnail: builtins.bytes = ...
    """The thumbnail of a person's face."""

    def __init__(self,
        *,
        tracks : typing.Optional[typing.Iterable[global___Track]] = ...,
        thumbnail : builtins.bytes = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["thumbnail",b"thumbnail","tracks",b"tracks"]) -> None: ...
global___FaceDetectionAnnotation = FaceDetectionAnnotation

class PersonDetectionAnnotation(google.protobuf.message.Message):
    """Person detection annotation per video."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    TRACKS_FIELD_NUMBER: builtins.int
    @property
    def tracks(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___Track]:
        """The detected tracks of a person."""
        pass
    def __init__(self,
        *,
        tracks : typing.Optional[typing.Iterable[global___Track]] = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["tracks",b"tracks"]) -> None: ...
global___PersonDetectionAnnotation = PersonDetectionAnnotation

class VideoAnnotationResults(google.protobuf.message.Message):
    """Annotation results for a single video."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    INPUT_URI_FIELD_NUMBER: builtins.int
    SEGMENT_FIELD_NUMBER: builtins.int
    SEGMENT_LABEL_ANNOTATIONS_FIELD_NUMBER: builtins.int
    SEGMENT_PRESENCE_LABEL_ANNOTATIONS_FIELD_NUMBER: builtins.int
    SHOT_LABEL_ANNOTATIONS_FIELD_NUMBER: builtins.int
    SHOT_PRESENCE_LABEL_ANNOTATIONS_FIELD_NUMBER: builtins.int
    FRAME_LABEL_ANNOTATIONS_FIELD_NUMBER: builtins.int
    FACE_DETECTION_ANNOTATIONS_FIELD_NUMBER: builtins.int
    SHOT_ANNOTATIONS_FIELD_NUMBER: builtins.int
    EXPLICIT_ANNOTATION_FIELD_NUMBER: builtins.int
    SPEECH_TRANSCRIPTIONS_FIELD_NUMBER: builtins.int
    TEXT_ANNOTATIONS_FIELD_NUMBER: builtins.int
    OBJECT_ANNOTATIONS_FIELD_NUMBER: builtins.int
    LOGO_RECOGNITION_ANNOTATIONS_FIELD_NUMBER: builtins.int
    PERSON_DETECTION_ANNOTATIONS_FIELD_NUMBER: builtins.int
    CELEBRITY_RECOGNITION_ANNOTATIONS_FIELD_NUMBER: builtins.int
    ERROR_FIELD_NUMBER: builtins.int
    input_uri: typing.Text = ...
    """Video file location in
    [Cloud Storage](https://cloud.google.com/storage/).
    """

    @property
    def segment(self) -> global___VideoSegment:
        """Video segment on which the annotation is run."""
        pass
    @property
    def segment_label_annotations(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___LabelAnnotation]:
        """Topical label annotations on video level or user-specified segment level.
        There is exactly one element for each unique label.
        """
        pass
    @property
    def segment_presence_label_annotations(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___LabelAnnotation]:
        """Presence label annotations on video level or user-specified segment level.
        There is exactly one element for each unique label. Compared to the
        existing topical `segment_label_annotations`, this field presents more
        fine-grained, segment-level labels detected in video content and is made
        available only when the client sets `LabelDetectionConfig.model` to
        "builtin/latest" in the request.
        """
        pass
    @property
    def shot_label_annotations(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___LabelAnnotation]:
        """Topical label annotations on shot level.
        There is exactly one element for each unique label.
        """
        pass
    @property
    def shot_presence_label_annotations(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___LabelAnnotation]:
        """Presence label annotations on shot level. There is exactly one element for
        each unique label. Compared to the existing topical
        `shot_label_annotations`, this field presents more fine-grained, shot-level
        labels detected in video content and is made available only when the client
        sets `LabelDetectionConfig.model` to "builtin/latest" in the request.
        """
        pass
    @property
    def frame_label_annotations(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___LabelAnnotation]:
        """Label annotations on frame level.
        There is exactly one element for each unique label.
        """
        pass
    @property
    def face_detection_annotations(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___FaceDetectionAnnotation]:
        """Face detection annotations."""
        pass
    @property
    def shot_annotations(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___VideoSegment]:
        """Shot annotations. Each shot is represented as a video segment."""
        pass
    @property
    def explicit_annotation(self) -> global___ExplicitContentAnnotation:
        """Explicit content annotation."""
        pass
    @property
    def speech_transcriptions(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___SpeechTranscription]:
        """Speech transcription."""
        pass
    @property
    def text_annotations(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___TextAnnotation]:
        """OCR text detection and tracking.
        Annotations for list of detected text snippets. Each will have list of
        frame information associated with it.
        """
        pass
    @property
    def object_annotations(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___ObjectTrackingAnnotation]:
        """Annotations for list of objects detected and tracked in video."""
        pass
    @property
    def logo_recognition_annotations(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___LogoRecognitionAnnotation]:
        """Annotations for list of logos detected, tracked and recognized in video."""
        pass
    @property
    def person_detection_annotations(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___PersonDetectionAnnotation]:
        """Person detection annotations."""
        pass
    @property
    def celebrity_recognition_annotations(self) -> global___CelebrityRecognitionAnnotation:
        """Celebrity recognition annotations."""
        pass
    @property
    def error(self) -> google.rpc.status_pb2.Status:
        """If set, indicates an error. Note that for a single `AnnotateVideoRequest`
        some videos may succeed and some may fail.
        """
        pass
    def __init__(self,
        *,
        input_uri : typing.Text = ...,
        segment : typing.Optional[global___VideoSegment] = ...,
        segment_label_annotations : typing.Optional[typing.Iterable[global___LabelAnnotation]] = ...,
        segment_presence_label_annotations : typing.Optional[typing.Iterable[global___LabelAnnotation]] = ...,
        shot_label_annotations : typing.Optional[typing.Iterable[global___LabelAnnotation]] = ...,
        shot_presence_label_annotations : typing.Optional[typing.Iterable[global___LabelAnnotation]] = ...,
        frame_label_annotations : typing.Optional[typing.Iterable[global___LabelAnnotation]] = ...,
        face_detection_annotations : typing.Optional[typing.Iterable[global___FaceDetectionAnnotation]] = ...,
        shot_annotations : typing.Optional[typing.Iterable[global___VideoSegment]] = ...,
        explicit_annotation : typing.Optional[global___ExplicitContentAnnotation] = ...,
        speech_transcriptions : typing.Optional[typing.Iterable[global___SpeechTranscription]] = ...,
        text_annotations : typing.Optional[typing.Iterable[global___TextAnnotation]] = ...,
        object_annotations : typing.Optional[typing.Iterable[global___ObjectTrackingAnnotation]] = ...,
        logo_recognition_annotations : typing.Optional[typing.Iterable[global___LogoRecognitionAnnotation]] = ...,
        person_detection_annotations : typing.Optional[typing.Iterable[global___PersonDetectionAnnotation]] = ...,
        celebrity_recognition_annotations : typing.Optional[global___CelebrityRecognitionAnnotation] = ...,
        error : typing.Optional[google.rpc.status_pb2.Status] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["celebrity_recognition_annotations",b"celebrity_recognition_annotations","error",b"error","explicit_annotation",b"explicit_annotation","segment",b"segment"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["celebrity_recognition_annotations",b"celebrity_recognition_annotations","error",b"error","explicit_annotation",b"explicit_annotation","face_detection_annotations",b"face_detection_annotations","frame_label_annotations",b"frame_label_annotations","input_uri",b"input_uri","logo_recognition_annotations",b"logo_recognition_annotations","object_annotations",b"object_annotations","person_detection_annotations",b"person_detection_annotations","segment",b"segment","segment_label_annotations",b"segment_label_annotations","segment_presence_label_annotations",b"segment_presence_label_annotations","shot_annotations",b"shot_annotations","shot_label_annotations",b"shot_label_annotations","shot_presence_label_annotations",b"shot_presence_label_annotations","speech_transcriptions",b"speech_transcriptions","text_annotations",b"text_annotations"]) -> None: ...
global___VideoAnnotationResults = VideoAnnotationResults

class AnnotateVideoResponse(google.protobuf.message.Message):
    """Video annotation response. Included in the `response`
    field of the `Operation` returned by the `GetOperation`
    call of the `google::longrunning::Operations` service.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    ANNOTATION_RESULTS_FIELD_NUMBER: builtins.int
    @property
    def annotation_results(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___VideoAnnotationResults]:
        """Annotation results for all videos specified in `AnnotateVideoRequest`."""
        pass
    def __init__(self,
        *,
        annotation_results : typing.Optional[typing.Iterable[global___VideoAnnotationResults]] = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["annotation_results",b"annotation_results"]) -> None: ...
global___AnnotateVideoResponse = AnnotateVideoResponse

class VideoAnnotationProgress(google.protobuf.message.Message):
    """Annotation progress for a single video."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    INPUT_URI_FIELD_NUMBER: builtins.int
    PROGRESS_PERCENT_FIELD_NUMBER: builtins.int
    START_TIME_FIELD_NUMBER: builtins.int
    UPDATE_TIME_FIELD_NUMBER: builtins.int
    FEATURE_FIELD_NUMBER: builtins.int
    SEGMENT_FIELD_NUMBER: builtins.int
    input_uri: typing.Text = ...
    """Video file location in
    [Cloud Storage](https://cloud.google.com/storage/).
    """

    progress_percent: builtins.int = ...
    """Approximate percentage processed thus far. Guaranteed to be
    100 when fully processed.
    """

    @property
    def start_time(self) -> google.protobuf.timestamp_pb2.Timestamp:
        """Time when the request was received."""
        pass
    @property
    def update_time(self) -> google.protobuf.timestamp_pb2.Timestamp:
        """Time of the most recent update."""
        pass
    feature: global___Feature.ValueType = ...
    """Specifies which feature is being tracked if the request contains more than
    one feature.
    """

    @property
    def segment(self) -> global___VideoSegment:
        """Specifies which segment is being tracked if the request contains more than
        one segment.
        """
        pass
    def __init__(self,
        *,
        input_uri : typing.Text = ...,
        progress_percent : builtins.int = ...,
        start_time : typing.Optional[google.protobuf.timestamp_pb2.Timestamp] = ...,
        update_time : typing.Optional[google.protobuf.timestamp_pb2.Timestamp] = ...,
        feature : global___Feature.ValueType = ...,
        segment : typing.Optional[global___VideoSegment] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["segment",b"segment","start_time",b"start_time","update_time",b"update_time"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["feature",b"feature","input_uri",b"input_uri","progress_percent",b"progress_percent","segment",b"segment","start_time",b"start_time","update_time",b"update_time"]) -> None: ...
global___VideoAnnotationProgress = VideoAnnotationProgress

class AnnotateVideoProgress(google.protobuf.message.Message):
    """Video annotation progress. Included in the `metadata`
    field of the `Operation` returned by the `GetOperation`
    call of the `google::longrunning::Operations` service.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    ANNOTATION_PROGRESS_FIELD_NUMBER: builtins.int
    @property
    def annotation_progress(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___VideoAnnotationProgress]:
        """Progress metadata for all videos specified in `AnnotateVideoRequest`."""
        pass
    def __init__(self,
        *,
        annotation_progress : typing.Optional[typing.Iterable[global___VideoAnnotationProgress]] = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["annotation_progress",b"annotation_progress"]) -> None: ...
global___AnnotateVideoProgress = AnnotateVideoProgress

class SpeechTranscriptionConfig(google.protobuf.message.Message):
    """Config for SPEECH_TRANSCRIPTION."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    LANGUAGE_CODE_FIELD_NUMBER: builtins.int
    MAX_ALTERNATIVES_FIELD_NUMBER: builtins.int
    FILTER_PROFANITY_FIELD_NUMBER: builtins.int
    SPEECH_CONTEXTS_FIELD_NUMBER: builtins.int
    ENABLE_AUTOMATIC_PUNCTUATION_FIELD_NUMBER: builtins.int
    AUDIO_TRACKS_FIELD_NUMBER: builtins.int
    ENABLE_SPEAKER_DIARIZATION_FIELD_NUMBER: builtins.int
    DIARIZATION_SPEAKER_COUNT_FIELD_NUMBER: builtins.int
    ENABLE_WORD_CONFIDENCE_FIELD_NUMBER: builtins.int
    language_code: typing.Text = ...
    """Required. *Required* The language of the supplied audio as a
    [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag.
    Example: "en-US".
    See [Language Support](https://cloud.google.com/speech/docs/languages)
    for a list of the currently supported language codes.
    """

    max_alternatives: builtins.int = ...
    """Optional. Maximum number of recognition hypotheses to be returned.
    Specifically, the maximum number of `SpeechRecognitionAlternative` messages
    within each `SpeechTranscription`. The server may return fewer than
    `max_alternatives`. Valid values are `0`-`30`. A value of `0` or `1` will
    return a maximum of one. If omitted, will return a maximum of one.
    """

    filter_profanity: builtins.bool = ...
    """Optional. If set to `true`, the server will attempt to filter out
    profanities, replacing all but the initial character in each filtered word
    with asterisks, e.g. "f***". If set to `false` or omitted, profanities
    won't be filtered out.
    """

    @property
    def speech_contexts(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___SpeechContext]:
        """Optional. A means to provide context to assist the speech recognition."""
        pass
    enable_automatic_punctuation: builtins.bool = ...
    """Optional. If 'true', adds punctuation to recognition result hypotheses.
    This feature is only available in select languages. Setting this for
    requests in other languages has no effect at all. The default 'false' value
    does not add punctuation to result hypotheses. NOTE: "This is currently
    offered as an experimental service, complimentary to all users. In the
    future this may be exclusively available as a premium feature."
    """

    @property
    def audio_tracks(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[builtins.int]:
        """Optional. For file formats, such as MXF or MKV, supporting multiple audio
        tracks, specify up to two tracks. Default: track 0.
        """
        pass
    enable_speaker_diarization: builtins.bool = ...
    """Optional. If 'true', enables speaker detection for each recognized word in
    the top alternative of the recognition result using a speaker_tag provided
    in the WordInfo.
    Note: When this is true, we send all the words from the beginning of the
    audio for the top alternative in every consecutive response.
    This is done in order to improve our speaker tags as our models learn to
    identify the speakers in the conversation over time.
    """

    diarization_speaker_count: builtins.int = ...
    """Optional. If set, specifies the estimated number of speakers in the
    conversation. If not set, defaults to '2'. Ignored unless
    enable_speaker_diarization is set to true.
    """

    enable_word_confidence: builtins.bool = ...
    """Optional. If `true`, the top result includes a list of words and the
    confidence for those words. If `false`, no word-level confidence
    information is returned. The default is `false`.
    """

    def __init__(self,
        *,
        language_code : typing.Text = ...,
        max_alternatives : builtins.int = ...,
        filter_profanity : builtins.bool = ...,
        speech_contexts : typing.Optional[typing.Iterable[global___SpeechContext]] = ...,
        enable_automatic_punctuation : builtins.bool = ...,
        audio_tracks : typing.Optional[typing.Iterable[builtins.int]] = ...,
        enable_speaker_diarization : builtins.bool = ...,
        diarization_speaker_count : builtins.int = ...,
        enable_word_confidence : builtins.bool = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["audio_tracks",b"audio_tracks","diarization_speaker_count",b"diarization_speaker_count","enable_automatic_punctuation",b"enable_automatic_punctuation","enable_speaker_diarization",b"enable_speaker_diarization","enable_word_confidence",b"enable_word_confidence","filter_profanity",b"filter_profanity","language_code",b"language_code","max_alternatives",b"max_alternatives","speech_contexts",b"speech_contexts"]) -> None: ...
global___SpeechTranscriptionConfig = SpeechTranscriptionConfig

class SpeechContext(google.protobuf.message.Message):
    """Provides "hints" to the speech recognizer to favor specific words and phrases
    in the results.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    PHRASES_FIELD_NUMBER: builtins.int
    @property
    def phrases(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
        """Optional. A list of strings containing words and phrases "hints" so that
        the speech recognition is more likely to recognize them. This can be used
        to improve the accuracy for specific words and phrases, for example, if
        specific commands are typically spoken by the user. This can also be used
        to add additional words to the vocabulary of the recognizer. See
        [usage limits](https://cloud.google.com/speech/limits#content).
        """
        pass
    def __init__(self,
        *,
        phrases : typing.Optional[typing.Iterable[typing.Text]] = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["phrases",b"phrases"]) -> None: ...
global___SpeechContext = SpeechContext

class SpeechTranscription(google.protobuf.message.Message):
    """A speech recognition result corresponding to a portion of the audio."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    ALTERNATIVES_FIELD_NUMBER: builtins.int
    LANGUAGE_CODE_FIELD_NUMBER: builtins.int
    @property
    def alternatives(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___SpeechRecognitionAlternative]:
        """May contain one or more recognition hypotheses (up to the maximum specified
        in `max_alternatives`).  These alternatives are ordered in terms of
        accuracy, with the top (first) alternative being the most probable, as
        ranked by the recognizer.
        """
        pass
    language_code: typing.Text = ...
    """Output only. The [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt)
    language tag of the language in this result. This language code was
    detected to have the most likelihood of being spoken in the audio.
    """

    def __init__(self,
        *,
        alternatives : typing.Optional[typing.Iterable[global___SpeechRecognitionAlternative]] = ...,
        language_code : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["alternatives",b"alternatives","language_code",b"language_code"]) -> None: ...
global___SpeechTranscription = SpeechTranscription

class SpeechRecognitionAlternative(google.protobuf.message.Message):
    """Alternative hypotheses (a.k.a. n-best list)."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    TRANSCRIPT_FIELD_NUMBER: builtins.int
    CONFIDENCE_FIELD_NUMBER: builtins.int
    WORDS_FIELD_NUMBER: builtins.int
    transcript: typing.Text = ...
    """Transcript text representing the words that the user spoke."""

    confidence: builtins.float = ...
    """Output only. The confidence estimate between 0.0 and 1.0. A higher number
    indicates an estimated greater likelihood that the recognized words are
    correct. This field is set only for the top alternative.
    This field is not guaranteed to be accurate and users should not rely on it
    to be always provided.
    The default of 0.0 is a sentinel value indicating `confidence` was not set.
    """

    @property
    def words(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___WordInfo]:
        """Output only. A list of word-specific information for each recognized word.
        Note: When `enable_speaker_diarization` is set to true, you will see all
        the words from the beginning of the audio.
        """
        pass
    def __init__(self,
        *,
        transcript : typing.Text = ...,
        confidence : builtins.float = ...,
        words : typing.Optional[typing.Iterable[global___WordInfo]] = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["confidence",b"confidence","transcript",b"transcript","words",b"words"]) -> None: ...
global___SpeechRecognitionAlternative = SpeechRecognitionAlternative

class WordInfo(google.protobuf.message.Message):
    """Word-specific information for recognized words. Word information is only
    included in the response when certain request parameters are set, such
    as `enable_word_time_offsets`.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    START_TIME_FIELD_NUMBER: builtins.int
    END_TIME_FIELD_NUMBER: builtins.int
    WORD_FIELD_NUMBER: builtins.int
    CONFIDENCE_FIELD_NUMBER: builtins.int
    SPEAKER_TAG_FIELD_NUMBER: builtins.int
    @property
    def start_time(self) -> google.protobuf.duration_pb2.Duration:
        """Time offset relative to the beginning of the audio, and
        corresponding to the start of the spoken word. This field is only set if
        `enable_word_time_offsets=true` and only in the top hypothesis. This is an
        experimental feature and the accuracy of the time offset can vary.
        """
        pass
    @property
    def end_time(self) -> google.protobuf.duration_pb2.Duration:
        """Time offset relative to the beginning of the audio, and
        corresponding to the end of the spoken word. This field is only set if
        `enable_word_time_offsets=true` and only in the top hypothesis. This is an
        experimental feature and the accuracy of the time offset can vary.
        """
        pass
    word: typing.Text = ...
    """The word corresponding to this set of information."""

    confidence: builtins.float = ...
    """Output only. The confidence estimate between 0.0 and 1.0. A higher number
    indicates an estimated greater likelihood that the recognized words are
    correct. This field is set only for the top alternative.
    This field is not guaranteed to be accurate and users should not rely on it
    to be always provided.
    The default of 0.0 is a sentinel value indicating `confidence` was not set.
    """

    speaker_tag: builtins.int = ...
    """Output only. A distinct integer value is assigned for every speaker within
    the audio. This field specifies which one of those speakers was detected to
    have spoken this word. Value ranges from 1 up to diarization_speaker_count,
    and is only set if speaker diarization is enabled.
    """

    def __init__(self,
        *,
        start_time : typing.Optional[google.protobuf.duration_pb2.Duration] = ...,
        end_time : typing.Optional[google.protobuf.duration_pb2.Duration] = ...,
        word : typing.Text = ...,
        confidence : builtins.float = ...,
        speaker_tag : builtins.int = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["end_time",b"end_time","start_time",b"start_time"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["confidence",b"confidence","end_time",b"end_time","speaker_tag",b"speaker_tag","start_time",b"start_time","word",b"word"]) -> None: ...
global___WordInfo = WordInfo

class NormalizedVertex(google.protobuf.message.Message):
    """A vertex represents a 2D point in the image.
    NOTE: the normalized vertex coordinates are relative to the original image
    and range from 0 to 1.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    X_FIELD_NUMBER: builtins.int
    Y_FIELD_NUMBER: builtins.int
    x: builtins.float = ...
    """X coordinate."""

    y: builtins.float = ...
    """Y coordinate."""

    def __init__(self,
        *,
        x : builtins.float = ...,
        y : builtins.float = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["x",b"x","y",b"y"]) -> None: ...
global___NormalizedVertex = NormalizedVertex

class NormalizedBoundingPoly(google.protobuf.message.Message):
    """Normalized bounding polygon for text (that might not be aligned with axis).
    Contains list of the corner points in clockwise order starting from
    top-left corner. For example, for a rectangular bounding box:
    When the text is horizontal it might look like:
            0----1
            |    |
            3----2

    When it's clockwise rotated 180 degrees around the top-left corner it
    becomes:
            2----3
            |    |
            1----0

    and the vertex order will still be (0, 1, 2, 3). Note that values can be less
    than 0, or greater than 1 due to trignometric calculations for location of
    the box.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    VERTICES_FIELD_NUMBER: builtins.int
    @property
    def vertices(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___NormalizedVertex]:
        """Normalized vertices of the bounding polygon."""
        pass
    def __init__(self,
        *,
        vertices : typing.Optional[typing.Iterable[global___NormalizedVertex]] = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["vertices",b"vertices"]) -> None: ...
global___NormalizedBoundingPoly = NormalizedBoundingPoly

class TextSegment(google.protobuf.message.Message):
    """Video segment level annotation results for text detection."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    SEGMENT_FIELD_NUMBER: builtins.int
    CONFIDENCE_FIELD_NUMBER: builtins.int
    FRAMES_FIELD_NUMBER: builtins.int
    @property
    def segment(self) -> global___VideoSegment:
        """Video segment where a text snippet was detected."""
        pass
    confidence: builtins.float = ...
    """Confidence for the track of detected text. It is calculated as the highest
    over all frames where OCR detected text appears.
    """

    @property
    def frames(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___TextFrame]:
        """Information related to the frames where OCR detected text appears."""
        pass
    def __init__(self,
        *,
        segment : typing.Optional[global___VideoSegment] = ...,
        confidence : builtins.float = ...,
        frames : typing.Optional[typing.Iterable[global___TextFrame]] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["segment",b"segment"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["confidence",b"confidence","frames",b"frames","segment",b"segment"]) -> None: ...
global___TextSegment = TextSegment

class TextFrame(google.protobuf.message.Message):
    """Video frame level annotation results for text annotation (OCR).
    Contains information regarding timestamp and bounding box locations for the
    frames containing detected OCR text snippets.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    ROTATED_BOUNDING_BOX_FIELD_NUMBER: builtins.int
    TIME_OFFSET_FIELD_NUMBER: builtins.int
    @property
    def rotated_bounding_box(self) -> global___NormalizedBoundingPoly:
        """Bounding polygon of the detected text for this frame."""
        pass
    @property
    def time_offset(self) -> google.protobuf.duration_pb2.Duration:
        """Timestamp of this frame."""
        pass
    def __init__(self,
        *,
        rotated_bounding_box : typing.Optional[global___NormalizedBoundingPoly] = ...,
        time_offset : typing.Optional[google.protobuf.duration_pb2.Duration] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["rotated_bounding_box",b"rotated_bounding_box","time_offset",b"time_offset"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["rotated_bounding_box",b"rotated_bounding_box","time_offset",b"time_offset"]) -> None: ...
global___TextFrame = TextFrame

class TextAnnotation(google.protobuf.message.Message):
    """Annotations related to one detected OCR text snippet. This will contain the
    corresponding text, confidence value, and frame level information for each
    detection.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    TEXT_FIELD_NUMBER: builtins.int
    SEGMENTS_FIELD_NUMBER: builtins.int
    text: typing.Text = ...
    """The detected text."""

    @property
    def segments(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___TextSegment]:
        """All video segments where OCR detected text appears."""
        pass
    def __init__(self,
        *,
        text : typing.Text = ...,
        segments : typing.Optional[typing.Iterable[global___TextSegment]] = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["segments",b"segments","text",b"text"]) -> None: ...
global___TextAnnotation = TextAnnotation

class ObjectTrackingFrame(google.protobuf.message.Message):
    """Video frame level annotations for object detection and tracking. This field
    stores per frame location, time offset, and confidence.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    NORMALIZED_BOUNDING_BOX_FIELD_NUMBER: builtins.int
    TIME_OFFSET_FIELD_NUMBER: builtins.int
    @property
    def normalized_bounding_box(self) -> global___NormalizedBoundingBox:
        """The normalized bounding box location of this object track for the frame."""
        pass
    @property
    def time_offset(self) -> google.protobuf.duration_pb2.Duration:
        """The timestamp of the frame in microseconds."""
        pass
    def __init__(self,
        *,
        normalized_bounding_box : typing.Optional[global___NormalizedBoundingBox] = ...,
        time_offset : typing.Optional[google.protobuf.duration_pb2.Duration] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["normalized_bounding_box",b"normalized_bounding_box","time_offset",b"time_offset"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["normalized_bounding_box",b"normalized_bounding_box","time_offset",b"time_offset"]) -> None: ...
global___ObjectTrackingFrame = ObjectTrackingFrame

class ObjectTrackingAnnotation(google.protobuf.message.Message):
    """Annotations corresponding to one tracked object."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    SEGMENT_FIELD_NUMBER: builtins.int
    TRACK_ID_FIELD_NUMBER: builtins.int
    ENTITY_FIELD_NUMBER: builtins.int
    CONFIDENCE_FIELD_NUMBER: builtins.int
    FRAMES_FIELD_NUMBER: builtins.int
    @property
    def segment(self) -> global___VideoSegment:
        """Non-streaming batch mode ONLY.
        Each object track corresponds to one video segment where it appears.
        """
        pass
    track_id: builtins.int = ...
    """Streaming mode ONLY.
    In streaming mode, we do not know the end time of a tracked object
    before it is completed. Hence, there is no VideoSegment info returned.
    Instead, we provide a unique identifiable integer track_id so that
    the customers can correlate the results of the ongoing
    ObjectTrackAnnotation of the same track_id over time.
    """

    @property
    def entity(self) -> global___Entity:
        """Entity to specify the object category that this track is labeled as."""
        pass
    confidence: builtins.float = ...
    """Object category's labeling confidence of this track."""

    @property
    def frames(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___ObjectTrackingFrame]:
        """Information corresponding to all frames where this object track appears.
        Non-streaming batch mode: it may be one or multiple ObjectTrackingFrame
        messages in frames.
        Streaming mode: it can only be one ObjectTrackingFrame message in frames.
        """
        pass
    def __init__(self,
        *,
        segment : typing.Optional[global___VideoSegment] = ...,
        track_id : builtins.int = ...,
        entity : typing.Optional[global___Entity] = ...,
        confidence : builtins.float = ...,
        frames : typing.Optional[typing.Iterable[global___ObjectTrackingFrame]] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["entity",b"entity","segment",b"segment","track_id",b"track_id","track_info",b"track_info"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["confidence",b"confidence","entity",b"entity","frames",b"frames","segment",b"segment","track_id",b"track_id","track_info",b"track_info"]) -> None: ...
    def WhichOneof(self, oneof_group: typing_extensions.Literal["track_info",b"track_info"]) -> typing.Optional[typing_extensions.Literal["segment","track_id"]]: ...
global___ObjectTrackingAnnotation = ObjectTrackingAnnotation

class LogoRecognitionAnnotation(google.protobuf.message.Message):
    """Annotation corresponding to one detected, tracked and recognized logo class."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    ENTITY_FIELD_NUMBER: builtins.int
    TRACKS_FIELD_NUMBER: builtins.int
    SEGMENTS_FIELD_NUMBER: builtins.int
    @property
    def entity(self) -> global___Entity:
        """Entity category information to specify the logo class that all the logo
        tracks within this LogoRecognitionAnnotation are recognized as.
        """
        pass
    @property
    def tracks(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___Track]:
        """All logo tracks where the recognized logo appears. Each track corresponds
        to one logo instance appearing in consecutive frames.
        """
        pass
    @property
    def segments(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___VideoSegment]:
        """All video segments where the recognized logo appears. There might be
        multiple instances of the same logo class appearing in one VideoSegment.
        """
        pass
    def __init__(self,
        *,
        entity : typing.Optional[global___Entity] = ...,
        tracks : typing.Optional[typing.Iterable[global___Track]] = ...,
        segments : typing.Optional[typing.Iterable[global___VideoSegment]] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["entity",b"entity"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["entity",b"entity","segments",b"segments","tracks",b"tracks"]) -> None: ...
global___LogoRecognitionAnnotation = LogoRecognitionAnnotation

class StreamingAnnotateVideoRequest(google.protobuf.message.Message):
    """The top-level message sent by the client for the `StreamingAnnotateVideo`
    method. Multiple `StreamingAnnotateVideoRequest` messages are sent.
    The first message must only contain a `StreamingVideoConfig` message.
    All subsequent messages must only contain `input_content` data.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    VIDEO_CONFIG_FIELD_NUMBER: builtins.int
    INPUT_CONTENT_FIELD_NUMBER: builtins.int
    @property
    def video_config(self) -> global___StreamingVideoConfig:
        """Provides information to the annotator, specifing how to process the
        request. The first `AnnotateStreamingVideoRequest` message must only
        contain a `video_config` message.
        """
        pass
    input_content: builtins.bytes = ...
    """The video data to be annotated. Chunks of video data are sequentially
    sent in `StreamingAnnotateVideoRequest` messages. Except the initial
    `StreamingAnnotateVideoRequest` message containing only
    `video_config`, all subsequent `AnnotateStreamingVideoRequest`
    messages must only contain `input_content` field.
    Note: as with all bytes fields, protobuffers use a pure binary
    representation (not base64).
    """

    def __init__(self,
        *,
        video_config : typing.Optional[global___StreamingVideoConfig] = ...,
        input_content : builtins.bytes = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["input_content",b"input_content","streaming_request",b"streaming_request","video_config",b"video_config"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["input_content",b"input_content","streaming_request",b"streaming_request","video_config",b"video_config"]) -> None: ...
    def WhichOneof(self, oneof_group: typing_extensions.Literal["streaming_request",b"streaming_request"]) -> typing.Optional[typing_extensions.Literal["video_config","input_content"]]: ...
global___StreamingAnnotateVideoRequest = StreamingAnnotateVideoRequest

class StreamingVideoConfig(google.protobuf.message.Message):
    """Provides information to the annotator that specifies how to process the
    request.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    SHOT_CHANGE_DETECTION_CONFIG_FIELD_NUMBER: builtins.int
    LABEL_DETECTION_CONFIG_FIELD_NUMBER: builtins.int
    EXPLICIT_CONTENT_DETECTION_CONFIG_FIELD_NUMBER: builtins.int
    OBJECT_TRACKING_CONFIG_FIELD_NUMBER: builtins.int
    AUTOML_ACTION_RECOGNITION_CONFIG_FIELD_NUMBER: builtins.int
    AUTOML_CLASSIFICATION_CONFIG_FIELD_NUMBER: builtins.int
    AUTOML_OBJECT_TRACKING_CONFIG_FIELD_NUMBER: builtins.int
    FEATURE_FIELD_NUMBER: builtins.int
    STORAGE_CONFIG_FIELD_NUMBER: builtins.int
    @property
    def shot_change_detection_config(self) -> global___StreamingShotChangeDetectionConfig:
        """Config for STREAMING_SHOT_CHANGE_DETECTION."""
        pass
    @property
    def label_detection_config(self) -> global___StreamingLabelDetectionConfig:
        """Config for STREAMING_LABEL_DETECTION."""
        pass
    @property
    def explicit_content_detection_config(self) -> global___StreamingExplicitContentDetectionConfig:
        """Config for STREAMING_EXPLICIT_CONTENT_DETECTION."""
        pass
    @property
    def object_tracking_config(self) -> global___StreamingObjectTrackingConfig:
        """Config for STREAMING_OBJECT_TRACKING."""
        pass
    @property
    def automl_action_recognition_config(self) -> global___StreamingAutomlActionRecognitionConfig:
        """Config for STREAMING_AUTOML_ACTION_RECOGNITION."""
        pass
    @property
    def automl_classification_config(self) -> global___StreamingAutomlClassificationConfig:
        """Config for STREAMING_AUTOML_CLASSIFICATION."""
        pass
    @property
    def automl_object_tracking_config(self) -> global___StreamingAutomlObjectTrackingConfig:
        """Config for STREAMING_AUTOML_OBJECT_TRACKING."""
        pass
    feature: global___StreamingFeature.ValueType = ...
    """Requested annotation feature."""

    @property
    def storage_config(self) -> global___StreamingStorageConfig:
        """Streaming storage option. By default: storage is disabled."""
        pass
    def __init__(self,
        *,
        shot_change_detection_config : typing.Optional[global___StreamingShotChangeDetectionConfig] = ...,
        label_detection_config : typing.Optional[global___StreamingLabelDetectionConfig] = ...,
        explicit_content_detection_config : typing.Optional[global___StreamingExplicitContentDetectionConfig] = ...,
        object_tracking_config : typing.Optional[global___StreamingObjectTrackingConfig] = ...,
        automl_action_recognition_config : typing.Optional[global___StreamingAutomlActionRecognitionConfig] = ...,
        automl_classification_config : typing.Optional[global___StreamingAutomlClassificationConfig] = ...,
        automl_object_tracking_config : typing.Optional[global___StreamingAutomlObjectTrackingConfig] = ...,
        feature : global___StreamingFeature.ValueType = ...,
        storage_config : typing.Optional[global___StreamingStorageConfig] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["automl_action_recognition_config",b"automl_action_recognition_config","automl_classification_config",b"automl_classification_config","automl_object_tracking_config",b"automl_object_tracking_config","explicit_content_detection_config",b"explicit_content_detection_config","label_detection_config",b"label_detection_config","object_tracking_config",b"object_tracking_config","shot_change_detection_config",b"shot_change_detection_config","storage_config",b"storage_config","streaming_config",b"streaming_config"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["automl_action_recognition_config",b"automl_action_recognition_config","automl_classification_config",b"automl_classification_config","automl_object_tracking_config",b"automl_object_tracking_config","explicit_content_detection_config",b"explicit_content_detection_config","feature",b"feature","label_detection_config",b"label_detection_config","object_tracking_config",b"object_tracking_config","shot_change_detection_config",b"shot_change_detection_config","storage_config",b"storage_config","streaming_config",b"streaming_config"]) -> None: ...
    def WhichOneof(self, oneof_group: typing_extensions.Literal["streaming_config",b"streaming_config"]) -> typing.Optional[typing_extensions.Literal["shot_change_detection_config","label_detection_config","explicit_content_detection_config","object_tracking_config","automl_action_recognition_config","automl_classification_config","automl_object_tracking_config"]]: ...
global___StreamingVideoConfig = StreamingVideoConfig

class StreamingAnnotateVideoResponse(google.protobuf.message.Message):
    """`StreamingAnnotateVideoResponse` is the only message returned to the client
    by `StreamingAnnotateVideo`. A series of zero or more
    `StreamingAnnotateVideoResponse` messages are streamed back to the client.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    ERROR_FIELD_NUMBER: builtins.int
    ANNOTATION_RESULTS_FIELD_NUMBER: builtins.int
    ANNOTATION_RESULTS_URI_FIELD_NUMBER: builtins.int
    @property
    def error(self) -> google.rpc.status_pb2.Status:
        """If set, returns a [google.rpc.Status][google.rpc.Status] message that
        specifies the error for the operation.
        """
        pass
    @property
    def annotation_results(self) -> global___StreamingVideoAnnotationResults:
        """Streaming annotation results."""
        pass
    annotation_results_uri: typing.Text = ...
    """Google Cloud Storage(GCS) URI that stores annotation results of one
    streaming session in JSON format.
    It is the annotation_result_storage_directory
    from the request followed by '/cloud_project_number-session_id'.
    """

    def __init__(self,
        *,
        error : typing.Optional[google.rpc.status_pb2.Status] = ...,
        annotation_results : typing.Optional[global___StreamingVideoAnnotationResults] = ...,
        annotation_results_uri : typing.Text = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["annotation_results",b"annotation_results","error",b"error"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["annotation_results",b"annotation_results","annotation_results_uri",b"annotation_results_uri","error",b"error"]) -> None: ...
global___StreamingAnnotateVideoResponse = StreamingAnnotateVideoResponse

class StreamingVideoAnnotationResults(google.protobuf.message.Message):
    """Streaming annotation results corresponding to a portion of the video
    that is currently being processed.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    SHOT_ANNOTATIONS_FIELD_NUMBER: builtins.int
    LABEL_ANNOTATIONS_FIELD_NUMBER: builtins.int
    EXPLICIT_ANNOTATION_FIELD_NUMBER: builtins.int
    OBJECT_ANNOTATIONS_FIELD_NUMBER: builtins.int
    @property
    def shot_annotations(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___VideoSegment]:
        """Shot annotation results. Each shot is represented as a video segment."""
        pass
    @property
    def label_annotations(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___LabelAnnotation]:
        """Label annotation results."""
        pass
    @property
    def explicit_annotation(self) -> global___ExplicitContentAnnotation:
        """Explicit content annotation results."""
        pass
    @property
    def object_annotations(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___ObjectTrackingAnnotation]:
        """Object tracking results."""
        pass
    def __init__(self,
        *,
        shot_annotations : typing.Optional[typing.Iterable[global___VideoSegment]] = ...,
        label_annotations : typing.Optional[typing.Iterable[global___LabelAnnotation]] = ...,
        explicit_annotation : typing.Optional[global___ExplicitContentAnnotation] = ...,
        object_annotations : typing.Optional[typing.Iterable[global___ObjectTrackingAnnotation]] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["explicit_annotation",b"explicit_annotation"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["explicit_annotation",b"explicit_annotation","label_annotations",b"label_annotations","object_annotations",b"object_annotations","shot_annotations",b"shot_annotations"]) -> None: ...
global___StreamingVideoAnnotationResults = StreamingVideoAnnotationResults

class StreamingShotChangeDetectionConfig(google.protobuf.message.Message):
    """Config for STREAMING_SHOT_CHANGE_DETECTION."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    def __init__(self,
        ) -> None: ...
global___StreamingShotChangeDetectionConfig = StreamingShotChangeDetectionConfig

class StreamingLabelDetectionConfig(google.protobuf.message.Message):
    """Config for STREAMING_LABEL_DETECTION."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    STATIONARY_CAMERA_FIELD_NUMBER: builtins.int
    stationary_camera: builtins.bool = ...
    """Whether the video has been captured from a stationary (i.e. non-moving)
    camera. When set to true, might improve detection accuracy for moving
    objects. Default: false.
    """

    def __init__(self,
        *,
        stationary_camera : builtins.bool = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["stationary_camera",b"stationary_camera"]) -> None: ...
global___StreamingLabelDetectionConfig = StreamingLabelDetectionConfig

class StreamingExplicitContentDetectionConfig(google.protobuf.message.Message):
    """Config for STREAMING_EXPLICIT_CONTENT_DETECTION."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    def __init__(self,
        ) -> None: ...
global___StreamingExplicitContentDetectionConfig = StreamingExplicitContentDetectionConfig

class StreamingObjectTrackingConfig(google.protobuf.message.Message):
    """Config for STREAMING_OBJECT_TRACKING."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    def __init__(self,
        ) -> None: ...
global___StreamingObjectTrackingConfig = StreamingObjectTrackingConfig

class StreamingAutomlActionRecognitionConfig(google.protobuf.message.Message):
    """Config for STREAMING_AUTOML_ACTION_RECOGNITION."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    MODEL_NAME_FIELD_NUMBER: builtins.int
    model_name: typing.Text = ...
    """Resource name of AutoML model.
    Format: `projects/{project_id}/locations/{location_id}/models/{model_id}`
    """

    def __init__(self,
        *,
        model_name : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["model_name",b"model_name"]) -> None: ...
global___StreamingAutomlActionRecognitionConfig = StreamingAutomlActionRecognitionConfig

class StreamingAutomlClassificationConfig(google.protobuf.message.Message):
    """Config for STREAMING_AUTOML_CLASSIFICATION."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    MODEL_NAME_FIELD_NUMBER: builtins.int
    model_name: typing.Text = ...
    """Resource name of AutoML model.
    Format:
    `projects/{project_number}/locations/{location_id}/models/{model_id}`
    """

    def __init__(self,
        *,
        model_name : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["model_name",b"model_name"]) -> None: ...
global___StreamingAutomlClassificationConfig = StreamingAutomlClassificationConfig

class StreamingAutomlObjectTrackingConfig(google.protobuf.message.Message):
    """Config for STREAMING_AUTOML_OBJECT_TRACKING."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    MODEL_NAME_FIELD_NUMBER: builtins.int
    model_name: typing.Text = ...
    """Resource name of AutoML model.
    Format: `projects/{project_id}/locations/{location_id}/models/{model_id}`
    """

    def __init__(self,
        *,
        model_name : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["model_name",b"model_name"]) -> None: ...
global___StreamingAutomlObjectTrackingConfig = StreamingAutomlObjectTrackingConfig

class StreamingStorageConfig(google.protobuf.message.Message):
    """Config for streaming storage option."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    ENABLE_STORAGE_ANNOTATION_RESULT_FIELD_NUMBER: builtins.int
    ANNOTATION_RESULT_STORAGE_DIRECTORY_FIELD_NUMBER: builtins.int
    enable_storage_annotation_result: builtins.bool = ...
    """Enable streaming storage. Default: false."""

    annotation_result_storage_directory: typing.Text = ...
    """Cloud Storage URI to store all annotation results for one client. Client
    should specify this field as the top-level storage directory. Annotation
    results of different sessions will be put into different sub-directories
    denoted by project_name and session_id. All sub-directories will be auto
    generated by program and will be made accessible to client in response
    proto. URIs must be specified in the following format:
    `gs://bucket-id/object-id` `bucket-id` should be a valid Cloud Storage
    bucket created by client and bucket permission shall also be configured
    properly. `object-id` can be arbitrary string that make sense to client.
    Other URI formats will return error and cause Cloud Storage write failure.
    """

    def __init__(self,
        *,
        enable_storage_annotation_result : builtins.bool = ...,
        annotation_result_storage_directory : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["annotation_result_storage_directory",b"annotation_result_storage_directory","enable_storage_annotation_result",b"enable_storage_annotation_result"]) -> None: ...
global___StreamingStorageConfig = StreamingStorageConfig
