"""
@generated by mypy-protobuf.  Do not edit manually!
isort:skip_file
"""
import builtins
import google.protobuf.descriptor
import google.protobuf.internal.containers
import google.protobuf.internal.enum_type_wrapper
import google.protobuf.message
import google.protobuf.struct_pb2
import typing
import typing_extensions

DESCRIPTOR: google.protobuf.descriptor.FileDescriptor = ...

class ExplanationMetadata(google.protobuf.message.Message):
    """Metadata describing the Model's input and output for explanation."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class InputMetadata(google.protobuf.message.Message):
        """Metadata of the input of a feature.

        Fields other than [InputMetadata.input_baselines][google.cloud.aiplatform.v1beta1.ExplanationMetadata.InputMetadata.input_baselines] are applicable only
        for Models that are using Vertex AI-provided images for Tensorflow.
        """
        DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
        class _Encoding:
            ValueType = typing.NewType('ValueType', builtins.int)
            V: typing_extensions.TypeAlias = ValueType
        class _EncodingEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_Encoding.ValueType], builtins.type):
            DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor = ...
            ENCODING_UNSPECIFIED: ExplanationMetadata.InputMetadata.Encoding.ValueType = ...  # 0
            """Default value. This is the same as IDENTITY."""

            IDENTITY: ExplanationMetadata.InputMetadata.Encoding.ValueType = ...  # 1
            """The tensor represents one feature."""

            BAG_OF_FEATURES: ExplanationMetadata.InputMetadata.Encoding.ValueType = ...  # 2
            """The tensor represents a bag of features where each index maps to
            a feature. [InputMetadata.index_feature_mapping][google.cloud.aiplatform.v1beta1.ExplanationMetadata.InputMetadata.index_feature_mapping] must be provided for
            this encoding. For example:
            ```
            input = [27, 6.0, 150]
            index_feature_mapping = ["age", "height", "weight"]
            ```
            """

            BAG_OF_FEATURES_SPARSE: ExplanationMetadata.InputMetadata.Encoding.ValueType = ...  # 3
            """The tensor represents a bag of features where each index maps to a
            feature. Zero values in the tensor indicates feature being
            non-existent. [InputMetadata.index_feature_mapping][google.cloud.aiplatform.v1beta1.ExplanationMetadata.InputMetadata.index_feature_mapping] must be provided
            for this encoding. For example:
            ```
            input = [2, 0, 5, 0, 1]
            index_feature_mapping = ["a", "b", "c", "d", "e"]
            ```
            """

            INDICATOR: ExplanationMetadata.InputMetadata.Encoding.ValueType = ...  # 4
            """The tensor is a list of binaries representing whether a feature exists
            or not (1 indicates existence). [InputMetadata.index_feature_mapping][google.cloud.aiplatform.v1beta1.ExplanationMetadata.InputMetadata.index_feature_mapping]
            must be provided for this encoding. For example:
            ```
            input = [1, 0, 1, 0, 1]
            index_feature_mapping = ["a", "b", "c", "d", "e"]
            ```
            """

            COMBINED_EMBEDDING: ExplanationMetadata.InputMetadata.Encoding.ValueType = ...  # 5
            """The tensor is encoded into a 1-dimensional array represented by an
            encoded tensor. [InputMetadata.encoded_tensor_name][google.cloud.aiplatform.v1beta1.ExplanationMetadata.InputMetadata.encoded_tensor_name] must be provided
            for this encoding. For example:
            ```
            input = ["This", "is", "a", "test", "."]
            encoded = [0.1, 0.2, 0.3, 0.4, 0.5]
            ```
            """

            CONCAT_EMBEDDING: ExplanationMetadata.InputMetadata.Encoding.ValueType = ...  # 6
            """Select this encoding when the input tensor is encoded into a
            2-dimensional array represented by an encoded tensor.
            [InputMetadata.encoded_tensor_name][google.cloud.aiplatform.v1beta1.ExplanationMetadata.InputMetadata.encoded_tensor_name] must be provided for this
            encoding. The first dimension of the encoded tensor's shape is the same
            as the input tensor's shape. For example:
            ```
            input = ["This", "is", "a", "test", "."]
            encoded = [[0.1, 0.2, 0.3, 0.4, 0.5],
                       [0.2, 0.1, 0.4, 0.3, 0.5],
                       [0.5, 0.1, 0.3, 0.5, 0.4],
                       [0.5, 0.3, 0.1, 0.2, 0.4],
                       [0.4, 0.3, 0.2, 0.5, 0.1]]
            ```
            """

        class Encoding(_Encoding, metaclass=_EncodingEnumTypeWrapper):
            """Defines how a feature is encoded. Defaults to IDENTITY."""
            pass

        ENCODING_UNSPECIFIED: ExplanationMetadata.InputMetadata.Encoding.ValueType = ...  # 0
        """Default value. This is the same as IDENTITY."""

        IDENTITY: ExplanationMetadata.InputMetadata.Encoding.ValueType = ...  # 1
        """The tensor represents one feature."""

        BAG_OF_FEATURES: ExplanationMetadata.InputMetadata.Encoding.ValueType = ...  # 2
        """The tensor represents a bag of features where each index maps to
        a feature. [InputMetadata.index_feature_mapping][google.cloud.aiplatform.v1beta1.ExplanationMetadata.InputMetadata.index_feature_mapping] must be provided for
        this encoding. For example:
        ```
        input = [27, 6.0, 150]
        index_feature_mapping = ["age", "height", "weight"]
        ```
        """

        BAG_OF_FEATURES_SPARSE: ExplanationMetadata.InputMetadata.Encoding.ValueType = ...  # 3
        """The tensor represents a bag of features where each index maps to a
        feature. Zero values in the tensor indicates feature being
        non-existent. [InputMetadata.index_feature_mapping][google.cloud.aiplatform.v1beta1.ExplanationMetadata.InputMetadata.index_feature_mapping] must be provided
        for this encoding. For example:
        ```
        input = [2, 0, 5, 0, 1]
        index_feature_mapping = ["a", "b", "c", "d", "e"]
        ```
        """

        INDICATOR: ExplanationMetadata.InputMetadata.Encoding.ValueType = ...  # 4
        """The tensor is a list of binaries representing whether a feature exists
        or not (1 indicates existence). [InputMetadata.index_feature_mapping][google.cloud.aiplatform.v1beta1.ExplanationMetadata.InputMetadata.index_feature_mapping]
        must be provided for this encoding. For example:
        ```
        input = [1, 0, 1, 0, 1]
        index_feature_mapping = ["a", "b", "c", "d", "e"]
        ```
        """

        COMBINED_EMBEDDING: ExplanationMetadata.InputMetadata.Encoding.ValueType = ...  # 5
        """The tensor is encoded into a 1-dimensional array represented by an
        encoded tensor. [InputMetadata.encoded_tensor_name][google.cloud.aiplatform.v1beta1.ExplanationMetadata.InputMetadata.encoded_tensor_name] must be provided
        for this encoding. For example:
        ```
        input = ["This", "is", "a", "test", "."]
        encoded = [0.1, 0.2, 0.3, 0.4, 0.5]
        ```
        """

        CONCAT_EMBEDDING: ExplanationMetadata.InputMetadata.Encoding.ValueType = ...  # 6
        """Select this encoding when the input tensor is encoded into a
        2-dimensional array represented by an encoded tensor.
        [InputMetadata.encoded_tensor_name][google.cloud.aiplatform.v1beta1.ExplanationMetadata.InputMetadata.encoded_tensor_name] must be provided for this
        encoding. The first dimension of the encoded tensor's shape is the same
        as the input tensor's shape. For example:
        ```
        input = ["This", "is", "a", "test", "."]
        encoded = [[0.1, 0.2, 0.3, 0.4, 0.5],
                   [0.2, 0.1, 0.4, 0.3, 0.5],
                   [0.5, 0.1, 0.3, 0.5, 0.4],
                   [0.5, 0.3, 0.1, 0.2, 0.4],
                   [0.4, 0.3, 0.2, 0.5, 0.1]]
        ```
        """


        class FeatureValueDomain(google.protobuf.message.Message):
            """Domain details of the input feature value. Provides numeric information
            about the feature, such as its range (min, max). If the feature has been
            pre-processed, for example with z-scoring, then it provides information
            about how to recover the original feature. For example, if the input
            feature is an image and it has been pre-processed to obtain 0-mean and
            stddev = 1 values, then original_mean, and original_stddev refer to the
            mean and stddev of the original feature (e.g. image tensor) from which
            input feature (with mean = 0 and stddev = 1) was obtained.
            """
            DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
            MIN_VALUE_FIELD_NUMBER: builtins.int
            MAX_VALUE_FIELD_NUMBER: builtins.int
            ORIGINAL_MEAN_FIELD_NUMBER: builtins.int
            ORIGINAL_STDDEV_FIELD_NUMBER: builtins.int
            min_value: builtins.float = ...
            """The minimum permissible value for this feature."""

            max_value: builtins.float = ...
            """The maximum permissible value for this feature."""

            original_mean: builtins.float = ...
            """If this input feature has been normalized to a mean value of 0,
            the original_mean specifies the mean value of the domain prior to
            normalization.
            """

            original_stddev: builtins.float = ...
            """If this input feature has been normalized to a standard deviation of
            1.0, the original_stddev specifies the standard deviation of the domain
            prior to normalization.
            """

            def __init__(self,
                *,
                min_value : builtins.float = ...,
                max_value : builtins.float = ...,
                original_mean : builtins.float = ...,
                original_stddev : builtins.float = ...,
                ) -> None: ...
            def ClearField(self, field_name: typing_extensions.Literal["max_value",b"max_value","min_value",b"min_value","original_mean",b"original_mean","original_stddev",b"original_stddev"]) -> None: ...

        class Visualization(google.protobuf.message.Message):
            """Visualization configurations for image explanation."""
            DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
            class _Type:
                ValueType = typing.NewType('ValueType', builtins.int)
                V: typing_extensions.TypeAlias = ValueType
            class _TypeEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_Type.ValueType], builtins.type):
                DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor = ...
                TYPE_UNSPECIFIED: ExplanationMetadata.InputMetadata.Visualization.Type.ValueType = ...  # 0
                """Should not be used."""

                PIXELS: ExplanationMetadata.InputMetadata.Visualization.Type.ValueType = ...  # 1
                """Shows which pixel contributed to the image prediction."""

                OUTLINES: ExplanationMetadata.InputMetadata.Visualization.Type.ValueType = ...  # 2
                """Shows which region contributed to the image prediction by outlining
                the region.
                """

            class Type(_Type, metaclass=_TypeEnumTypeWrapper):
                """Type of the image visualization. Only applicable to
                [Integrated Gradients attribution][google.cloud.aiplatform.v1beta1.ExplanationParameters.integrated_gradients_attribution].
                """
                pass

            TYPE_UNSPECIFIED: ExplanationMetadata.InputMetadata.Visualization.Type.ValueType = ...  # 0
            """Should not be used."""

            PIXELS: ExplanationMetadata.InputMetadata.Visualization.Type.ValueType = ...  # 1
            """Shows which pixel contributed to the image prediction."""

            OUTLINES: ExplanationMetadata.InputMetadata.Visualization.Type.ValueType = ...  # 2
            """Shows which region contributed to the image prediction by outlining
            the region.
            """


            class _Polarity:
                ValueType = typing.NewType('ValueType', builtins.int)
                V: typing_extensions.TypeAlias = ValueType
            class _PolarityEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_Polarity.ValueType], builtins.type):
                DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor = ...
                POLARITY_UNSPECIFIED: ExplanationMetadata.InputMetadata.Visualization.Polarity.ValueType = ...  # 0
                """Default value. This is the same as POSITIVE."""

                POSITIVE: ExplanationMetadata.InputMetadata.Visualization.Polarity.ValueType = ...  # 1
                """Highlights the pixels/outlines that were most influential to the
                model's prediction.
                """

                NEGATIVE: ExplanationMetadata.InputMetadata.Visualization.Polarity.ValueType = ...  # 2
                """Setting polarity to negative highlights areas that does not lead to
                the models's current prediction.
                """

                BOTH: ExplanationMetadata.InputMetadata.Visualization.Polarity.ValueType = ...  # 3
                """Shows both positive and negative attributions."""

            class Polarity(_Polarity, metaclass=_PolarityEnumTypeWrapper):
                """Whether to only highlight pixels with positive contributions, negative
                or both. Defaults to POSITIVE.
                """
                pass

            POLARITY_UNSPECIFIED: ExplanationMetadata.InputMetadata.Visualization.Polarity.ValueType = ...  # 0
            """Default value. This is the same as POSITIVE."""

            POSITIVE: ExplanationMetadata.InputMetadata.Visualization.Polarity.ValueType = ...  # 1
            """Highlights the pixels/outlines that were most influential to the
            model's prediction.
            """

            NEGATIVE: ExplanationMetadata.InputMetadata.Visualization.Polarity.ValueType = ...  # 2
            """Setting polarity to negative highlights areas that does not lead to
            the models's current prediction.
            """

            BOTH: ExplanationMetadata.InputMetadata.Visualization.Polarity.ValueType = ...  # 3
            """Shows both positive and negative attributions."""


            class _ColorMap:
                ValueType = typing.NewType('ValueType', builtins.int)
                V: typing_extensions.TypeAlias = ValueType
            class _ColorMapEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_ColorMap.ValueType], builtins.type):
                DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor = ...
                COLOR_MAP_UNSPECIFIED: ExplanationMetadata.InputMetadata.Visualization.ColorMap.ValueType = ...  # 0
                """Should not be used."""

                PINK_GREEN: ExplanationMetadata.InputMetadata.Visualization.ColorMap.ValueType = ...  # 1
                """Positive: green. Negative: pink."""

                VIRIDIS: ExplanationMetadata.InputMetadata.Visualization.ColorMap.ValueType = ...  # 2
                """Viridis color map: A perceptually uniform color mapping which is
                easier to see by those with colorblindness and progresses from yellow
                to green to blue. Positive: yellow. Negative: blue.
                """

                RED: ExplanationMetadata.InputMetadata.Visualization.ColorMap.ValueType = ...  # 3
                """Positive: red. Negative: red."""

                GREEN: ExplanationMetadata.InputMetadata.Visualization.ColorMap.ValueType = ...  # 4
                """Positive: green. Negative: green."""

                RED_GREEN: ExplanationMetadata.InputMetadata.Visualization.ColorMap.ValueType = ...  # 6
                """Positive: green. Negative: red."""

                PINK_WHITE_GREEN: ExplanationMetadata.InputMetadata.Visualization.ColorMap.ValueType = ...  # 5
                """PiYG palette."""

            class ColorMap(_ColorMap, metaclass=_ColorMapEnumTypeWrapper):
                """The color scheme used for highlighting areas."""
                pass

            COLOR_MAP_UNSPECIFIED: ExplanationMetadata.InputMetadata.Visualization.ColorMap.ValueType = ...  # 0
            """Should not be used."""

            PINK_GREEN: ExplanationMetadata.InputMetadata.Visualization.ColorMap.ValueType = ...  # 1
            """Positive: green. Negative: pink."""

            VIRIDIS: ExplanationMetadata.InputMetadata.Visualization.ColorMap.ValueType = ...  # 2
            """Viridis color map: A perceptually uniform color mapping which is
            easier to see by those with colorblindness and progresses from yellow
            to green to blue. Positive: yellow. Negative: blue.
            """

            RED: ExplanationMetadata.InputMetadata.Visualization.ColorMap.ValueType = ...  # 3
            """Positive: red. Negative: red."""

            GREEN: ExplanationMetadata.InputMetadata.Visualization.ColorMap.ValueType = ...  # 4
            """Positive: green. Negative: green."""

            RED_GREEN: ExplanationMetadata.InputMetadata.Visualization.ColorMap.ValueType = ...  # 6
            """Positive: green. Negative: red."""

            PINK_WHITE_GREEN: ExplanationMetadata.InputMetadata.Visualization.ColorMap.ValueType = ...  # 5
            """PiYG palette."""


            class _OverlayType:
                ValueType = typing.NewType('ValueType', builtins.int)
                V: typing_extensions.TypeAlias = ValueType
            class _OverlayTypeEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_OverlayType.ValueType], builtins.type):
                DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor = ...
                OVERLAY_TYPE_UNSPECIFIED: ExplanationMetadata.InputMetadata.Visualization.OverlayType.ValueType = ...  # 0
                """Default value. This is the same as NONE."""

                NONE: ExplanationMetadata.InputMetadata.Visualization.OverlayType.ValueType = ...  # 1
                """No overlay."""

                ORIGINAL: ExplanationMetadata.InputMetadata.Visualization.OverlayType.ValueType = ...  # 2
                """The attributions are shown on top of the original image."""

                GRAYSCALE: ExplanationMetadata.InputMetadata.Visualization.OverlayType.ValueType = ...  # 3
                """The attributions are shown on top of grayscaled version of the
                original image.
                """

                MASK_BLACK: ExplanationMetadata.InputMetadata.Visualization.OverlayType.ValueType = ...  # 4
                """The attributions are used as a mask to reveal predictive parts of
                the image and hide the un-predictive parts.
                """

            class OverlayType(_OverlayType, metaclass=_OverlayTypeEnumTypeWrapper):
                """How the original image is displayed in the visualization."""
                pass

            OVERLAY_TYPE_UNSPECIFIED: ExplanationMetadata.InputMetadata.Visualization.OverlayType.ValueType = ...  # 0
            """Default value. This is the same as NONE."""

            NONE: ExplanationMetadata.InputMetadata.Visualization.OverlayType.ValueType = ...  # 1
            """No overlay."""

            ORIGINAL: ExplanationMetadata.InputMetadata.Visualization.OverlayType.ValueType = ...  # 2
            """The attributions are shown on top of the original image."""

            GRAYSCALE: ExplanationMetadata.InputMetadata.Visualization.OverlayType.ValueType = ...  # 3
            """The attributions are shown on top of grayscaled version of the
            original image.
            """

            MASK_BLACK: ExplanationMetadata.InputMetadata.Visualization.OverlayType.ValueType = ...  # 4
            """The attributions are used as a mask to reveal predictive parts of
            the image and hide the un-predictive parts.
            """


            TYPE_FIELD_NUMBER: builtins.int
            POLARITY_FIELD_NUMBER: builtins.int
            COLOR_MAP_FIELD_NUMBER: builtins.int
            CLIP_PERCENT_UPPERBOUND_FIELD_NUMBER: builtins.int
            CLIP_PERCENT_LOWERBOUND_FIELD_NUMBER: builtins.int
            OVERLAY_TYPE_FIELD_NUMBER: builtins.int
            type: global___ExplanationMetadata.InputMetadata.Visualization.Type.ValueType = ...
            """Type of the image visualization. Only applicable to
            [Integrated Gradients attribution][google.cloud.aiplatform.v1beta1.ExplanationParameters.integrated_gradients_attribution].
            OUTLINES shows regions of attribution, while PIXELS shows per-pixel
            attribution. Defaults to OUTLINES.
            """

            polarity: global___ExplanationMetadata.InputMetadata.Visualization.Polarity.ValueType = ...
            """Whether to only highlight pixels with positive contributions, negative
            or both. Defaults to POSITIVE.
            """

            color_map: global___ExplanationMetadata.InputMetadata.Visualization.ColorMap.ValueType = ...
            """The color scheme used for the highlighted areas.

            Defaults to PINK_GREEN for
            [Integrated Gradients attribution][google.cloud.aiplatform.v1beta1.ExplanationParameters.integrated_gradients_attribution],
            which shows positive attributions in green and negative in pink.

            Defaults to VIRIDIS for
            [XRAI attribution][google.cloud.aiplatform.v1beta1.ExplanationParameters.xrai_attribution], which
            highlights the most influential regions in yellow and the least
            influential in blue.
            """

            clip_percent_upperbound: builtins.float = ...
            """Excludes attributions above the specified percentile from the
            highlighted areas. Using the clip_percent_upperbound and
            clip_percent_lowerbound together can be useful for filtering out noise
            and making it easier to see areas of strong attribution. Defaults to
            99.9.
            """

            clip_percent_lowerbound: builtins.float = ...
            """Excludes attributions below the specified percentile, from the
            highlighted areas. Defaults to 62.
            """

            overlay_type: global___ExplanationMetadata.InputMetadata.Visualization.OverlayType.ValueType = ...
            """How the original image is displayed in the visualization.
            Adjusting the overlay can help increase visual clarity if the original
            image makes it difficult to view the visualization. Defaults to NONE.
            """

            def __init__(self,
                *,
                type : global___ExplanationMetadata.InputMetadata.Visualization.Type.ValueType = ...,
                polarity : global___ExplanationMetadata.InputMetadata.Visualization.Polarity.ValueType = ...,
                color_map : global___ExplanationMetadata.InputMetadata.Visualization.ColorMap.ValueType = ...,
                clip_percent_upperbound : builtins.float = ...,
                clip_percent_lowerbound : builtins.float = ...,
                overlay_type : global___ExplanationMetadata.InputMetadata.Visualization.OverlayType.ValueType = ...,
                ) -> None: ...
            def ClearField(self, field_name: typing_extensions.Literal["clip_percent_lowerbound",b"clip_percent_lowerbound","clip_percent_upperbound",b"clip_percent_upperbound","color_map",b"color_map","overlay_type",b"overlay_type","polarity",b"polarity","type",b"type"]) -> None: ...

        INPUT_BASELINES_FIELD_NUMBER: builtins.int
        INPUT_TENSOR_NAME_FIELD_NUMBER: builtins.int
        ENCODING_FIELD_NUMBER: builtins.int
        MODALITY_FIELD_NUMBER: builtins.int
        FEATURE_VALUE_DOMAIN_FIELD_NUMBER: builtins.int
        INDICES_TENSOR_NAME_FIELD_NUMBER: builtins.int
        DENSE_SHAPE_TENSOR_NAME_FIELD_NUMBER: builtins.int
        INDEX_FEATURE_MAPPING_FIELD_NUMBER: builtins.int
        ENCODED_TENSOR_NAME_FIELD_NUMBER: builtins.int
        ENCODED_BASELINES_FIELD_NUMBER: builtins.int
        VISUALIZATION_FIELD_NUMBER: builtins.int
        GROUP_NAME_FIELD_NUMBER: builtins.int
        @property
        def input_baselines(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[google.protobuf.struct_pb2.Value]:
            """Baseline inputs for this feature.

            If no baseline is specified, Vertex AI chooses the baseline for this
            feature. If multiple baselines are specified, Vertex AI returns the
            average attributions across them in [Attribution.feature_attributions][google.cloud.aiplatform.v1beta1.Attribution.feature_attributions].

            For Vertex AI-provided Tensorflow images (both 1.x and 2.x), the shape
            of each baseline must match the shape of the input tensor. If a scalar is
            provided, we broadcast to the same shape as the input tensor.

            For custom images, the element of the baselines must be in the same
            format as the feature's input in the
            [instance][google.cloud.aiplatform.v1beta1.ExplainRequest.instances][]. The schema of any single instance
            may be specified via Endpoint's DeployedModels'
            [Model's][google.cloud.aiplatform.v1beta1.DeployedModel.model]
            [PredictSchemata's][google.cloud.aiplatform.v1beta1.Model.predict_schemata]
            [instance_schema_uri][google.cloud.aiplatform.v1beta1.PredictSchemata.instance_schema_uri].
            """
            pass
        input_tensor_name: typing.Text = ...
        """Name of the input tensor for this feature. Required and is only
        applicable to Vertex AI-provided images for Tensorflow.
        """

        encoding: global___ExplanationMetadata.InputMetadata.Encoding.ValueType = ...
        """Defines how the feature is encoded into the input tensor. Defaults to
        IDENTITY.
        """

        modality: typing.Text = ...
        """Modality of the feature. Valid values are: numeric, image. Defaults to
        numeric.
        """

        @property
        def feature_value_domain(self) -> global___ExplanationMetadata.InputMetadata.FeatureValueDomain:
            """The domain details of the input feature value. Like min/max, original
            mean or standard deviation if normalized.
            """
            pass
        indices_tensor_name: typing.Text = ...
        """Specifies the index of the values of the input tensor.
        Required when the input tensor is a sparse representation. Refer to
        Tensorflow documentation for more details:
        https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor.
        """

        dense_shape_tensor_name: typing.Text = ...
        """Specifies the shape of the values of the input if the input is a sparse
        representation. Refer to Tensorflow documentation for more details:
        https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor.
        """

        @property
        def index_feature_mapping(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[typing.Text]:
            """A list of feature names for each index in the input tensor.
            Required when the input [InputMetadata.encoding][google.cloud.aiplatform.v1beta1.ExplanationMetadata.InputMetadata.encoding] is BAG_OF_FEATURES,
            BAG_OF_FEATURES_SPARSE, INDICATOR.
            """
            pass
        encoded_tensor_name: typing.Text = ...
        """Encoded tensor is a transformation of the input tensor. Must be provided
        if choosing
        [Integrated Gradients attribution][google.cloud.aiplatform.v1beta1.ExplanationParameters.integrated_gradients_attribution]
        or [XRAI attribution][google.cloud.aiplatform.v1beta1.ExplanationParameters.xrai_attribution] and the
        input tensor is not differentiable.

        An encoded tensor is generated if the input tensor is encoded by a lookup
        table.
        """

        @property
        def encoded_baselines(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[google.protobuf.struct_pb2.Value]:
            """A list of baselines for the encoded tensor.

            The shape of each baseline should match the shape of the encoded tensor.
            If a scalar is provided, Vertex AI broadcasts to the same shape as the
            encoded tensor.
            """
            pass
        @property
        def visualization(self) -> global___ExplanationMetadata.InputMetadata.Visualization:
            """Visualization configurations for image explanation."""
            pass
        group_name: typing.Text = ...
        """Name of the group that the input belongs to. Features with the same group
        name will be treated as one feature when computing attributions. Features
        grouped together can have different shapes in value. If provided, there
        will be one single attribution generated in
        [Attribution.feature_attributions][google.cloud.aiplatform.v1beta1.Attribution.feature_attributions], keyed by the group name.
        """

        def __init__(self,
            *,
            input_baselines : typing.Optional[typing.Iterable[google.protobuf.struct_pb2.Value]] = ...,
            input_tensor_name : typing.Text = ...,
            encoding : global___ExplanationMetadata.InputMetadata.Encoding.ValueType = ...,
            modality : typing.Text = ...,
            feature_value_domain : typing.Optional[global___ExplanationMetadata.InputMetadata.FeatureValueDomain] = ...,
            indices_tensor_name : typing.Text = ...,
            dense_shape_tensor_name : typing.Text = ...,
            index_feature_mapping : typing.Optional[typing.Iterable[typing.Text]] = ...,
            encoded_tensor_name : typing.Text = ...,
            encoded_baselines : typing.Optional[typing.Iterable[google.protobuf.struct_pb2.Value]] = ...,
            visualization : typing.Optional[global___ExplanationMetadata.InputMetadata.Visualization] = ...,
            group_name : typing.Text = ...,
            ) -> None: ...
        def HasField(self, field_name: typing_extensions.Literal["feature_value_domain",b"feature_value_domain","visualization",b"visualization"]) -> builtins.bool: ...
        def ClearField(self, field_name: typing_extensions.Literal["dense_shape_tensor_name",b"dense_shape_tensor_name","encoded_baselines",b"encoded_baselines","encoded_tensor_name",b"encoded_tensor_name","encoding",b"encoding","feature_value_domain",b"feature_value_domain","group_name",b"group_name","index_feature_mapping",b"index_feature_mapping","indices_tensor_name",b"indices_tensor_name","input_baselines",b"input_baselines","input_tensor_name",b"input_tensor_name","modality",b"modality","visualization",b"visualization"]) -> None: ...

    class OutputMetadata(google.protobuf.message.Message):
        """Metadata of the prediction output to be explained."""
        DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
        INDEX_DISPLAY_NAME_MAPPING_FIELD_NUMBER: builtins.int
        DISPLAY_NAME_MAPPING_KEY_FIELD_NUMBER: builtins.int
        OUTPUT_TENSOR_NAME_FIELD_NUMBER: builtins.int
        @property
        def index_display_name_mapping(self) -> google.protobuf.struct_pb2.Value:
            """Static mapping between the index and display name.

            Use this if the outputs are a deterministic n-dimensional array, e.g. a
            list of scores of all the classes in a pre-defined order for a
            multi-classification Model. It's not feasible if the outputs are
            non-deterministic, e.g. the Model produces top-k classes or sort the
            outputs by their values.

            The shape of the value must be an n-dimensional array of strings. The
            number of dimensions must match that of the outputs to be explained.
            The [Attribution.output_display_name][google.cloud.aiplatform.v1beta1.Attribution.output_display_name] is populated by locating in the
            mapping with [Attribution.output_index][google.cloud.aiplatform.v1beta1.Attribution.output_index].
            """
            pass
        display_name_mapping_key: typing.Text = ...
        """Specify a field name in the prediction to look for the display name.

        Use this if the prediction contains the display names for the outputs.

        The display names in the prediction must have the same shape of the
        outputs, so that it can be located by [Attribution.output_index][google.cloud.aiplatform.v1beta1.Attribution.output_index] for
        a specific output.
        """

        output_tensor_name: typing.Text = ...
        """Name of the output tensor. Required and is only applicable to Vertex
        AI provided images for Tensorflow.
        """

        def __init__(self,
            *,
            index_display_name_mapping : typing.Optional[google.protobuf.struct_pb2.Value] = ...,
            display_name_mapping_key : typing.Text = ...,
            output_tensor_name : typing.Text = ...,
            ) -> None: ...
        def HasField(self, field_name: typing_extensions.Literal["display_name_mapping",b"display_name_mapping","display_name_mapping_key",b"display_name_mapping_key","index_display_name_mapping",b"index_display_name_mapping"]) -> builtins.bool: ...
        def ClearField(self, field_name: typing_extensions.Literal["display_name_mapping",b"display_name_mapping","display_name_mapping_key",b"display_name_mapping_key","index_display_name_mapping",b"index_display_name_mapping","output_tensor_name",b"output_tensor_name"]) -> None: ...
        def WhichOneof(self, oneof_group: typing_extensions.Literal["display_name_mapping",b"display_name_mapping"]) -> typing.Optional[typing_extensions.Literal["index_display_name_mapping","display_name_mapping_key"]]: ...

    class InputsEntry(google.protobuf.message.Message):
        DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
        KEY_FIELD_NUMBER: builtins.int
        VALUE_FIELD_NUMBER: builtins.int
        key: typing.Text = ...
        @property
        def value(self) -> global___ExplanationMetadata.InputMetadata: ...
        def __init__(self,
            *,
            key : typing.Text = ...,
            value : typing.Optional[global___ExplanationMetadata.InputMetadata] = ...,
            ) -> None: ...
        def HasField(self, field_name: typing_extensions.Literal["value",b"value"]) -> builtins.bool: ...
        def ClearField(self, field_name: typing_extensions.Literal["key",b"key","value",b"value"]) -> None: ...

    class OutputsEntry(google.protobuf.message.Message):
        DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
        KEY_FIELD_NUMBER: builtins.int
        VALUE_FIELD_NUMBER: builtins.int
        key: typing.Text = ...
        @property
        def value(self) -> global___ExplanationMetadata.OutputMetadata: ...
        def __init__(self,
            *,
            key : typing.Text = ...,
            value : typing.Optional[global___ExplanationMetadata.OutputMetadata] = ...,
            ) -> None: ...
        def HasField(self, field_name: typing_extensions.Literal["value",b"value"]) -> builtins.bool: ...
        def ClearField(self, field_name: typing_extensions.Literal["key",b"key","value",b"value"]) -> None: ...

    INPUTS_FIELD_NUMBER: builtins.int
    OUTPUTS_FIELD_NUMBER: builtins.int
    FEATURE_ATTRIBUTIONS_SCHEMA_URI_FIELD_NUMBER: builtins.int
    @property
    def inputs(self) -> google.protobuf.internal.containers.MessageMap[typing.Text, global___ExplanationMetadata.InputMetadata]:
        """Required. Map from feature names to feature input metadata. Keys are the name of the
        features. Values are the specification of the feature.

        An empty InputMetadata is valid. It describes a text feature which has the
        name specified as the key in [ExplanationMetadata.inputs][google.cloud.aiplatform.v1beta1.ExplanationMetadata.inputs]. The baseline
        of the empty feature is chosen by Vertex AI.

        For Vertex AI-provided Tensorflow images, the key can be any friendly
        name of the feature. Once specified,
        [featureAttributions][google.cloud.aiplatform.v1beta1.Attribution.feature_attributions] are keyed by
        this key (if not grouped with another feature).

        For custom images, the key must match with the key in
        [instance][google.cloud.aiplatform.v1beta1.ExplainRequest.instances].
        """
        pass
    @property
    def outputs(self) -> google.protobuf.internal.containers.MessageMap[typing.Text, global___ExplanationMetadata.OutputMetadata]:
        """Required. Map from output names to output metadata.

        For Vertex AI-provided Tensorflow images, keys can be any user defined
        string that consists of any UTF-8 characters.

        For custom images, keys are the name of the output field in the prediction
        to be explained.

        Currently only one key is allowed.
        """
        pass
    feature_attributions_schema_uri: typing.Text = ...
    """Points to a YAML file stored on Google Cloud Storage describing the format
    of the [feature attributions][google.cloud.aiplatform.v1beta1.Attribution.feature_attributions].
    The schema is defined as an OpenAPI 3.0.2 [Schema
    Object](https://github.com/OAI/OpenAPI-Specification/blob/main/versions/3.0.2.md#schemaObject).
    AutoML tabular Models always have this field populated by Vertex AI.
    Note: The URI given on output may be different, including the URI scheme,
    than the one given on input. The output URI will point to a location where
    the user only has a read access.
    """

    def __init__(self,
        *,
        inputs : typing.Optional[typing.Mapping[typing.Text, global___ExplanationMetadata.InputMetadata]] = ...,
        outputs : typing.Optional[typing.Mapping[typing.Text, global___ExplanationMetadata.OutputMetadata]] = ...,
        feature_attributions_schema_uri : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["feature_attributions_schema_uri",b"feature_attributions_schema_uri","inputs",b"inputs","outputs",b"outputs"]) -> None: ...
global___ExplanationMetadata = ExplanationMetadata
