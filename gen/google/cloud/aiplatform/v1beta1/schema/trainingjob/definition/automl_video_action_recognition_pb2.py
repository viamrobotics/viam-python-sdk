# -*- coding: utf-8 -*-
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# source: google/cloud/aiplatform/v1beta1/schema/trainingjob/definition/automl_video_action_recognition.proto
"""Generated protocol buffer code."""
from google.protobuf import descriptor as _descriptor
from google.protobuf import descriptor_pool as _descriptor_pool
from google.protobuf import message as _message
from google.protobuf import reflection as _reflection
from google.protobuf import symbol_database as _symbol_database
# @@protoc_insertion_point(imports)

_sym_db = _symbol_database.Default()


from google.api import annotations_pb2 as google_dot_api_dot_annotations__pb2


DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\ncgoogle/cloud/aiplatform/v1beta1/schema/trainingjob/definition/automl_video_action_recognition.proto\x12=google.cloud.aiplatform.v1beta1.schema.trainingjob.definition\x1a\x1cgoogle/api/annotations.proto\"\x99\x01\n\x1c\x41utoMlVideoActionRecognition\x12y\n\x06inputs\x18\x01 \x01(\x0b\x32\x61.google.cloud.aiplatform.v1beta1.schema.trainingjob.definition.AutoMlVideoActionRecognitionInputsR\x06inputs\"\xbb\x02\n\"AutoMlVideoActionRecognitionInputs\x12\x8a\x01\n\nmodel_type\x18\x01 \x01(\x0e\x32k.google.cloud.aiplatform.v1beta1.schema.trainingjob.definition.AutoMlVideoActionRecognitionInputs.ModelTypeR\tmodelType\"\x87\x01\n\tModelType\x12\x1a\n\x16MODEL_TYPE_UNSPECIFIED\x10\x00\x12\t\n\x05\x43LOUD\x10\x01\x12\x16\n\x12MOBILE_VERSATILE_1\x10\x02\x12\x1d\n\x19MOBILE_JETSON_VERSATILE_1\x10\x03\x12\x1c\n\x18MOBILE_CORAL_VERSATILE_1\x10\x04\x42\xd1\x01\nAcom.google.cloud.aiplatform.v1beta1.schema.trainingjob.definitionB!AutoMLVideoActionRecognitionProtoP\x01Zggoogle.golang.org/genproto/googleapis/cloud/aiplatform/v1beta1/schema/trainingjob/definition;definitionb\x06proto3')



_AUTOMLVIDEOACTIONRECOGNITION = DESCRIPTOR.message_types_by_name['AutoMlVideoActionRecognition']
_AUTOMLVIDEOACTIONRECOGNITIONINPUTS = DESCRIPTOR.message_types_by_name['AutoMlVideoActionRecognitionInputs']
_AUTOMLVIDEOACTIONRECOGNITIONINPUTS_MODELTYPE = _AUTOMLVIDEOACTIONRECOGNITIONINPUTS.enum_types_by_name['ModelType']
AutoMlVideoActionRecognition = _reflection.GeneratedProtocolMessageType('AutoMlVideoActionRecognition', (_message.Message,), {
  'DESCRIPTOR' : _AUTOMLVIDEOACTIONRECOGNITION,
  '__module__' : 'google.cloud.aiplatform.v1beta1.schema.trainingjob.definition.automl_video_action_recognition_pb2'
  # @@protoc_insertion_point(class_scope:google.cloud.aiplatform.v1beta1.schema.trainingjob.definition.AutoMlVideoActionRecognition)
  })
_sym_db.RegisterMessage(AutoMlVideoActionRecognition)

AutoMlVideoActionRecognitionInputs = _reflection.GeneratedProtocolMessageType('AutoMlVideoActionRecognitionInputs', (_message.Message,), {
  'DESCRIPTOR' : _AUTOMLVIDEOACTIONRECOGNITIONINPUTS,
  '__module__' : 'google.cloud.aiplatform.v1beta1.schema.trainingjob.definition.automl_video_action_recognition_pb2'
  # @@protoc_insertion_point(class_scope:google.cloud.aiplatform.v1beta1.schema.trainingjob.definition.AutoMlVideoActionRecognitionInputs)
  })
_sym_db.RegisterMessage(AutoMlVideoActionRecognitionInputs)

if _descriptor._USE_C_DESCRIPTORS == False:

  DESCRIPTOR._options = None
  DESCRIPTOR._serialized_options = b'\nAcom.google.cloud.aiplatform.v1beta1.schema.trainingjob.definitionB!AutoMLVideoActionRecognitionProtoP\001Zggoogle.golang.org/genproto/googleapis/cloud/aiplatform/v1beta1/schema/trainingjob/definition;definition'
  _AUTOMLVIDEOACTIONRECOGNITION._serialized_start=197
  _AUTOMLVIDEOACTIONRECOGNITION._serialized_end=350
  _AUTOMLVIDEOACTIONRECOGNITIONINPUTS._serialized_start=353
  _AUTOMLVIDEOACTIONRECOGNITIONINPUTS._serialized_end=668
  _AUTOMLVIDEOACTIONRECOGNITIONINPUTS_MODELTYPE._serialized_start=533
  _AUTOMLVIDEOACTIONRECOGNITIONINPUTS_MODELTYPE._serialized_end=668
# @@protoc_insertion_point(module_scope)
