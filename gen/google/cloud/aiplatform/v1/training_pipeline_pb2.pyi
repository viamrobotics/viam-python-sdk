"""
@generated by mypy-protobuf.  Do not edit manually!
isort:skip_file
"""
import builtins
import google.cloud.aiplatform.v1.encryption_spec_pb2
import google.cloud.aiplatform.v1.io_pb2
import google.cloud.aiplatform.v1.model_pb2
import google.cloud.aiplatform.v1.pipeline_state_pb2
import google.protobuf.descriptor
import google.protobuf.internal.containers
import google.protobuf.message
import google.protobuf.struct_pb2
import google.protobuf.timestamp_pb2
import google.rpc.status_pb2
import typing
import typing_extensions

DESCRIPTOR: google.protobuf.descriptor.FileDescriptor = ...

class TrainingPipeline(google.protobuf.message.Message):
    """The TrainingPipeline orchestrates tasks associated with training a Model. It
    always executes the training task, and optionally may also
    export data from Vertex AI's Dataset which becomes the training input,
    [upload][google.cloud.aiplatform.v1.ModelService.UploadModel] the Model to Vertex AI, and evaluate the
    Model.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    class LabelsEntry(google.protobuf.message.Message):
        DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
        KEY_FIELD_NUMBER: builtins.int
        VALUE_FIELD_NUMBER: builtins.int
        key: typing.Text = ...
        value: typing.Text = ...
        def __init__(self,
            *,
            key : typing.Text = ...,
            value : typing.Text = ...,
            ) -> None: ...
        def ClearField(self, field_name: typing_extensions.Literal["key",b"key","value",b"value"]) -> None: ...

    NAME_FIELD_NUMBER: builtins.int
    DISPLAY_NAME_FIELD_NUMBER: builtins.int
    INPUT_DATA_CONFIG_FIELD_NUMBER: builtins.int
    TRAINING_TASK_DEFINITION_FIELD_NUMBER: builtins.int
    TRAINING_TASK_INPUTS_FIELD_NUMBER: builtins.int
    TRAINING_TASK_METADATA_FIELD_NUMBER: builtins.int
    MODEL_TO_UPLOAD_FIELD_NUMBER: builtins.int
    STATE_FIELD_NUMBER: builtins.int
    ERROR_FIELD_NUMBER: builtins.int
    CREATE_TIME_FIELD_NUMBER: builtins.int
    START_TIME_FIELD_NUMBER: builtins.int
    END_TIME_FIELD_NUMBER: builtins.int
    UPDATE_TIME_FIELD_NUMBER: builtins.int
    LABELS_FIELD_NUMBER: builtins.int
    ENCRYPTION_SPEC_FIELD_NUMBER: builtins.int
    name: typing.Text = ...
    """Output only. Resource name of the TrainingPipeline."""

    display_name: typing.Text = ...
    """Required. The user-defined name of this TrainingPipeline."""

    @property
    def input_data_config(self) -> global___InputDataConfig:
        """Specifies Vertex AI owned input data that may be used for training the
        Model. The TrainingPipeline's [training_task_definition][google.cloud.aiplatform.v1.TrainingPipeline.training_task_definition] should make
        clear whether this config is used and if there are any special requirements
        on how it should be filled. If nothing about this config is mentioned in
        the [training_task_definition][google.cloud.aiplatform.v1.TrainingPipeline.training_task_definition], then it should be assumed that the
        TrainingPipeline does not depend on this configuration.
        """
        pass
    training_task_definition: typing.Text = ...
    """Required. A Google Cloud Storage path to the YAML file that defines the training task
    which is responsible for producing the model artifact, and may also include
    additional auxiliary work.
    The definition files that can be used here are found in
    gs://google-cloud-aiplatform/schema/trainingjob/definition/.
    Note: The URI given on output will be immutable and probably different,
    including the URI scheme, than the one given on input. The output URI will
    point to a location where the user only has a read access.
    """

    @property
    def training_task_inputs(self) -> google.protobuf.struct_pb2.Value:
        """Required. The training task's parameter(s), as specified in the
        [training_task_definition][google.cloud.aiplatform.v1.TrainingPipeline.training_task_definition]'s `inputs`.
        """
        pass
    @property
    def training_task_metadata(self) -> google.protobuf.struct_pb2.Value:
        """Output only. The metadata information as specified in the [training_task_definition][google.cloud.aiplatform.v1.TrainingPipeline.training_task_definition]'s
        `metadata`. This metadata is an auxiliary runtime and final information
        about the training task. While the pipeline is running this information is
        populated only at a best effort basis. Only present if the
        pipeline's [training_task_definition][google.cloud.aiplatform.v1.TrainingPipeline.training_task_definition] contains `metadata` object.
        """
        pass
    @property
    def model_to_upload(self) -> google.cloud.aiplatform.v1.model_pb2.Model:
        """Describes the Model that may be uploaded (via [ModelService.UploadModel][google.cloud.aiplatform.v1.ModelService.UploadModel])
        by this TrainingPipeline. The TrainingPipeline's
        [training_task_definition][google.cloud.aiplatform.v1.TrainingPipeline.training_task_definition] should make clear whether this Model
        description should be populated, and if there are any special requirements
        regarding how it should be filled. If nothing is mentioned in the
        [training_task_definition][google.cloud.aiplatform.v1.TrainingPipeline.training_task_definition], then it should be assumed that this field
        should not be filled and the training task either uploads the Model without
        a need of this information, or that training task does not support
        uploading a Model as part of the pipeline.
        When the Pipeline's state becomes `PIPELINE_STATE_SUCCEEDED` and
        the trained Model had been uploaded into Vertex AI, then the
        model_to_upload's resource [name][google.cloud.aiplatform.v1.Model.name] is populated. The Model
        is always uploaded into the Project and Location in which this pipeline
        is.
        """
        pass
    state: google.cloud.aiplatform.v1.pipeline_state_pb2.PipelineState.ValueType = ...
    """Output only. The detailed state of the pipeline."""

    @property
    def error(self) -> google.rpc.status_pb2.Status:
        """Output only. Only populated when the pipeline's state is `PIPELINE_STATE_FAILED` or
        `PIPELINE_STATE_CANCELLED`.
        """
        pass
    @property
    def create_time(self) -> google.protobuf.timestamp_pb2.Timestamp:
        """Output only. Time when the TrainingPipeline was created."""
        pass
    @property
    def start_time(self) -> google.protobuf.timestamp_pb2.Timestamp:
        """Output only. Time when the TrainingPipeline for the first time entered the
        `PIPELINE_STATE_RUNNING` state.
        """
        pass
    @property
    def end_time(self) -> google.protobuf.timestamp_pb2.Timestamp:
        """Output only. Time when the TrainingPipeline entered any of the following states:
        `PIPELINE_STATE_SUCCEEDED`, `PIPELINE_STATE_FAILED`,
        `PIPELINE_STATE_CANCELLED`.
        """
        pass
    @property
    def update_time(self) -> google.protobuf.timestamp_pb2.Timestamp:
        """Output only. Time when the TrainingPipeline was most recently updated."""
        pass
    @property
    def labels(self) -> google.protobuf.internal.containers.ScalarMap[typing.Text, typing.Text]:
        """The labels with user-defined metadata to organize TrainingPipelines.

        Label keys and values can be no longer than 64 characters
        (Unicode codepoints), can only contain lowercase letters, numeric
        characters, underscores and dashes. International characters are allowed.

        See https://goo.gl/xmQnxf for more information and examples of labels.
        """
        pass
    @property
    def encryption_spec(self) -> google.cloud.aiplatform.v1.encryption_spec_pb2.EncryptionSpec:
        """Customer-managed encryption key spec for a TrainingPipeline. If set, this
        TrainingPipeline will be secured by this key.

        Note: Model trained by this TrainingPipeline is also secured by this key if
        [model_to_upload][google.cloud.aiplatform.v1.TrainingPipeline.encryption_spec] is not set separately.
        """
        pass
    def __init__(self,
        *,
        name : typing.Text = ...,
        display_name : typing.Text = ...,
        input_data_config : typing.Optional[global___InputDataConfig] = ...,
        training_task_definition : typing.Text = ...,
        training_task_inputs : typing.Optional[google.protobuf.struct_pb2.Value] = ...,
        training_task_metadata : typing.Optional[google.protobuf.struct_pb2.Value] = ...,
        model_to_upload : typing.Optional[google.cloud.aiplatform.v1.model_pb2.Model] = ...,
        state : google.cloud.aiplatform.v1.pipeline_state_pb2.PipelineState.ValueType = ...,
        error : typing.Optional[google.rpc.status_pb2.Status] = ...,
        create_time : typing.Optional[google.protobuf.timestamp_pb2.Timestamp] = ...,
        start_time : typing.Optional[google.protobuf.timestamp_pb2.Timestamp] = ...,
        end_time : typing.Optional[google.protobuf.timestamp_pb2.Timestamp] = ...,
        update_time : typing.Optional[google.protobuf.timestamp_pb2.Timestamp] = ...,
        labels : typing.Optional[typing.Mapping[typing.Text, typing.Text]] = ...,
        encryption_spec : typing.Optional[google.cloud.aiplatform.v1.encryption_spec_pb2.EncryptionSpec] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["create_time",b"create_time","encryption_spec",b"encryption_spec","end_time",b"end_time","error",b"error","input_data_config",b"input_data_config","model_to_upload",b"model_to_upload","start_time",b"start_time","training_task_inputs",b"training_task_inputs","training_task_metadata",b"training_task_metadata","update_time",b"update_time"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["create_time",b"create_time","display_name",b"display_name","encryption_spec",b"encryption_spec","end_time",b"end_time","error",b"error","input_data_config",b"input_data_config","labels",b"labels","model_to_upload",b"model_to_upload","name",b"name","start_time",b"start_time","state",b"state","training_task_definition",b"training_task_definition","training_task_inputs",b"training_task_inputs","training_task_metadata",b"training_task_metadata","update_time",b"update_time"]) -> None: ...
global___TrainingPipeline = TrainingPipeline

class InputDataConfig(google.protobuf.message.Message):
    """Specifies Vertex AI owned input data to be used for training, and
    possibly evaluating, the Model.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    FRACTION_SPLIT_FIELD_NUMBER: builtins.int
    FILTER_SPLIT_FIELD_NUMBER: builtins.int
    PREDEFINED_SPLIT_FIELD_NUMBER: builtins.int
    TIMESTAMP_SPLIT_FIELD_NUMBER: builtins.int
    STRATIFIED_SPLIT_FIELD_NUMBER: builtins.int
    GCS_DESTINATION_FIELD_NUMBER: builtins.int
    BIGQUERY_DESTINATION_FIELD_NUMBER: builtins.int
    DATASET_ID_FIELD_NUMBER: builtins.int
    ANNOTATIONS_FILTER_FIELD_NUMBER: builtins.int
    ANNOTATION_SCHEMA_URI_FIELD_NUMBER: builtins.int
    @property
    def fraction_split(self) -> global___FractionSplit:
        """Split based on fractions defining the size of each set."""
        pass
    @property
    def filter_split(self) -> global___FilterSplit:
        """Split based on the provided filters for each set."""
        pass
    @property
    def predefined_split(self) -> global___PredefinedSplit:
        """Supported only for tabular Datasets.

        Split based on a predefined key.
        """
        pass
    @property
    def timestamp_split(self) -> global___TimestampSplit:
        """Supported only for tabular Datasets.

        Split based on the timestamp of the input data pieces.
        """
        pass
    @property
    def stratified_split(self) -> global___StratifiedSplit:
        """Supported only for tabular Datasets.

        Split based on the distribution of the specified column.
        """
        pass
    @property
    def gcs_destination(self) -> google.cloud.aiplatform.v1.io_pb2.GcsDestination:
        """The Cloud Storage location where the training data is to be
        written to. In the given directory a new directory is created with
        name:
        `dataset-<dataset-id>-<annotation-type>-<timestamp-of-training-call>`
        where timestamp is in YYYY-MM-DDThh:mm:ss.sssZ ISO-8601 format.
        All training input data is written into that directory.

        The Vertex AI environment variables representing Cloud Storage
        data URIs are represented in the Cloud Storage wildcard
        format to support sharded data. e.g.: "gs://.../training-*.jsonl"

        * AIP_DATA_FORMAT = "jsonl" for non-tabular data, "csv" for tabular data
        * AIP_TRAINING_DATA_URI =
        "gcs_destination/dataset-<dataset-id>-<annotation-type>-<time>/training-*.${AIP_DATA_FORMAT}"

        * AIP_VALIDATION_DATA_URI =
        "gcs_destination/dataset-<dataset-id>-<annotation-type>-<time>/validation-*.${AIP_DATA_FORMAT}"

        * AIP_TEST_DATA_URI =
        "gcs_destination/dataset-<dataset-id>-<annotation-type>-<time>/test-*.${AIP_DATA_FORMAT}"
        """
        pass
    @property
    def bigquery_destination(self) -> google.cloud.aiplatform.v1.io_pb2.BigQueryDestination:
        """Only applicable to custom training with tabular Dataset with BigQuery
        source.

        The BigQuery project location where the training data is to be written
        to. In the given project a new dataset is created with name
        `dataset_<dataset-id>_<annotation-type>_<timestamp-of-training-call>`
        where timestamp is in YYYY_MM_DDThh_mm_ss_sssZ format. All training
        input data is written into that dataset. In the dataset three
        tables are created, `training`, `validation` and `test`.

        * AIP_DATA_FORMAT = "bigquery".
        * AIP_TRAINING_DATA_URI  =
        "bigquery_destination.dataset_<dataset-id>_<annotation-type>_<time>.training"

        * AIP_VALIDATION_DATA_URI =
        "bigquery_destination.dataset_<dataset-id>_<annotation-type>_<time>.validation"

        * AIP_TEST_DATA_URI =
        "bigquery_destination.dataset_<dataset-id>_<annotation-type>_<time>.test"
        """
        pass
    dataset_id: typing.Text = ...
    """Required. The ID of the Dataset in the same Project and Location which data will be
    used to train the Model. The Dataset must use schema compatible with
    Model being trained, and what is compatible should be described in the
    used TrainingPipeline's [training_task_definition]
    [google.cloud.aiplatform.v1.TrainingPipeline.training_task_definition].
    For tabular Datasets, all their data is exported to training, to pick
    and choose from.
    """

    annotations_filter: typing.Text = ...
    """Applicable only to Datasets that have DataItems and Annotations.

    A filter on Annotations of the Dataset. Only Annotations that both
    match this filter and belong to DataItems not ignored by the split method
    are used in respectively training, validation or test role, depending on
    the role of the DataItem they are on (for the auto-assigned that role is
    decided by Vertex AI). A filter with same syntax as the one used in
    [ListAnnotations][google.cloud.aiplatform.v1.DatasetService.ListAnnotations] may be used, but note
    here it filters across all Annotations of the Dataset, and not just within
    a single DataItem.
    """

    annotation_schema_uri: typing.Text = ...
    """Applicable only to custom training with Datasets that have DataItems and
    Annotations.

    Cloud Storage URI that points to a YAML file describing the annotation
    schema. The schema is defined as an OpenAPI 3.0.2 [Schema
    Object](https://github.com/OAI/OpenAPI-Specification/blob/main/versions/3.0.2.md#schemaObject).
    The schema files that can be used here are found in
    gs://google-cloud-aiplatform/schema/dataset/annotation/ , note that the
    chosen schema must be consistent with
    [metadata][google.cloud.aiplatform.v1.Dataset.metadata_schema_uri] of the Dataset specified by
    [dataset_id][google.cloud.aiplatform.v1.InputDataConfig.dataset_id].

    Only Annotations that both match this schema and belong to DataItems not
    ignored by the split method are used in respectively training, validation
    or test role, depending on the role of the DataItem they are on.

    When used in conjunction with [annotations_filter][google.cloud.aiplatform.v1.InputDataConfig.annotations_filter], the Annotations used
    for training are filtered by both [annotations_filter][google.cloud.aiplatform.v1.InputDataConfig.annotations_filter] and
    [annotation_schema_uri][google.cloud.aiplatform.v1.InputDataConfig.annotation_schema_uri].
    """

    def __init__(self,
        *,
        fraction_split : typing.Optional[global___FractionSplit] = ...,
        filter_split : typing.Optional[global___FilterSplit] = ...,
        predefined_split : typing.Optional[global___PredefinedSplit] = ...,
        timestamp_split : typing.Optional[global___TimestampSplit] = ...,
        stratified_split : typing.Optional[global___StratifiedSplit] = ...,
        gcs_destination : typing.Optional[google.cloud.aiplatform.v1.io_pb2.GcsDestination] = ...,
        bigquery_destination : typing.Optional[google.cloud.aiplatform.v1.io_pb2.BigQueryDestination] = ...,
        dataset_id : typing.Text = ...,
        annotations_filter : typing.Text = ...,
        annotation_schema_uri : typing.Text = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["bigquery_destination",b"bigquery_destination","destination",b"destination","filter_split",b"filter_split","fraction_split",b"fraction_split","gcs_destination",b"gcs_destination","predefined_split",b"predefined_split","split",b"split","stratified_split",b"stratified_split","timestamp_split",b"timestamp_split"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["annotation_schema_uri",b"annotation_schema_uri","annotations_filter",b"annotations_filter","bigquery_destination",b"bigquery_destination","dataset_id",b"dataset_id","destination",b"destination","filter_split",b"filter_split","fraction_split",b"fraction_split","gcs_destination",b"gcs_destination","predefined_split",b"predefined_split","split",b"split","stratified_split",b"stratified_split","timestamp_split",b"timestamp_split"]) -> None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing_extensions.Literal["destination",b"destination"]) -> typing.Optional[typing_extensions.Literal["gcs_destination","bigquery_destination"]]: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing_extensions.Literal["split",b"split"]) -> typing.Optional[typing_extensions.Literal["fraction_split","filter_split","predefined_split","timestamp_split","stratified_split"]]: ...
global___InputDataConfig = InputDataConfig

class FractionSplit(google.protobuf.message.Message):
    """Assigns the input data to training, validation, and test sets as per the
    given fractions. Any of `training_fraction`, `validation_fraction` and
    `test_fraction` may optionally be provided, they must sum to up to 1. If the
    provided ones sum to less than 1, the remainder is assigned to sets as
    decided by Vertex AI. If none of the fractions are set, by default roughly
    80% of data is used for training, 10% for validation, and 10% for test.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    TRAINING_FRACTION_FIELD_NUMBER: builtins.int
    VALIDATION_FRACTION_FIELD_NUMBER: builtins.int
    TEST_FRACTION_FIELD_NUMBER: builtins.int
    training_fraction: builtins.float = ...
    """The fraction of the input data that is to be used to train the Model."""

    validation_fraction: builtins.float = ...
    """The fraction of the input data that is to be used to validate the Model."""

    test_fraction: builtins.float = ...
    """The fraction of the input data that is to be used to evaluate the Model."""

    def __init__(self,
        *,
        training_fraction : builtins.float = ...,
        validation_fraction : builtins.float = ...,
        test_fraction : builtins.float = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["test_fraction",b"test_fraction","training_fraction",b"training_fraction","validation_fraction",b"validation_fraction"]) -> None: ...
global___FractionSplit = FractionSplit

class FilterSplit(google.protobuf.message.Message):
    """Assigns input data to training, validation, and test sets based on the given
    filters, data pieces not matched by any filter are ignored. Currently only
    supported for Datasets containing DataItems.
    If any of the filters in this message are to match nothing, then they can be
    set as '-' (the minus sign).

    Supported only for unstructured Datasets.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    TRAINING_FILTER_FIELD_NUMBER: builtins.int
    VALIDATION_FILTER_FIELD_NUMBER: builtins.int
    TEST_FILTER_FIELD_NUMBER: builtins.int
    training_filter: typing.Text = ...
    """Required. A filter on DataItems of the Dataset. DataItems that match
    this filter are used to train the Model. A filter with same syntax
    as the one used in [DatasetService.ListDataItems][google.cloud.aiplatform.v1.DatasetService.ListDataItems] may be used. If a
    single DataItem is matched by more than one of the FilterSplit filters,
    then it is assigned to the first set that applies to it in the
    training, validation, test order.
    """

    validation_filter: typing.Text = ...
    """Required. A filter on DataItems of the Dataset. DataItems that match
    this filter are used to validate the Model. A filter with same syntax
    as the one used in [DatasetService.ListDataItems][google.cloud.aiplatform.v1.DatasetService.ListDataItems] may be used. If a
    single DataItem is matched by more than one of the FilterSplit filters,
    then it is assigned to the first set that applies to it in the
    training, validation, test order.
    """

    test_filter: typing.Text = ...
    """Required. A filter on DataItems of the Dataset. DataItems that match
    this filter are used to test the Model. A filter with same syntax
    as the one used in [DatasetService.ListDataItems][google.cloud.aiplatform.v1.DatasetService.ListDataItems] may be used. If a
    single DataItem is matched by more than one of the FilterSplit filters,
    then it is assigned to the first set that applies to it in the
    training, validation, test order.
    """

    def __init__(self,
        *,
        training_filter : typing.Text = ...,
        validation_filter : typing.Text = ...,
        test_filter : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["test_filter",b"test_filter","training_filter",b"training_filter","validation_filter",b"validation_filter"]) -> None: ...
global___FilterSplit = FilterSplit

class PredefinedSplit(google.protobuf.message.Message):
    """Assigns input data to training, validation, and test sets based on the
    value of a provided key.

    Supported only for tabular Datasets.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    KEY_FIELD_NUMBER: builtins.int
    key: typing.Text = ...
    """Required. The key is a name of one of the Dataset's data columns.
    The value of the key (either the label's value or value in the column)
    must be one of {`training`, `validation`, `test`}, and it defines to which
    set the given piece of data is assigned. If for a piece of data the key
    is not present or has an invalid value, that piece is ignored by the
    pipeline.
    """

    def __init__(self,
        *,
        key : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["key",b"key"]) -> None: ...
global___PredefinedSplit = PredefinedSplit

class TimestampSplit(google.protobuf.message.Message):
    """Assigns input data to training, validation, and test sets based on a
    provided timestamps. The youngest data pieces are assigned to training set,
    next to validation set, and the oldest to the test set.

    Supported only for tabular Datasets.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    TRAINING_FRACTION_FIELD_NUMBER: builtins.int
    VALIDATION_FRACTION_FIELD_NUMBER: builtins.int
    TEST_FRACTION_FIELD_NUMBER: builtins.int
    KEY_FIELD_NUMBER: builtins.int
    training_fraction: builtins.float = ...
    """The fraction of the input data that is to be used to train the Model."""

    validation_fraction: builtins.float = ...
    """The fraction of the input data that is to be used to validate the Model."""

    test_fraction: builtins.float = ...
    """The fraction of the input data that is to be used to evaluate the Model."""

    key: typing.Text = ...
    """Required. The key is a name of one of the Dataset's data columns.
    The values of the key (the values in the column) must be in RFC 3339
    `date-time` format, where `time-offset` = `"Z"`
    (e.g. 1985-04-12T23:20:50.52Z). If for a piece of data the key is not
    present or has an invalid value, that piece is ignored by the pipeline.
    """

    def __init__(self,
        *,
        training_fraction : builtins.float = ...,
        validation_fraction : builtins.float = ...,
        test_fraction : builtins.float = ...,
        key : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["key",b"key","test_fraction",b"test_fraction","training_fraction",b"training_fraction","validation_fraction",b"validation_fraction"]) -> None: ...
global___TimestampSplit = TimestampSplit

class StratifiedSplit(google.protobuf.message.Message):
    """Assigns input data to the training, validation, and test sets so that the
    distribution of values found in the categorical column (as specified by the
    `key` field) is mirrored within each split. The fraction values determine
    the relative sizes of the splits.

    For example, if the specified column has three values, with 50% of the rows
    having value "A", 25% value "B", and 25% value "C", and the split fractions
    are specified as 80/10/10, then the training set will constitute 80% of the
    training data, with about 50% of the training set rows having the value "A"
    for the specified column, about 25% having the value "B", and about 25%
    having the value "C".

    Only the top 500 occurring values are used; any values not in the top
    500 values are randomly assigned to a split. If less than three rows contain
    a specific value, those rows are randomly assigned.

    Supported only for tabular Datasets.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    TRAINING_FRACTION_FIELD_NUMBER: builtins.int
    VALIDATION_FRACTION_FIELD_NUMBER: builtins.int
    TEST_FRACTION_FIELD_NUMBER: builtins.int
    KEY_FIELD_NUMBER: builtins.int
    training_fraction: builtins.float = ...
    """The fraction of the input data that is to be used to train the Model."""

    validation_fraction: builtins.float = ...
    """The fraction of the input data that is to be used to validate the Model."""

    test_fraction: builtins.float = ...
    """The fraction of the input data that is to be used to evaluate the Model."""

    key: typing.Text = ...
    """Required. The key is a name of one of the Dataset's data columns.
    The key provided must be for a categorical column.
    """

    def __init__(self,
        *,
        training_fraction : builtins.float = ...,
        validation_fraction : builtins.float = ...,
        test_fraction : builtins.float = ...,
        key : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["key",b"key","test_fraction",b"test_fraction","training_fraction",b"training_fraction","validation_fraction",b"validation_fraction"]) -> None: ...
global___StratifiedSplit = StratifiedSplit
