"""
@generated by mypy-protobuf.  Do not edit manually!
isort:skip_file
"""
import builtins
import google.cloud.bigquery.storage.v1beta1.arrow_pb2
import google.cloud.bigquery.storage.v1beta1.avro_pb2
import google.cloud.bigquery.storage.v1beta1.read_options_pb2
import google.cloud.bigquery.storage.v1beta1.table_reference_pb2
import google.protobuf.descriptor
import google.protobuf.internal.containers
import google.protobuf.internal.enum_type_wrapper
import google.protobuf.message
import google.protobuf.timestamp_pb2
import typing
import typing_extensions

DESCRIPTOR: google.protobuf.descriptor.FileDescriptor = ...

class _DataFormat:
    ValueType = typing.NewType('ValueType', builtins.int)
    V: typing_extensions.TypeAlias = ValueType
class _DataFormatEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_DataFormat.ValueType], builtins.type):
    DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor = ...
    DATA_FORMAT_UNSPECIFIED: DataFormat.ValueType = ...  # 0
    """Data format is unspecified."""

    AVRO: DataFormat.ValueType = ...  # 1
    """Avro is a standard open source row based file format.
    See https://avro.apache.org/ for more details.
    """

    ARROW: DataFormat.ValueType = ...  # 3
class DataFormat(_DataFormat, metaclass=_DataFormatEnumTypeWrapper):
    """Data format for input or output data."""
    pass

DATA_FORMAT_UNSPECIFIED: DataFormat.ValueType = ...  # 0
"""Data format is unspecified."""

AVRO: DataFormat.ValueType = ...  # 1
"""Avro is a standard open source row based file format.
See https://avro.apache.org/ for more details.
"""

ARROW: DataFormat.ValueType = ...  # 3
global___DataFormat = DataFormat


class _ShardingStrategy:
    ValueType = typing.NewType('ValueType', builtins.int)
    V: typing_extensions.TypeAlias = ValueType
class _ShardingStrategyEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_ShardingStrategy.ValueType], builtins.type):
    DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor = ...
    SHARDING_STRATEGY_UNSPECIFIED: ShardingStrategy.ValueType = ...  # 0
    """Same as LIQUID."""

    LIQUID: ShardingStrategy.ValueType = ...  # 1
    """Assigns data to each stream based on the client's read rate. The faster the
    client reads from a stream, the more data is assigned to the stream. In
    this strategy, it's possible to read all data from a single stream even if
    there are other streams present.
    """

    BALANCED: ShardingStrategy.ValueType = ...  # 2
    """Assigns data to each stream such that roughly the same number of rows can
    be read from each stream. Because the server-side unit for assigning data
    is collections of rows, the API does not guarantee that each stream will
    return the same number or rows. Additionally, the limits are enforced based
    on the number of pre-filtering rows, so some filters can lead to lopsided
    assignments.
    """

class ShardingStrategy(_ShardingStrategy, metaclass=_ShardingStrategyEnumTypeWrapper):
    """Strategy for distributing data among multiple streams in a read session."""
    pass

SHARDING_STRATEGY_UNSPECIFIED: ShardingStrategy.ValueType = ...  # 0
"""Same as LIQUID."""

LIQUID: ShardingStrategy.ValueType = ...  # 1
"""Assigns data to each stream based on the client's read rate. The faster the
client reads from a stream, the more data is assigned to the stream. In
this strategy, it's possible to read all data from a single stream even if
there are other streams present.
"""

BALANCED: ShardingStrategy.ValueType = ...  # 2
"""Assigns data to each stream such that roughly the same number of rows can
be read from each stream. Because the server-side unit for assigning data
is collections of rows, the API does not guarantee that each stream will
return the same number or rows. Additionally, the limits are enforced based
on the number of pre-filtering rows, so some filters can lead to lopsided
assignments.
"""

global___ShardingStrategy = ShardingStrategy


class Stream(google.protobuf.message.Message):
    """Information about a single data stream within a read session."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    NAME_FIELD_NUMBER: builtins.int
    name: typing.Text = ...
    """Name of the stream, in the form
    `projects/{project_id}/locations/{location}/streams/{stream_id}`.
    """

    def __init__(self,
        *,
        name : typing.Text = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["name",b"name"]) -> None: ...
global___Stream = Stream

class StreamPosition(google.protobuf.message.Message):
    """Expresses a point within a given stream using an offset position."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    STREAM_FIELD_NUMBER: builtins.int
    OFFSET_FIELD_NUMBER: builtins.int
    @property
    def stream(self) -> global___Stream:
        """Identifier for a given Stream."""
        pass
    offset: builtins.int = ...
    """Position in the stream."""

    def __init__(self,
        *,
        stream : typing.Optional[global___Stream] = ...,
        offset : builtins.int = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["stream",b"stream"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["offset",b"offset","stream",b"stream"]) -> None: ...
global___StreamPosition = StreamPosition

class ReadSession(google.protobuf.message.Message):
    """Information returned from a `CreateReadSession` request."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    NAME_FIELD_NUMBER: builtins.int
    EXPIRE_TIME_FIELD_NUMBER: builtins.int
    AVRO_SCHEMA_FIELD_NUMBER: builtins.int
    ARROW_SCHEMA_FIELD_NUMBER: builtins.int
    STREAMS_FIELD_NUMBER: builtins.int
    TABLE_REFERENCE_FIELD_NUMBER: builtins.int
    TABLE_MODIFIERS_FIELD_NUMBER: builtins.int
    SHARDING_STRATEGY_FIELD_NUMBER: builtins.int
    name: typing.Text = ...
    """Unique identifier for the session, in the form
    `projects/{project_id}/locations/{location}/sessions/{session_id}`.
    """

    @property
    def expire_time(self) -> google.protobuf.timestamp_pb2.Timestamp:
        """Time at which the session becomes invalid. After this time, subsequent
        requests to read this Session will return errors.
        """
        pass
    @property
    def avro_schema(self) -> google.cloud.bigquery.storage.v1beta1.avro_pb2.AvroSchema:
        """Avro schema."""
        pass
    @property
    def arrow_schema(self) -> google.cloud.bigquery.storage.v1beta1.arrow_pb2.ArrowSchema:
        """Arrow schema."""
        pass
    @property
    def streams(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___Stream]:
        """Streams associated with this session."""
        pass
    @property
    def table_reference(self) -> google.cloud.bigquery.storage.v1beta1.table_reference_pb2.TableReference:
        """Table that this ReadSession is reading from."""
        pass
    @property
    def table_modifiers(self) -> google.cloud.bigquery.storage.v1beta1.table_reference_pb2.TableModifiers:
        """Any modifiers which are applied when reading from the specified table."""
        pass
    sharding_strategy: global___ShardingStrategy.ValueType = ...
    """The strategy to use for distributing data among the streams."""

    def __init__(self,
        *,
        name : typing.Text = ...,
        expire_time : typing.Optional[google.protobuf.timestamp_pb2.Timestamp] = ...,
        avro_schema : typing.Optional[google.cloud.bigquery.storage.v1beta1.avro_pb2.AvroSchema] = ...,
        arrow_schema : typing.Optional[google.cloud.bigquery.storage.v1beta1.arrow_pb2.ArrowSchema] = ...,
        streams : typing.Optional[typing.Iterable[global___Stream]] = ...,
        table_reference : typing.Optional[google.cloud.bigquery.storage.v1beta1.table_reference_pb2.TableReference] = ...,
        table_modifiers : typing.Optional[google.cloud.bigquery.storage.v1beta1.table_reference_pb2.TableModifiers] = ...,
        sharding_strategy : global___ShardingStrategy.ValueType = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["arrow_schema",b"arrow_schema","avro_schema",b"avro_schema","expire_time",b"expire_time","schema",b"schema","table_modifiers",b"table_modifiers","table_reference",b"table_reference"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["arrow_schema",b"arrow_schema","avro_schema",b"avro_schema","expire_time",b"expire_time","name",b"name","schema",b"schema","sharding_strategy",b"sharding_strategy","streams",b"streams","table_modifiers",b"table_modifiers","table_reference",b"table_reference"]) -> None: ...
    def WhichOneof(self, oneof_group: typing_extensions.Literal["schema",b"schema"]) -> typing.Optional[typing_extensions.Literal["avro_schema","arrow_schema"]]: ...
global___ReadSession = ReadSession

class CreateReadSessionRequest(google.protobuf.message.Message):
    """Creates a new read session, which may include additional options such as
    requested parallelism, projection filters and constraints.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    TABLE_REFERENCE_FIELD_NUMBER: builtins.int
    PARENT_FIELD_NUMBER: builtins.int
    TABLE_MODIFIERS_FIELD_NUMBER: builtins.int
    REQUESTED_STREAMS_FIELD_NUMBER: builtins.int
    READ_OPTIONS_FIELD_NUMBER: builtins.int
    FORMAT_FIELD_NUMBER: builtins.int
    SHARDING_STRATEGY_FIELD_NUMBER: builtins.int
    @property
    def table_reference(self) -> google.cloud.bigquery.storage.v1beta1.table_reference_pb2.TableReference:
        """Required. Reference to the table to read."""
        pass
    parent: typing.Text = ...
    """Required. String of the form `projects/{project_id}` indicating the
    project this ReadSession is associated with. This is the project that will
    be billed for usage.
    """

    @property
    def table_modifiers(self) -> google.cloud.bigquery.storage.v1beta1.table_reference_pb2.TableModifiers:
        """Any modifiers to the Table (e.g. snapshot timestamp)."""
        pass
    requested_streams: builtins.int = ...
    """Initial number of streams. If unset or 0, we will
    provide a value of streams so as to produce reasonable throughput. Must be
    non-negative. The number of streams may be lower than the requested number,
    depending on the amount parallelism that is reasonable for the table and
    the maximum amount of parallelism allowed by the system.

    Streams must be read starting from offset 0.
    """

    @property
    def read_options(self) -> google.cloud.bigquery.storage.v1beta1.read_options_pb2.TableReadOptions:
        """Read options for this session (e.g. column selection, filters)."""
        pass
    format: global___DataFormat.ValueType = ...
    """Data output format. Currently default to Avro."""

    sharding_strategy: global___ShardingStrategy.ValueType = ...
    """The strategy to use for distributing data among multiple streams. Currently
    defaults to liquid sharding.
    """

    def __init__(self,
        *,
        table_reference : typing.Optional[google.cloud.bigquery.storage.v1beta1.table_reference_pb2.TableReference] = ...,
        parent : typing.Text = ...,
        table_modifiers : typing.Optional[google.cloud.bigquery.storage.v1beta1.table_reference_pb2.TableModifiers] = ...,
        requested_streams : builtins.int = ...,
        read_options : typing.Optional[google.cloud.bigquery.storage.v1beta1.read_options_pb2.TableReadOptions] = ...,
        format : global___DataFormat.ValueType = ...,
        sharding_strategy : global___ShardingStrategy.ValueType = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["read_options",b"read_options","table_modifiers",b"table_modifiers","table_reference",b"table_reference"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["format",b"format","parent",b"parent","read_options",b"read_options","requested_streams",b"requested_streams","sharding_strategy",b"sharding_strategy","table_modifiers",b"table_modifiers","table_reference",b"table_reference"]) -> None: ...
global___CreateReadSessionRequest = CreateReadSessionRequest

class ReadRowsRequest(google.protobuf.message.Message):
    """Requesting row data via `ReadRows` must provide Stream position information."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    READ_POSITION_FIELD_NUMBER: builtins.int
    @property
    def read_position(self) -> global___StreamPosition:
        """Required. Identifier of the position in the stream to start reading from.
        The offset requested must be less than the last row read from ReadRows.
        Requesting a larger offset is undefined.
        """
        pass
    def __init__(self,
        *,
        read_position : typing.Optional[global___StreamPosition] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["read_position",b"read_position"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["read_position",b"read_position"]) -> None: ...
global___ReadRowsRequest = ReadRowsRequest

class StreamStatus(google.protobuf.message.Message):
    """Progress information for a given Stream."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    ESTIMATED_ROW_COUNT_FIELD_NUMBER: builtins.int
    FRACTION_CONSUMED_FIELD_NUMBER: builtins.int
    PROGRESS_FIELD_NUMBER: builtins.int
    IS_SPLITTABLE_FIELD_NUMBER: builtins.int
    estimated_row_count: builtins.int = ...
    """Number of estimated rows in the current stream. May change over time as
    different readers in the stream progress at rates which are relatively fast
    or slow.
    """

    fraction_consumed: builtins.float = ...
    """A value in the range [0.0, 1.0] that represents the fraction of rows
    assigned to this stream that have been processed by the server. In the
    presence of read filters, the server may process more rows than it returns,
    so this value reflects progress through the pre-filtering rows.

    This value is only populated for sessions created through the BALANCED
    sharding strategy.
    """

    @property
    def progress(self) -> global___Progress:
        """Represents the progress of the current stream."""
        pass
    is_splittable: builtins.bool = ...
    """Whether this stream can be split. For sessions that use the LIQUID sharding
    strategy, this value is always false. For BALANCED sessions, this value is
    false when enough data have been read such that no more splits are possible
    at that point or beyond. For small tables or streams that are the result of
    a chain of splits, this value may never be true.
    """

    def __init__(self,
        *,
        estimated_row_count : builtins.int = ...,
        fraction_consumed : builtins.float = ...,
        progress : typing.Optional[global___Progress] = ...,
        is_splittable : builtins.bool = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["progress",b"progress"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["estimated_row_count",b"estimated_row_count","fraction_consumed",b"fraction_consumed","is_splittable",b"is_splittable","progress",b"progress"]) -> None: ...
global___StreamStatus = StreamStatus

class Progress(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    AT_RESPONSE_START_FIELD_NUMBER: builtins.int
    AT_RESPONSE_END_FIELD_NUMBER: builtins.int
    at_response_start: builtins.float = ...
    """The fraction of rows assigned to the stream that have been processed by the
    server so far, not including the rows in the current response message.

    This value, along with `at_response_end`, can be used to interpolate the
    progress made as the rows in the message are being processed using the
    following formula: `at_response_start + (at_response_end -
    at_response_start) * rows_processed_from_response / rows_in_response`.

    Note that if a filter is provided, the `at_response_end` value of the
    previous response may not necessarily be equal to the `at_response_start`
    value of the current response.
    """

    at_response_end: builtins.float = ...
    """Similar to `at_response_start`, except that this value includes the rows in
    the current response.
    """

    def __init__(self,
        *,
        at_response_start : builtins.float = ...,
        at_response_end : builtins.float = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["at_response_end",b"at_response_end","at_response_start",b"at_response_start"]) -> None: ...
global___Progress = Progress

class ThrottleStatus(google.protobuf.message.Message):
    """Information on if the current connection is being throttled."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    THROTTLE_PERCENT_FIELD_NUMBER: builtins.int
    throttle_percent: builtins.int = ...
    """How much this connection is being throttled.
    0 is no throttling, 100 is completely throttled.
    """

    def __init__(self,
        *,
        throttle_percent : builtins.int = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["throttle_percent",b"throttle_percent"]) -> None: ...
global___ThrottleStatus = ThrottleStatus

class ReadRowsResponse(google.protobuf.message.Message):
    """Response from calling `ReadRows` may include row data, progress and
    throttling information.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    AVRO_ROWS_FIELD_NUMBER: builtins.int
    ARROW_RECORD_BATCH_FIELD_NUMBER: builtins.int
    ROW_COUNT_FIELD_NUMBER: builtins.int
    STATUS_FIELD_NUMBER: builtins.int
    THROTTLE_STATUS_FIELD_NUMBER: builtins.int
    @property
    def avro_rows(self) -> google.cloud.bigquery.storage.v1beta1.avro_pb2.AvroRows:
        """Serialized row data in AVRO format."""
        pass
    @property
    def arrow_record_batch(self) -> google.cloud.bigquery.storage.v1beta1.arrow_pb2.ArrowRecordBatch:
        """Serialized row data in Arrow RecordBatch format."""
        pass
    row_count: builtins.int = ...
    """Number of serialized rows in the rows block. This value is recorded here,
    in addition to the row_count values in the output-specific messages in
    `rows`, so that code which needs to record progress through the stream can
    do so in an output format-independent way.
    """

    @property
    def status(self) -> global___StreamStatus:
        """Estimated stream statistics."""
        pass
    @property
    def throttle_status(self) -> global___ThrottleStatus:
        """Throttling status. If unset, the latest response still describes
        the current throttling status.
        """
        pass
    def __init__(self,
        *,
        avro_rows : typing.Optional[google.cloud.bigquery.storage.v1beta1.avro_pb2.AvroRows] = ...,
        arrow_record_batch : typing.Optional[google.cloud.bigquery.storage.v1beta1.arrow_pb2.ArrowRecordBatch] = ...,
        row_count : builtins.int = ...,
        status : typing.Optional[global___StreamStatus] = ...,
        throttle_status : typing.Optional[global___ThrottleStatus] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["arrow_record_batch",b"arrow_record_batch","avro_rows",b"avro_rows","rows",b"rows","status",b"status","throttle_status",b"throttle_status"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["arrow_record_batch",b"arrow_record_batch","avro_rows",b"avro_rows","row_count",b"row_count","rows",b"rows","status",b"status","throttle_status",b"throttle_status"]) -> None: ...
    def WhichOneof(self, oneof_group: typing_extensions.Literal["rows",b"rows"]) -> typing.Optional[typing_extensions.Literal["avro_rows","arrow_record_batch"]]: ...
global___ReadRowsResponse = ReadRowsResponse

class BatchCreateReadSessionStreamsRequest(google.protobuf.message.Message):
    """Information needed to request additional streams for an established read
    session.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    SESSION_FIELD_NUMBER: builtins.int
    REQUESTED_STREAMS_FIELD_NUMBER: builtins.int
    @property
    def session(self) -> global___ReadSession:
        """Required. Must be a non-expired session obtained from a call to
        CreateReadSession. Only the name field needs to be set.
        """
        pass
    requested_streams: builtins.int = ...
    """Required. Number of new streams requested. Must be positive.
    Number of added streams may be less than this, see CreateReadSessionRequest
    for more information.
    """

    def __init__(self,
        *,
        session : typing.Optional[global___ReadSession] = ...,
        requested_streams : builtins.int = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["session",b"session"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["requested_streams",b"requested_streams","session",b"session"]) -> None: ...
global___BatchCreateReadSessionStreamsRequest = BatchCreateReadSessionStreamsRequest

class BatchCreateReadSessionStreamsResponse(google.protobuf.message.Message):
    """The response from `BatchCreateReadSessionStreams` returns the stream
    identifiers for the newly created streams.
    """
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    STREAMS_FIELD_NUMBER: builtins.int
    @property
    def streams(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___Stream]:
        """Newly added streams."""
        pass
    def __init__(self,
        *,
        streams : typing.Optional[typing.Iterable[global___Stream]] = ...,
        ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["streams",b"streams"]) -> None: ...
global___BatchCreateReadSessionStreamsResponse = BatchCreateReadSessionStreamsResponse

class FinalizeStreamRequest(google.protobuf.message.Message):
    """Request information for invoking `FinalizeStream`."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    STREAM_FIELD_NUMBER: builtins.int
    @property
    def stream(self) -> global___Stream:
        """Required. Stream to finalize."""
        pass
    def __init__(self,
        *,
        stream : typing.Optional[global___Stream] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["stream",b"stream"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["stream",b"stream"]) -> None: ...
global___FinalizeStreamRequest = FinalizeStreamRequest

class SplitReadStreamRequest(google.protobuf.message.Message):
    """Request information for `SplitReadStream`."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    ORIGINAL_STREAM_FIELD_NUMBER: builtins.int
    FRACTION_FIELD_NUMBER: builtins.int
    @property
    def original_stream(self) -> global___Stream:
        """Required. Stream to split."""
        pass
    fraction: builtins.float = ...
    """A value in the range (0.0, 1.0) that specifies the fractional point at
    which the original stream should be split. The actual split point is
    evaluated on pre-filtered rows, so if a filter is provided, then there is
    no guarantee that the division of the rows between the new child streams
    will be proportional to this fractional value. Additionally, because the
    server-side unit for assigning data is collections of rows, this fraction
    will always map to to a data storage boundary on the server side.
    """

    def __init__(self,
        *,
        original_stream : typing.Optional[global___Stream] = ...,
        fraction : builtins.float = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["original_stream",b"original_stream"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["fraction",b"fraction","original_stream",b"original_stream"]) -> None: ...
global___SplitReadStreamRequest = SplitReadStreamRequest

class SplitReadStreamResponse(google.protobuf.message.Message):
    """Response from `SplitReadStream`."""
    DESCRIPTOR: google.protobuf.descriptor.Descriptor = ...
    PRIMARY_STREAM_FIELD_NUMBER: builtins.int
    REMAINDER_STREAM_FIELD_NUMBER: builtins.int
    @property
    def primary_stream(self) -> global___Stream:
        """Primary stream, which contains the beginning portion of
        |original_stream|. An empty value indicates that the original stream can no
        longer be split.
        """
        pass
    @property
    def remainder_stream(self) -> global___Stream:
        """Remainder stream, which contains the tail of |original_stream|. An empty
        value indicates that the original stream can no longer be split.
        """
        pass
    def __init__(self,
        *,
        primary_stream : typing.Optional[global___Stream] = ...,
        remainder_stream : typing.Optional[global___Stream] = ...,
        ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["primary_stream",b"primary_stream","remainder_stream",b"remainder_stream"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["primary_stream",b"primary_stream","remainder_stream",b"remainder_stream"]) -> None: ...
global___SplitReadStreamResponse = SplitReadStreamResponse
