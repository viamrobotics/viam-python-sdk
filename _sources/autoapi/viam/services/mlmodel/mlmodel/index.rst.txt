viam.services.mlmodel.mlmodel
=============================

.. py:module:: viam.services.mlmodel.mlmodel


Classes
-------

.. autoapisummary::

   viam.services.mlmodel.mlmodel.MLModel


Module Contents
---------------

.. py:class:: MLModel(name: str, *, logger: Optional[logging.Logger] = None)

   Bases: :py:obj:`viam.services.service_base.ServiceBase`


   MLModel represents a Machine Learning Model service.

   This acts as an abstract base class for any drivers representing specific
   arm implementations. This cannot be used on its own. If the ``__init__()`` function is
   overridden, it must call the ``super().__init__()`` function.

   For more information, see `ML model service <https://docs.viam.com/services/ml/deploy/>`_.


   .. py:attribute:: SUBTYPE
      :type:  Final

      The Subtype of the Resource


   .. py:method:: infer(input_tensors: Dict[str, numpy.typing.NDArray], *, extra: Optional[Mapping[str, viam.utils.ValueTypes]] = None, timeout: Optional[float] = None) -> Dict[str, numpy.typing.NDArray]
      :abstractmethod:

      :async:


      Take an already ordered input tensor as an array, make an inference on the model, and return an output tensor map.

      ::

          import numpy as np

          my_mlmodel = MLModelClient.from_robot(robot=machine, name="my_mlmodel_service")

          nd_array = np.array([1, 2, 3], dtype=np.float64)
          input_tensors = {"0": nd_array}

          output_tensors = await my_mlmodel.infer(input_tensors)

      :param input_tensors: A dictionary of input flat tensors as specified in the metadata
      :type input_tensors: Dict[str, NDArray]

      :returns: A dictionary of output flat tensors as specified in the metadata
      :rtype: Dict[str, NDArray]

      For more information, see `ML model service <https://docs.viam.com/services/ml/deploy/>`_.



   .. py:method:: metadata(*, extra: Optional[Mapping[str, viam.utils.ValueTypes]] = None, timeout: Optional[float] = None) -> viam.proto.service.mlmodel.Metadata
      :abstractmethod:

      :async:


      Get the metadata (such as name, type, expected tensor/array shape, inputs, and outputs) associated with the ML model.

      ::

          my_mlmodel = MLModelClient.from_robot(robot=machine, name="my_mlmodel_service")

          metadata = await my_mlmodel.metadata()

      :returns: The metadata
      :rtype: Metadata

      For more information, see `ML model service <https://docs.viam.com/services/ml/deploy/>`_.



   .. py:method:: from_robot(robot: viam.robot.client.RobotClient, name: str) -> typing_extensions.Self
      :classmethod:


      Get the service named ``name`` from the provided robot.

      ::

          async def connect() -> RobotClient:
              # Replace "<API-KEY>" (including brackets) with your API key and "<API-KEY-ID>" with your API key ID
              options = RobotClient.Options.with_api_key("<API-KEY>", "<API-KEY-ID>")
              # Replace "<MACHINE-URL>" (included brackets) with your machine's connection URL or FQDN
              return await RobotClient.at_address("<MACHINE-URL>", options)

          async def main():
              robot = await connect()

              # Can be used with any resource, using the motion service as an example
              motion = MotionClient.from_robot(robot=machine, name="builtin")

              robot.close()

      :param robot: The robot
      :type robot: RobotClient
      :param name: The name of the service
      :type name: str

      :returns: The service, if it exists on the robot
      :rtype: Self



   .. py:method:: do_command(command: Mapping[str, viam.utils.ValueTypes], *, timeout: Optional[float] = None, **kwargs) -> Mapping[str, viam.utils.ValueTypes]
      :abstractmethod:

      :async:


      Send/receive arbitrary commands.

      ::

          service = SERVICE.from_robot(robot=machine, "builtin")  # replace SERVICE with the appropriate class

          my_command = {
            "cmnd": "dosomething",
            "someparameter": 52
          }

          # Can be used with any resource, using the motion service as an example
          await service.do_command(command=my_command)

      :param command: The command to execute
      :type command: Dict[str, ValueTypes]

      :returns: Result of the executed command
      :rtype: Dict[str, ValueTypes]



   .. py:method:: get_resource_name(name: str) -> viam.proto.common.ResourceName
      :classmethod:


      Get the ResourceName for this Resource with the given name

      ::

          # Can be used with any resource, using an arm as an example
          my_arm_name = Arm.get_resource_name("my_arm")

      :param name: The name of the Resource
      :type name: str

      :returns: The ResourceName of this Resource
      :rtype: ResourceName



   .. py:method:: get_operation(kwargs: Mapping[str, Any]) -> viam.operations.Operation

      Get the ``Operation`` associated with the currently running function.

      When writing custom resources, you should get the ``Operation`` by calling this function and check to see if it's cancelled.
      If the ``Operation`` is cancelled, then you can perform any necessary (terminating long running tasks, cleaning up connections, etc.
      ).

      :param kwargs: The kwargs object containing the operation
      :type kwargs: Mapping[str, Any]

      :returns: The operation associated with this function
      :rtype: viam.operations.Operation



   .. py:method:: close()
      :async:


      Safely shut down the resource and prevent further use.

      Close must be idempotent. Later configuration may allow a resource to be "open" again.
      If a resource does not want or need a close function, it is assumed that the resource does not need to return errors when future
      non-Close methods are called.

      ::

          await component.close()




